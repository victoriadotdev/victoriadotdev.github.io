<!doctype html><html lang=en-us><head><head><meta charset=utf-8><meta name=mobile-web-app-capable content="yes"><meta name=viewport content="width=device-width,initial-scale=1"><title>victoria.dev</title><meta name=monetization content="$ilp.uphold.com/pBRfRwg2EJAe"><link rel="shortcut icon" href=https://victoria.dev/favicon.ico><link rel=icon type=image/png href=https://victoria.dev/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=https://victoria.dev/favicon-16x16.png sizes=16x16><link rel=authorization_endpoint href=https://indieauth.com/auth><link rel=token_endpoint href=https://tokens.indieauth.com/token><link rel=me href=https://github.com/victoriadrake><link rel=me href=https://twitter.com/victoriadotdev><link rel=me href=https://mastodon.technology/@victoria><link rel=stylesheet href=https://victoria.dev/css/style.css crossorigin=anonymous media=screen><link rel=webmention href=https://webmention.io/victoria.dev/webmention><link rel=pingback href=https://webmention.io/victoria.dev/xmlrpc><meta property="og:url" content="https://victoria.dev/"><meta property="og:title" content="victoria.dev"><meta property="og:site_name" content="victoria.dev"><meta property="og:type" content="website"><meta name=twitter:title content="victoria.dev"></head></head><body><header role=banner><nav aria-label="Victoria.dev main menu" id=menu><ul role=menubar><li role=none class=menu-item><a role=menuitem href=/><img src=/icon/bookmark.svg alt="home page icon" height=18px class=filter-icon>&nbsp;hello</a></li><li role=none class="menu-item active"><span role=menuitem href=/blog/><img src=/icon/quote.svg alt="blog icon" height=18px class=filter-icon>&nbsp;blog</span></li><li role=none class=menu-item><a role=menuitem href=/about/><img src=/icon/profile.svg alt="about page icon" height=18px class=filter-icon>&nbsp;about</a></li><li role=none class=menu-item><a role=menuitem href=/bookshelf/><img src=/icon/book.svg alt="bookshelf icon" height=18px class=filter-icon>&nbsp;bookshelf</a></li><li role=none class=menu-item><a href=https://victoria.dev/blog/index.xml>rss</a></li><li role=none class=menu-item><a href=https://victoria.dev/blog/feed.json>json</a></li></ul></nav></header><main aria-role=main><div class=container><article><h1>Victoria Drake's Blog</h1><div id=all-tags><p><a class="tag button" href=/tags/coding/>coding</a> &nbsp;
<a class="tag button" href=/tags/cybersecurity/>cybersecurity</a> &nbsp;
<a class="tag button" href=/tags/life/>life</a> &nbsp;
<a class="tag button" href=/tags/terminal/>terminal</a> &nbsp;
<a class="tag button" href=/tags/data/>data</a> &nbsp;
<a class="tag button" href=/tags/linux/>linux</a> &nbsp;
<a class="tag button" href=/tags/websites/>websites</a> &nbsp;
<a class="tag button" href=/tags/ci/cd/>ci/cd</a> &nbsp;
<a class="tag button" href=/tags/open-source/>open-source</a> &nbsp;
<a class="tag button" href=/tags/tech-team/>tech-team</a> &nbsp;
<a class="tag button" href=/tags/aws/>aws</a> &nbsp;
<a class="tag button" href=/tags/computing/>computing</a> &nbsp;
<a class="tag button" href=/tags/go/>go</a> &nbsp;
<a class="tag button" href=/tags/python/>python</a> &nbsp;
<a class="tag button" href=/tags/privacy/>privacy</a> &nbsp;
<a class="tag button" href=/tags/git/>git</a> &nbsp;
<a class="tag button" href=/tags/javascript/>javascript</a> &nbsp;
<a class="tag button" href=/tags/algorithms/>algorithms</a> &nbsp;
<a class="tag button" href=/tags/docs/>docs</a> &nbsp;
<a class="tag button" href=/tags/api/>api</a> &nbsp;
<a class="tag button" href=/tags/leadership/>leadership</a> &nbsp;
<a class="tag button" href=/tags/protocols/>protocols</a> &nbsp;</p></div><div class=markdown><form id=search action=https://victoria.dev/search/ method=get><label hidden for=search-input>Search site</label>
<input type=text id=search-input autofocus name=q placeholder="🔎 Looking for something?">
<input type=submit value=search></form></div></article><hr><nav aria-label="article list" class=markdown><ul role=menubar id=article-list class=h-feed><div class="h-card hidden"><a href=https://victoria.dev/blog/ class="p-name u-url hidden" rel=me>Victoria Drake</a></div><li role=none class=h-entry><a rel=author class="p-author h-card hidden" href=https://victoria.dev/>Victoria Drake</a>
<a role=menuitem class="article-link u-url p-name" href=https://victoria.dev/blog/digital-resilience-redundancy-for-websites-and-communications/>Digital resilience: redundancy for websites and communications</a><p class="p-summary e-content">How you can make your digital life more resilient when using services you don't own.</p><p class=metadata><a class="p-category tag button" href=https://victoria.dev/tags/privacy/>privacy</a>
&nbsp;5 min read
<time class="hidden dt-published">2021-02-22 04:00:43 -0500 -0500</time></p></li><li role=none class=h-entry><a rel=author class="p-author h-card hidden" href=https://victoria.dev/>Victoria Drake</a>
<a role=menuitem class="article-link u-url p-name" href=https://victoria.dev/blog/create-a-self-hosted-chat-service-with-your-own-matrix-server/>Create a self-hosted chat service with your own Matrix server</a><p class="p-summary e-content">A speed-run introduction to Matrix via Dendrite.</p><p class=metadata><a class="p-category tag button" href=https://victoria.dev/tags/privacy/>privacy</a>
&nbsp;6 min read
<time class="hidden dt-published">2021-02-15 01:38:07 -0500 -0500</time></p></li><li role=none class=h-entry><a rel=author class="p-author h-card hidden" href=https://victoria.dev/>Victoria Drake</a>
<a role=menuitem class="article-link u-url p-name" href=https://victoria.dev/blog/do-i-raise-or-return-errors-in-python/>Do I raise or return errors in Python?</a><p class="p-summary e-content">Raise, return, and how to never fail silently in Python.</p><p class=metadata><a class="p-category tag button" href=https://victoria.dev/tags/python/>python</a>
&nbsp;4 min read
<time class="hidden dt-published">2021-02-09 05:34:48 -0500 -0500</time></p></li><li role=none class=h-entry><a rel=author class="p-author h-card hidden" href=https://victoria.dev/>Victoria Drake</a>
<a role=menuitem class="article-link u-url p-name" href=https://victoria.dev/blog/what-tech-leaders-do-before-going-on-vacation/>What tech leaders do before going on vacation</a><p class="p-summary e-content">How to be a responsible leader even while you're away.</p><p class=metadata><a class="p-category tag button" href=https://victoria.dev/tags/tech-team/>tech team</a>
&nbsp;3 min read
<time class="hidden dt-published">2021-02-01 04:02:54 -0600 -0600</time></p></li><li role=none class=h-entry><a rel=author class="p-author h-card hidden" href=https://victoria.dev/>Victoria Drake</a>
<a role=menuitem class="article-link u-url p-name" href=https://victoria.dev/blog/add-search-to-hugo-static-sites-with-lunr/>Add search to Hugo static sites with Lunr</a><p class="p-summary e-content">Make your static site searchable with a client-side search index.</p><p class=metadata><a class="p-category tag button" href=https://victoria.dev/tags/websites/>websites</a>
&nbsp;7 min read
<time class="hidden dt-published">2021-01-26 09:25:17 -0500 -0500</time></p></li><li role=none class=h-entry><a rel=author class="p-author h-card hidden" href=https://victoria.dev/>Victoria Drake</a>
<a role=menuitem class="article-link u-url p-name" href=https://victoria.dev/blog/make-your-own-independent-website/>Make your own independent website</a><p class="p-summary e-content">How to make 2021 the year of the independent web.</p><p class=metadata><a class="p-category tag button" href=https://victoria.dev/tags/websites/>websites</a>
&nbsp;3 min read
<time class="hidden dt-published">2021-01-16 08:41:27 -0500 -0500</time></p></li><li role=none class=h-entry><a rel=author class="p-author h-card hidden" href=https://victoria.dev/>Victoria Drake</a>
<a role=menuitem class="article-link u-url p-name" href=https://victoria.dev/blog/how-to-get-hired-as-a-software-developer/>How to get hired as a software developer</a><p class="p-summary e-content">What to know before applying for a software developer job.</p><p class=metadata><a class="p-category tag button" href=https://victoria.dev/tags/life/>life</a>
&nbsp;7 min read
<time class="hidden dt-published">2021-01-12 05:50:53 -0500 -0500</time></p></li><li role=none class=h-entry><a rel=author class="p-author h-card hidden" href=https://victoria.dev/>Victoria Drake</a>
<a role=menuitem class="article-link u-url p-name" href=https://victoria.dev/blog/how-to-become-a-software-developer/>How to become a software developer</a><p class="p-summary e-content">The article I wish I had read when I started coding.</p><p class=metadata><a class="p-category tag button" href=https://victoria.dev/tags/coding/>coding</a>
&nbsp;5 min read
<time class="hidden dt-published">2021-01-05 04:50:07 -0600 -0600</time></p></li><li role=none class=h-entry><a rel=author class="p-author h-card hidden" href=https://victoria.dev/>Victoria Drake</a>
<a role=menuitem class="article-link u-url p-name" href=https://victoria.dev/blog/be-brave-and-build-in-public/>Be brave and build in public</a><p class="p-summary e-content">Make every day feel like Christmas when you participate in open source communities.</p><p class=metadata><a class="p-category tag button" href=https://victoria.dev/tags/open-source/>open source</a>
&nbsp;5 min read
<time class="hidden dt-published">2020-12-24 04:57:31 -0600 -0600</time></p></li><li role=none class=h-entry><a rel=author class="p-author h-card hidden" href=https://victoria.dev/>Victoria Drake</a>
<a role=menuitem class="article-link u-url p-name" href=https://victoria.dev/blog/so-youre-the-family-tech-support/>So you're the family tech support</a><p class="p-summary e-content">Privacy and online security to-dos for the home-for-the-holidays tech support hero.</p><p class=metadata><a class="p-category tag button" href=https://victoria.dev/tags/privacy/>privacy</a>
&nbsp;2 min read
<time class="hidden dt-published">2020-12-21 08:42:24 -0500 -0500</time></p></li></ul></nav><hr><ul class=pagination><li class=page-item><span class=page-link>first page</span></li><li class=page-item><span class=page-link href=/blog/>1</a></li><li class="page-item number"><a class=page-link href=/blog/page/2/>2</a></li><li class="page-item number"><a class=page-link href=/blog/page/3/>3</a></li><li class="page-item disabled"><span aria-hidden=true>&nbsp;&mldr;&nbsp;</span></li><li class="page-item number"><a class=page-link href=/blog/page/10/>10</a></li><li class=page-item><a href=/blog/page/2/ class=page-link aria-label=Next><span aria-hidden=true>older</span></a> /
<a href=/blog/page/10/ class=page-link aria-label=Last><span aria-hidden=true>oldest</span></a></li></ul></div><footer class=container><div class="h-card row bio" rel=author><div class=markdown id=card><figure class=profile><img class=u-photo alt="Victoria Drake" src=/img/victoria_headshot.jpg></figure><h1 class=p-name>Victoria Drake</h1></address><p class=p-note><p>Victoria Drake is a Director of Engineering in Washington, DC. She is a core maintainer and co-author for the Open Web Application Security Project (OWASP) Web Security Testing Guide. She earned the annual Top Contributor award three years in a row from the freeCodeCamp non-profit, and was recognized two years in a row as a Distinguished Author on the DEV.to developer platform. She writes and educates programmers and business leaders about cybersecurity, software development, and building happy and productive technical teams.</p><p><a href=/about>about</a> - <a href=/about/#write-me>contact</a> - <a href=https://github.com/victoriadrake>github</a> - <a href=https://twitter.com/victoriadotdev>twitter</a> - <a href=https://www.linkedin.com/in/victoriadotdev/>linkedin</a></p></p></div></div><script>window.store={"https:\/\/victoria.dev\/neofeed\/7dfe5d66-2ad6-47de-a7aa-e647afd0d87a\/":{title:"7dfe5d66-2ad6-47de-a7aa-e647afd0d87a",tags:[],content:"I\u0026rsquo;m open-sourcing my MSCS notes at https://openmscs.com/. Want to learn along with me? The GitHub repo is here 👉 https://github.com/victoriadrake/open-mscs/\n",url:"https://victoria.dev/neofeed/7dfe5d66-2ad6-47de-a7aa-e647afd0d87a/"},"https:\/\/victoria.dev\/neofeed\/b61c9824-edfd-46ee-a901-467184f81c03\/":{title:"b61c9824-edfd-46ee-a901-467184f81c03",tags:[],content:"If you want it, find it. If it doesn\u0026rsquo;t exist, make it. If you don\u0026rsquo;t know how, learn.\n",url:"https://victoria.dev/neofeed/b61c9824-edfd-46ee-a901-467184f81c03/"},"https:\/\/victoria.dev\/neofeed\/a742565c-4124-4faa-a0f3-e4b7bf5e6470\/":{title:"a742565c-4124-4faa-a0f3-e4b7bf5e6470",tags:[],content:"There are two hard problems in computer science:\n Cache coherence Naming things Off-by-one errors  ",url:"https://victoria.dev/neofeed/a742565c-4124-4faa-a0f3-e4b7bf5e6470/"},"https:\/\/victoria.dev\/neofeed\/60c9c9de-aedf-48b2-b417-5d819c154b58\/":{title:"60c9c9de-aedf-48b2-b417-5d819c154b58",tags:[],content:"What\u0026rsquo;s something you\u0026rsquo;ve figured out that would improve the lives of most people if they did it too?\n",url:"https://victoria.dev/neofeed/60c9c9de-aedf-48b2-b417-5d819c154b58/"},"https:\/\/victoria.dev\/blog\/digital-resilience-redundancy-for-websites-and-communications\/":{title:"Digital resilience: redundancy for websites and communications",tags:["privacy","cybersecurity","data","life","websites"],content:"When what seems like half the planet noped out of WhatsApp after its terms of service update, applications like Signal (which I highly recommend) saw an unprecedented increase in user traffic. Signal had so many new users sign up that it overwhelmed their existing infrastructure and lead to a 24-hour-ish outage. The small team responded impressively quickly, especially given that a 4,200% spike in new users was utterly implausible before it ocurred.\nThe downside of so many people moving onto this fantastic application is that it caused a brief outage. If you rely solely on a certain application for your communications, brief outages can be debilitating. Even when it seems implausible that your favorite chat, email, or website service could just \u0026ndash; poof \u0026ndash; vanish overnight, recent events have proved it isn\u0026rsquo;t impossible.\nHave a backup plan. Have several. Here\u0026rsquo;s how you can improve your digital resiliency for things like websites, messaging, and email.\nMessaging I recommend Signal because it is open source, end-to-end encrypted, cross-platform, and offers text, voice, video, and group chat. It\u0026rsquo;s usually very reliable; however, strange things can happen.\nIt\u0026rsquo;s important to set up a backup plan ahead of any service outages with the people you communicate with the most. Have an agreement for a secondary method of messaging \u0026ndash; ideally another end-to-end encrypted service. Avoid falling back on insecure communications like SMS and social media messaging. Here\u0026rsquo;s a short list for you to explore:\n Signal Wire for teams or Wire Personal Session  If you\u0026rsquo;re particularly technically inclined, you can set up your own self-hosted chat service with Matrix.\nHaving a go-to plan B can help bring peace of mind and ensure you\u0026rsquo;re still able to communicate when strange things happen.\nCloud contacts Do you know the phone numbers of your closest contacts? While memorizing them might not be practical, storing them solely online is an unnecessary risk. Most services allow you to export your contacts to vCard or CSV format.\nI recommend keeping your contacts locally on your device whenever you can. This ensures you still know how to contact people if your cloud provider is unavailable, or if you don\u0026rsquo;t have Internet access.\nFull analog redundancy is also possible here. Remember that paper stuff? Write down the phone numbers of your most important contacts so you can access them if your devices run out of battery or otherwise can\u0026rsquo;t turn on (drop your phone much?).\nLocal email synchronization If your email service exists solely online, there\u0026rsquo;s a big email-shaped hole in your life. If you can\u0026rsquo;t log in to your email for any reason \u0026ndash; an outage on their end, a billing error, or your Internet is down \u0026ndash; you\u0026rsquo;ll have no way to access your messages for however long your exile lasts. If you think about all the things you do via email in a day, I think the appropriate reaction to not having local copies is 🤦.\nDownload an open source email client like Thunderbird. Follow instructions to install Thunderbird and set it up with your existing online email service. Your online service provider may have a help document that shows you how to set up Thunderbird.\nYou can maximize your privacy by turning off Thunderbird\u0026rsquo;s telemetry.\nTo ensure that Thunderbird downloads your email messages and stores them locally on your machine:\n Click the \u0026ldquo;hamburger\u0026rdquo; overflow menu and go to Account Settings Choose Synchronization \u0026amp; Storage in the sidebar Ensure that under Message Synchronizing, the checkbox for Keep messages in all folders for this account on this computer is checked.  You may need to visit each of your folders in order to trigger the initial download.\nSome other settings you may want to update:\n Choose Composition \u0026amp; Addressing and uncheck the box next to Compose messages in HTML format to send plaintext emails instead. Under Return Receipts choose Global Preferences. Select the radio button for Never send a return receipt.  You don\u0026rsquo;t need to start using Thunderbird for all your email tasks. Just make sure you open it up regularly so that your messages sync and download to your machine.\nWebsites I strongly believe you should have your own independent website for reasons that go beyond redundancy. To truly make your site resilient, it\u0026rsquo;s important to have your own domain.\nIf you know that my website is at the address victoria.dev, for example, it doesn\u0026rsquo;t matter whether I\u0026rsquo;m hosting it on GitHub Pages, AWS, Wordpress, or from a server in my basement. If my hosting provider becomes unavailable, my website won\u0026rsquo;t go down with it. Getting back up and running would be as simple as updating my DNS configuration to point to a new host.\nPrice is hardly an excuse, either. You can buy a domain for less than a cup of coffee with my Namecheap affiliate link (thanks!). Namecheap also handles your DNS settings, so it\u0026rsquo;s a one-stop shop.\nWith your own domain, you can build resiliency for your email address as well. Learn how to set up your custom domain with your email provider. If you need to switch providers in the future, your email address ports to the new service with you. Here are a few quick links for providers I\u0026rsquo;d recommend:\n ProtonMail: How to connect a custom domain? Tutanota: Adding of custom email domains Fastmail: Custom Domains with Fastmail  Build your digital resiliency I hope you\u0026rsquo;ve found this article useful on your path to building digital resiliency. If you\u0026rsquo;re interested in more privacy topics, you might like to learn about the VPN I use or great apps for outsourcing security.\nIf your threat model includes anonymity or censorship, building digital resiliency is just a first step. The rest is outside the scope of my blog, but here are a few great resources I\u0026rsquo;ve come across:\n Tor Browser IntelTechniques Can\u0026rsquo;t Cancel Me Tails portable OS  ",url:"https://victoria.dev/blog/digital-resilience-redundancy-for-websites-and-communications/"},"https:\/\/victoria.dev\/blog\/create-a-self-hosted-chat-service-with-your-own-matrix-server\/":{title:"Create a self-hosted chat service with your own Matrix server",tags:["privacy","protocols","linux","aws","go","terminal"],content:"Matrix is an open standard for decentralized real-time communication. The specification is production-ready and bridges to tons of silo products like Slack, Gitter, Telegram, Discord, and even Facebook Messenger. This lets you use Matrix to link together disjoint communities in one place, or create an alternative communication method that works with, but is independent of, communication silos.\nYou can create your own self-hosted Matrix chat for as little as $3.50 USD per month on an AWS Lightsail instance. Your homeserver can federate with other Matrix servers, giving you a reliable and fault-tolerant means of communication.\nMatrix is most widely installed via its Synapse homeserver implementation written in Python 3. Dendrite, its second-generation homeserver implementation written in Go, is currently released in beta. Dendrite will provide more memory efficiency and reliability out-of-the-box, making it an excellent choice for running on a virtual instance.\nHere\u0026rsquo;s how to set up your own homeserver on AWS Lightsail with Dendrite. You can also contribute to Dendrite today.\nCreate a Lightsail instance Spin up a new Lightsail instance on AWS with Debian as your operating system. It\u0026rsquo;s a good idea to create a new per-instance key for use with SSH. You can do this by with the SSH key pair manager on the instance creation page. Don\u0026rsquo;t forget to download your private key and .gitignore your secrets.\nClick Create Instance. Wait for the status of your instance to change from Pending to Running, then click its name to see further information. You\u0026rsquo;ll need the Public IP address.\nTo enable people including yourself to connect to the instance, go to the Networking tab and add a firewall rule for HTTPS. This will open 443 so you can connect over IPv4. You can also do this for IPv6.\nConnect DNS Give your instance a catchier address by buying a domain at Namecheap and setting up DNS records.\n On your domain management page in the Nameservers section, choose Namecheap BasicDNS. On the Advanced DNS tab, click Add New Record.  Add an A Record to your Lightsail Public IP. You can use a subdomain if you want one, for example,\n Type: A Record Host: matrix Value: 13.59.251.229  This points matrix.example.org to your Lightsail instance.\nSet up your Matrix homeserver Change permissions on the private key you downloaded:\nchmod 600 \u0026lt;path/to/key\u0026gt; Then SSH to your Public IP:\nssh -i \u0026lt;path/to/key\u0026gt; admin@\u0026lt;public ip\u0026gt; Welcome to your instance! You can make it more interesting by downloading some packages you\u0026rsquo;ll need for Dendrite. It\u0026rsquo;s a good idea ot use apt for this, but first you\u0026rsquo;ll want to make sure you\u0026rsquo;re getting the latest stuff.\nChange your sources list in order to get the newest version of Go:\nsudo vim /etc/apt/sources.list Delete everything except these two lines:\ndeb http://cdn-aws.deb.debian.org/debian buster maindeb-src http://cdn-aws.deb.debian.org/debian buster mainThen replace the distributions:\n:%s/buster main/testing main contrib non-free/gRun sudo apt dist-upgrade. If you\u0026rsquo;re asked about modified configuration files, choose the option to \u0026ldquo;keep the local version currently installed.\u0026rdquo;\nOnce the upgrade is finished, restart your instance with sudo shutdown -r now.\nGo make some coffee, then SSH back in. Get the packages you\u0026rsquo;ll need with:\nsudo apt update sudo apt upgrade sudo apt install -y git golang nginx python3-certbot-nginx You\u0026rsquo;re ready to get Dendrite.\nGet Dendrite Clone Dendrite and follow the README instructions to get started. You\u0026rsquo;ll need to choose whether you want your Matrix instance to be federating. For simplicity, here\u0026rsquo;s how to set up a non-federating deployment to start:\ngit clone https://github.com/matrix-org/dendrite cd dendrite ./build.sh # Generate a Matrix signing key for federation (required) ./bin/generate-keys --private-key matrix_key.pem # Generate a self-signed certificate (optional, but a valid TLS certificate is normally # needed for Matrix federation/clients to work properly!) ./bin/generate-keys --tls-cert server.crt --tls-key server.key # Copy and modify the config file - you\u0026#39;ll need to set a server name and paths to the keys # at the very least, along with setting up the database connection strings. cp dendrite-config.yaml dendrite.yaml Configure Dendrite Modify the configuration file you just copied:\nsudo vim dendrite.yaml At minimum, set:\n server name to your shiny new domain name, e.g. matrix.example.org disable_federation to true or false registration_disabled to true or false  You might like to read the Dendrite FAQ.\nConfigure nginx Get the required packages if you didn\u0026rsquo;t already install them above:\nsudo apt install nginx python3-certbot-nginx Create your site\u0026rsquo;s configuration file under sites-available with:\ncd /etc/nginx/sites-available ln -s /etc/nginx/sites-available/\u0026lt;sitename\u0026gt; /etc/nginx/sites-enabled/\u0026lt;sitename\u0026gt; sudo cp default \u0026lt;sitename\u0026gt; Edit your site configuration. Delete the root and index lines if you don\u0026rsquo;t need them, and input your server name.\nYour location block should look like:\nlocation / { proxy_pass https://localhost:8448; } Remove the default with: sudo rm /etc/nginx/sites-enabled/default.\nCreate self-signed certificates You can use Certbot to generate self-signed certificates with Let\u0026rsquo;s Encrypt.\nsudo certbot --nginx -d \u0026lt;your.site.address\u0026gt; If you don\u0026rsquo;t want to give an email, add the --register-unsafely-without-email flag.\nTest your configuration and restart nginx with:\nsudo nginx -t sudo systemctl restart nginx Then start up your Matrix server.\n# Build and run the server: ./bin/dendrite-monolith-server --tls-cert server.crt --tls-key server.key --config dendrite.yaml Your Matrix server is up and running at your web address! If you disabled registration in your configuration, you may need to create a user. You can do this by running the included dendrite/bin/createuser.\nYou can log on to your new homeserver with any Matrix client, or Matrix-capable applications like Pidgin with the Matrix plugin.\nOther troubleshooting Log files If you get an error such as:\n... [github.com/matrix-org/dendrite/internal/log.go:155] setupFileHook Couldn\u0026#39;t create directory /var/log/dendrite: \u0026#34;mkdir /var/log/dendrite: permission denied\u0026#34; You\u0026rsquo;ll need to create a spot for your log files. Avoid the bad practice of running stuff with sudo whenever you can. Instead, create the necessary file with the right permissions:\nsudo mkdir /var/log/dendrite sudo chown admin:admin /var/log/dendrite # Build and run the server: ./bin/dendrite-monolith-server --tls-cert server.crt --tls-key server.key --config dendrite.yaml Unable to decrypt If you see: Unable to decrypt: The sender's device has not sent us the keys for this message. you may need to verify a user (sometimes yourself).\n In your client, open the user\u0026rsquo;s profile. Click the lock icon if there is one, or otherwise look for a way to verify them. You may be asked to see if some emojis presented to both users match if you\u0026rsquo;re using certain clients like Element. You can then re-request encryption keys for any sent messages.  Set up your own Matrix server today I hope you found this introduction to setting up your own Matrix homeserver to be helpful! If you have anything to add, feel free to reply via Webmention.\n",url:"https://victoria.dev/blog/create-a-self-hosted-chat-service-with-your-own-matrix-server/"},"https:\/\/victoria.dev\/blog\/do-i-raise-or-return-errors-in-python\/":{title:"Do I raise or return errors in Python?",tags:["python","coding","tech-team"],content:"I hear this question a lot: \u0026ldquo;Do I raise or return this error in Python?\u0026rdquo;\nThis is probably because the right answer will depend on your situation and the goals of your application logic. Either choice can help you ensure your Python code doesn\u0026rsquo;t fail silently, saving you and your teammates from having to hunt down deeply entrenched errors.\nHere\u0026rsquo;s the difference between raise and return when handling failures in Python, and how to ensure your code doesn\u0026rsquo;t fail silently.\nWhen to raise  The raise statement allows the programmer to force a specific exception to occur. (8.4 Raising Exceptions)\n Use raise when you know you want a specific behavior, such as:\nraise TypeError(\u0026#34;Wanted strawberry, got grape.\u0026#34;) Raising an exception terminates the flow of your program, allowing the exception to bubble up the call stack. In the above example, this would let you explicitly handle TypeError later. If TypeError goes unhandled, code execution stops and you\u0026rsquo;ll get an unhandled exception message.\nRaise is useful in cases where you want to define a certain behavior to occur. For example, you may choose to disallow certain words in a text field:\nif \u0026#34;raisins\u0026#34; in text_field: raise ValueError(\u0026#34;That word is not allowed here\u0026#34;) Raise takes an instance of an exception, or a derivative of the Exception class. Here are all of Python\u0026rsquo;s built-in exceptions.\nRaise can help you avoid writing functions that fail silently. For example, this code will not raise an exception if JAM doesn\u0026rsquo;t exist:\nimport os def sandwich_or_bust(bread: str) -\u0026gt; str: jam = os.getenv(\u0026#34;JAM\u0026#34;) return bread + str(jam) + bread s = sandwich_or_bust(\u0026#34;\\U0001F35E\u0026#34;) print(s) # Prints \u0026#34;🍞None🍞\u0026#34; which is not very tasty. To cause the sandwich_or_bust() function to actually bust, add a raise:\nimport os def sandwich_or_bust(bread: str) -\u0026gt; str: jam = os.getenv(\u0026#34;JAM\u0026#34;) if not jam: raise ValueError(\u0026#34;There is no jam. Sad bread.\u0026#34;) return bread + str(jam) + bread s = sandwich_or_bust(\u0026#34;\\U0001F35E\u0026#34;) print(s) # ValueError: There is no jam. Sad bread. Any time your code interacts with an external variable, module, or service, there is a possibility of failure. You can use raise in an if statement to help ensure those failures aren\u0026rsquo;t silent.\nRaise in try and except To handle a possible failure by taking an action if there is one, use a try \u0026hellip; except statement.\ntry: s = sandwich_or_bust(\u0026#34;\\U0001F35E\u0026#34;) print(s) except ValueError: buy_more_jam() raise This lets you buy_more_jam() before re-raising the exception. If you want to propagate a caught exception, use raise without arguments to avoid possible loss of the stack trace.\nIf you don\u0026rsquo;t know that the exception will be a ValueError, you can also use a bare except: or catch any derivative of the Exception class with except Exception:. Whenever possible, it\u0026rsquo;s better to raise and handle exceptions explicitly.\nUse else for code to execute if the try does not raise an exception. For example:\ntry: s = sandwich_or_bust(\u0026#34;\\U0001F35E\u0026#34;) print(s) except ValueError: buy_more_jam() raise else: print(\u0026#34;Congratulations on your sandwich.\u0026#34;) You could also place the print line within the try block, however, this is less explicit.\nWhen to return When you use return in Python, you\u0026rsquo;re giving back a value. A function returns to the location it was called from.\nWhile it\u0026rsquo;s more idiomatic to raise errors in Python, there may be occasions where you find return to be more applicable.\nFor example, if your Python code is interacting with other components that do not handle exception classes, you may want to return a message instead. Here\u0026rsquo;s an example using a try \u0026hellip; except statement:\nfrom typing import Union def share_sandwich(sandwich: int) -\u0026gt; Union[float, Exception]: try: bad_math = sandwich / 0 return bad_math except Exception as e: return e s = share_sandwich(1) print(s) # Prints \u0026#34;division by zero\u0026#34; Note that when you return an Exception class object, you\u0026rsquo;ll get a representation of its associated value, usually the first item in its list of arguments. In the example above, this is the string explanation of the exception. In some cases, it may be a tuple with other information about the exception.\nYou may also use return to give a specific error object, such as with HttpResponseNotFound in Django. For example, you may want to return a 404 instead of a 403 for security reasons:\nif object.owner != request.user: return HttpResponseNotFound Using return can help you write appropriately noisy code when your function is expected to give back a certain value, and when interacting with outside elements.\nMake your errors noisy Silent failures create some of the most frustrating bugs to find and fix. You can help create a pleasant development experience for yourself and your team by using raise and return to handle errors in Python.\n",url:"https://victoria.dev/blog/do-i-raise-or-return-errors-in-python/"},"https:\/\/victoria.dev\/blog\/what-tech-leaders-do-before-going-on-vacation\/":{title:"What tech leaders do before going on vacation",tags:["tech-team","leadership"],content:"As a technical person who leads a technical team, I know firsthand that it can be easy to get lost in finishing up your own work before a vacation. It takes a bit of dedicated attention to ensure you don\u0026rsquo;t neglect the day-to-day tasks that don’t vanish while you’re away.\nHere’s a pre-PTO checklist to make sure you’ve taken care of those responsibilities before you take off for a much-deserved vacation.\nDoes everyone know who\u0026rsquo;s supposed to do what? A lot of information lives in your head alone, despite valiant efforts to document all the things. That’s the nature of the work. Instead of attempting to disseminate everything you’re thinking through, focus on the work your team will do while you’re away.\nHopefully, you already have a centralized list of priorities. Ensure it’s up to date, and that all the tasks that could conceivably be done during your time off have been assigned a caretaker.\nAre there any decisions waiting on me? Review notes, discussion boards, and ask your team directly if anyone is waiting on an answer from you. If you’re able to make a decision before your vacation, do so. If not, delegate the decision to someone else with a clear explanation of your overall goals and any applicable parameters.\nDon’t neglect internal auto-responders Set up auto-responders for communications from the public, as well as from your team! While a response from your public email may say that you’re out of the office and when you expect to be back, an internal auto-responder is an opportunity to provide even more value.\nIt could take the form of an email response for your internal inbox, but a post on your team’s message board or chat channel also works. Let people know where to look for information they might need, who to turn to if they need help while you’re away, and where to find your centralized priority list so they can decide what to work on next.\nTidy up loose ends Finally, close out work that depends on you and that no one else can do. If it’s work you can delegate with some written guidance attached, you might choose this route instead of attempting to finish it yourself in a hurry.\nIf you hand off any work, ensure that you communicate clear instructions as well as any deadlines.\nSet expectations and follow them Your vacation is precious time to recoup, relax, and make room for those creative moments that only visit a quiet mind. Ensure you set expectations with your team for how often you might check in, if at all.\nIf you do decide to log in while you\u0026rsquo;re away, be sure to protect your privacy and your company data with a privacy-focused VPN on all your devices.\nResponsible vacation planning for technology leaders I hope you benefit from the ideas in this post. Here\u0026rsquo;s an easy way to put them into practice right now: grab this post as a pre-PTO Notion checklist template and instantly gain conscientious leadership powers!\n Get checklist as a Notion template\nDon\u0026rsquo;t leave your team members hanging while you\u0026rsquo;re away! If you have other ideas for good things to do before your vacation, I\u0026rsquo;d love to hear about it.\n",url:"https://victoria.dev/blog/what-tech-leaders-do-before-going-on-vacation/"},"https:\/\/victoria.dev\/neofeed\/49cf5aab-af5f-4c0e-aa40-36a7afca2bfb\/":{title:"49cf5aab-af5f-4c0e-aa40-36a7afca2bfb",tags:[],content:"Why do whiteboard interviews not work? Wrong answers only.\n",url:"https://victoria.dev/neofeed/49cf5aab-af5f-4c0e-aa40-36a7afca2bfb/"},"https:\/\/victoria.dev\/blog\/add-search-to-hugo-static-sites-with-lunr\/":{title:"Add search to Hugo static sites with Lunr",tags:["websites","coding","data","javascript"],content:"Yes, you can have an interactive search feature on your static site! No need for servers or paid subscriptions here. Thanks to the open source Lunr and the power of Hugo static site generator, you can create a client-side search index with just a template and some JavaScript.\nA number of my readers have been kind enough to tell me that you find my blog useful, but there\u0026rsquo;s something that you don\u0026rsquo;t know. Up until I recently implemented a search feature on victoria.dev, I had been my own unhappiest user.\nMy blog exists for all to read, but it\u0026rsquo;s also my own personal Internet brain. I frequently pull up a post I\u0026rsquo;ve written when trying to re-discover some bit of knowledge that I may have had the foresight to record. Without a search, finding it again took a few clicks and more than a few guesses. Now, all my previous discoveries are conveniently at my fingertips, ready to be rolled into even more future work.\nIf you\u0026rsquo;d like to make your own personal Internet brain more useful, here\u0026rsquo;s how you can implement your own search feature on your static Hugo site.\nGet Lunr While you can install lunr.js via npm or include it from a CDN, I chose to vendorize it to minimize network impact. This means I host it from my own site files by placing the library in Hugo\u0026rsquo;s static directory.\nYou can save your visitors some bandwidth by minifying lunr.js, which I did just by downloading lunr.js from source and using the JS \u0026amp; CSS Minifier Visual Studio Code extension on the file. That brought the size down roughly 60% from 97.5 KB to 39.35 KB.\nSave this as static/js/lunr.min.js.\nCreate a search form partial To easily place your search form wherever you like on your site, create the form as a partial template at layouts/partials/search-form.html\n\u0026lt;form id=\u0026#34;search\u0026#34; action=\u0026#39;{{ with .GetPage \u0026#34;/search\u0026#34; }}{{.Permalink}}{{end}}\u0026#39; method=\u0026#34;get\u0026#34;\u0026gt; \u0026lt;label hidden for=\u0026#34;search-input\u0026#34;\u0026gt;Search site\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;search-input\u0026#34; name=\u0026#34;query\u0026#34; placeholder=\u0026#34;Type here to search\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;search\u0026#34;\u0026gt; \u0026lt;/form\u0026gt; Include your search form in other templates with:\n{{ partial \u0026#34;search-form.html\u0026#34; . }} Create a search page For your search to be useful, you\u0026rsquo;ll need a way to trigger one. You can create a (static!) /search page that responds to a GET request, runs your search, and displays results.\nHere\u0026rsquo;s how to create a Hugo template file for a search page and get it to render.\nCreate layouts/search/list.html with the following minimum markup, assuming you\u0026rsquo;re inheriting from a base template:\n{{ define \u0026#34;main\u0026#34; }} {{ partial \u0026#34;search-form.html\u0026#34; . }} \u0026lt;ul id=\u0026#34;results\u0026#34;\u0026gt; \u0026lt;li\u0026gt; Enter a keyword above to search this site. \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; {{ end }} In order to get Hugo to render the template, a matching content file must be available. Create content/search/_index.md to satisfy this requirement. The file just needs minimal front matter to render:\n--- title: Search me! --- You can run hugo serve and navigate to /search to see if everything builds as expected.\nA few libraries exist to help you build a search index and implement Lunr. You can find them here on the Hugo site. If you want to fully understand the process, however, you\u0026rsquo;ll find it\u0026rsquo;s not complicated do this without additional dependencies, thanks to the power of Hugo\u0026rsquo;s static site processing.\nBuild your search index Here\u0026rsquo;s how to build an index for Lunr to search using Hugo\u0026rsquo;s template rendering power. Use range to loop over the pages you want to make searchable, and capture your desired parameters in an array of documents. One way to do this is to create layouts/partials/search-index.html with:\n\u0026lt;script\u0026gt; window.store = { // You can specify your blog section only:  {{ range where .Site.Pages \u0026#34;Section\u0026#34; \u0026#34;blog\u0026#34; }} // For all pages in your site, use \u0026#34;range .Site.Pages\u0026#34;  // You can use any unique identifier here  \u0026#34;{{ .Permalink }}\u0026#34;: { // You can customize your searchable fields using any .Page parameters  \u0026#34;title\u0026#34;: \u0026#34;{{ .Title }}\u0026#34;, \u0026#34;tags\u0026#34;: [{{ range .Params.Tags }}\u0026#34;{{ . }}\u0026#34;,{{ end }}], \u0026#34;content\u0026#34;: {{ .Content | plainify }}, // Strip out HTML tags  \u0026#34;url\u0026#34;: \u0026#34;{{ .Permalink }}\u0026#34; }, {{ end }} } \u0026lt;/script\u0026gt; \u0026lt;!-- Include Lunr and code for your search function, which you\u0026#39;ll write in the next section --\u0026gt; \u0026lt;script src=\u0026#34;/js/lunr.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;/js/search.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; When Hugo renders your site, it will build your search index in much the same way as a List page is built, creating a document for each page with its parameters.\nThe last piece of the puzzle is the code to handle the search process: taking the search query, getting Lunr to perform the search, and displaying the results.\nPerform the search and show results Create static/js/search.js to hold the JavaScript that ties it all together. This file has three main tasks: get the search query, perform the search with Lunr, and display the results.\nGet query parameters with JavaScript This part\u0026rsquo;s straightforward thanks to URLSearchParams:\nconst params = new URLSearchParams(window.location.search) const query = params.get(\u0026#39;q\u0026#39;) Search for the query with Lunr Define and configure an index for Lunr. This tells Lunr what you\u0026rsquo;d like to search with, and you can optionally boost elements that are more important.\nconst idx = lunr(function () { // Search these fields  this.ref(\u0026#39;id\u0026#39;) this.field(\u0026#39;title\u0026#39;, { boost: 15 }) this.field(\u0026#39;tags\u0026#39;) this.field(\u0026#39;content\u0026#39;, { boost: 10 }) // Add the documents from your search index to  // provide the data to idx  for (const key in window.store) { this.add({ id: key, title: window.store[key].title, tags: window.store[key].category, content: window.store[key].content }) } }) You can then execute the search and store results with:\nconst results = idx.search(query) Display results You\u0026rsquo;ll need a function that builds a list of results and displays them on your search page. Recall the id you gave your ul element in layouts/search/list.html and store it as a variable:\nconst searchResults = document.getElementById(\u0026#39;results\u0026#39;) If a search results in some results (🥁), you can iterate over them and build a \u0026lt;li\u0026gt; element for each one.\nif (results.length) { // Length greater than 0 is truthy  let resultList = \u0026#39;\u0026#39; for (const n in results) { // Use the unique ref from the results list to get the full item  // so you can build its \u0026lt;li\u0026gt;  const item = store[results[n].ref] resultList += \u0026#39;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;\u0026#39; + item.url + \u0026#39;\u0026#34;\u0026gt;\u0026#39; + item.title + \u0026#39;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026#39; // Add a short clip of the content  resultList += \u0026#39;\u0026lt;p\u0026gt;\u0026#39; + item.content.substring(0, 150) + \u0026#39;...\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026#39; } searchResults.innerHTML = resultList } For each of your results, this produces a list item similar to:\n\u0026lt;li\u0026gt; \u0026lt;p\u0026gt; \u0026lt;a href=\u0026#34;.../blog/add-search-to-hugo-with-lunr/\u0026#34;\u0026gt; Add search to Hugo static sites with Lunr \u0026lt;/a\u0026gt; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Yes, you can have an interactive search feature on your static site!...\u0026lt;/p\u0026gt; \u0026lt;/li\u0026gt; If there are no results, ham-handedly insert a message instead.\nelse { searchResults.innerHTML = \u0026#39;No results found.\u0026#39; } Full code for search.js Here\u0026rsquo;s what static/js/search.js could look like in full.\n search.js full code function displayResults (results, store) { const searchResults = document.getElementById(\u0026#39;results\u0026#39;) if (results.length) { let resultList = \u0026#39;\u0026#39; // Iterate and build result list elements  for (const n in results) { const item = store[results[n].ref] resultList += \u0026#39;\u0026lt;li\u0026gt;\u0026lt;p\u0026gt;\u0026lt;a href=\u0026#34;\u0026#39; + item.url + \u0026#39;\u0026#34;\u0026gt;\u0026#39; + item.title + \u0026#39;\u0026lt;/a\u0026gt;\u0026lt;/p\u0026gt;\u0026#39; resultList += \u0026#39;\u0026lt;p\u0026gt;\u0026#39; + item.content.substring(0, 150) + \u0026#39;...\u0026lt;/p\u0026gt;\u0026lt;/li\u0026gt;\u0026#39; } searchResults.innerHTML = resultList } else { searchResults.innerHTML = \u0026#39;No results found.\u0026#39; } } // Get the query parameter(s) const params = new URLSearchParams(window.location.search) const query = params.get(\u0026#39;query\u0026#39;) // Perform a search if there is a query if (query) { // Retain the search input in the form when displaying results  document.getElementById(\u0026#39;search-input\u0026#39;).setAttribute(\u0026#39;value\u0026#39;, query) const idx = lunr(function () { this.ref(\u0026#39;id\u0026#39;) this.field(\u0026#39;title\u0026#39;, { boost: 15 }) this.field(\u0026#39;tags\u0026#39;) this.field(\u0026#39;content\u0026#39;, { boost: 10 }) for (const key in window.store) { this.add({ id: key, title: window.store[key].title, tags: window.store[key].category, content: window.store[key].content }) } }) // Perform the search  const results = idx.search(query) // Update the list with results  displayResults(results, window.store) }  Make it go You now have Lunr, the search index, and the code that displays results. Since these are all included in layouts/partials/search-index.html, add this partial on all pages with a search form. In your page footer, place:\n{{ partial \u0026#34;search-index.html\u0026#34; . }} You can see what this looks like when it\u0026rsquo;s all put together by trying it out on my blog.\nMake it go faster Since your site is static, it\u0026rsquo;s possible to pre-build your search index as a JSON data file for Lunr to load. This is where those aforementioned libraries may be helpful, since a JSON-formatted search index would need to be built outside of running hugo to generate your site.\nYou can maximize your search speed by minifying assets, and minimizing computationally expensive or blocking JavaScript in your code.\nStatic sites get search, too! I hope this helps you make your Internet brain more useful for yourself and others, too! Don\u0026rsquo;t worry if you haven\u0026rsquo;t got the time to implement a search feature today \u0026ndash; you can find this tutorial again when you visit victoria.dev and search for this post! 🥁\n",url:"https://victoria.dev/blog/add-search-to-hugo-static-sites-with-lunr/"},"https:\/\/victoria.dev\/neofeed\/bb1f25ba-d5d5-4a81-982d-0c1689edc946\/":{title:"bb1f25ba-d5d5-4a81-982d-0c1689edc946",tags:[],content:"Want your own website but not sure how to start? Read this for options for every level of coder, from no-code to fullstack and completely free to paid.\nNo code, free to paid You can use a WYSIWYG (What You See Is What You Get) site builder like Carrd to create a simple single page site, like a list of links to some of your projects or a few of your portfolio pieces. It\u0026rsquo;s dirt cheap at just $ for a year.\nFor a more feature-rich site with content, like a blog or full portfolio, try Webflow. Start for free and upgrade to a paid plan when it makes sense. Even if you pay for just one month, you can then export your HTML, CSS, and JavaScript so you\u0026rsquo;ll own your own code and can host it anywhere.\n The absolute beginner’s guide to Webflow  Learning to code, free Just starting your coding journey? You can use GitHub Pages and the Jekyll Theme Chooser if you want to own all your content files, for free. Write Markdown files, put them in a GitHub repository, then simply choose a few settings to build and host your site with GitHub Pages. As you learn to use Git and write your own code, your site\u0026rsquo;s GitHub repository grows with you.\n Getting Started with GitHub Pages  HTML + command line, free or paid Know how to open a terminal but don\u0026rsquo;t necessarily have great design chops? Use a static site generator like Hugo with a beautiful pre-made theme from the open source community. Creating a feature-rich site is as easy as making folders with Markdown content files and choosing configuration options in your text editor. As you grow your web dev skills, all your source code is at your fingertips to modify and extend as you please.\n Getting Started with Hugo  Working developer, but lazy You know how to code you own site, but for some reason just haven\u0026rsquo;t gotten around to it. Consider this your kick in the butt! Start with a static site generator like Hugo, Jekyll, or Eleventy to take most of the mundane workload off your hands. Get right to the fun stuff like creating your site pages, blog, or modifying a theme.\nOnce you\u0026rsquo;re ready to start exploring IndieWeb technologies, use services like Webmention.io and Bridgy to easily bring social interactions to your own home on the web, just like on this page.\nBut definitely get your own domain Whatever route you take, buy your own domain name \u0026ndash; and use it! That way, even if you change hosting services, technologies, or even registrars, folks will always know where to find your home on the web. Get one from Namecheap via my referral link if you\u0026rsquo;ve appreciated this post.\n",url:"https://victoria.dev/neofeed/bb1f25ba-d5d5-4a81-982d-0c1689edc946/"},"https:\/\/victoria.dev\/neofeed\/492f0cbc-257a-41a2-8a2a-3afe195c8c83\/":{title:"492f0cbc-257a-41a2-8a2a-3afe195c8c83",tags:[],content:"Help me discover more awesome indie webmasters! 😄 @ me if you or your favorite blog supports webmentions!\n",url:"https://victoria.dev/neofeed/492f0cbc-257a-41a2-8a2a-3afe195c8c83/"},"https:\/\/victoria.dev\/blog\/make-your-own-independent-website\/":{title:"Make your own independent website",tags:["websites","life"],content:"The web that raised me was a digital playground in the truest sense. It was made up of HTML experiments frankensteined together by people still figuring it all out.\nThe beauty of not completely knowing what you\u0026rsquo;re doing is a lack of premature judgement. Without a standard to rise to, you\u0026rsquo;re free to go sideways. Explore. Try things that don\u0026rsquo;t work, without any expectation they will work. An open world with a beginner\u0026rsquo;s mindset.\nThe web that raised me was a little broken. Things didn\u0026rsquo;t always display the way they were supposed to. That too is part of the beauty. It was just broken enough to make you think for yourself.\n1991 was the year of the individual on the web, the first year any layperson could open a web browser and access the new hypermedia dimension. There were no go-to, search-suggested, centralized websites. There were newsgroups. You had what you made and what your meatspace contacts sent you. In 2021, I think we need a return to that level of individualism. We need to make 2021 the year of the independent web.\nThat\u0026rsquo;s not to say I think the massive monopolistic platforms are going anywhere. Twitter, Facebook, mainstream \u0026ldquo;news\u0026rdquo; media sites \u0026ndash; they\u0026rsquo;re all a kind of utility now, like plumbing and electricity. They\u0026rsquo;ll find their place in regulation and history. But they are not your website.\nYour website is the one you create. Where the content, top-to-bottom, is yours alone to shape and present as you please. Your website is your place of self-expression, without follower counts or statistics to game. Your website is for creation, not reaction.\nIt\u0026rsquo;s all yours, but it doesn\u0026rsquo;t have to seem lonely. Your site can interact with the entire online world through syndication and protocols made possible by this thing we call the Internet. See:\n IndieWeb for POSSE, an abbreviation for Publish (on your) Own Site, Syndicate Elsewhere Webmention and an easy way to implement them  twtxt instances for a decentralized timeline experience Neofeed , my personal timeline project made for Neocities . (It\u0026rsquo;s open source and you can help me extend it! )  Your website is your beginning point. The one source of truth for your identity online, from which you can generate and distribute disposable copies to any platform you please. This is what it means to truly own your content. And on the Internet, your content is you.\nThis is my website. When I first created it, I did so for myself. I had no expectation of visitors. I just knew I\u0026rsquo;d rather have these thoughts and things I\u0026rsquo;ve learned here, out here, made indelible in the folds of the public Internet, instead of on some dark corner of my machine, to be lost forever once I am.\nMake your own website. You\u0026rsquo;ll grow your own sense of well-deserved accomplishment and contribute to your independence on the web. You\u0026rsquo;ll learn by doing, by scratching your own itch.\nLearn about web technologies. Use them as you would if you were a child holding a pencil or paintbrush for the first time. Experiment, with no expectations other than discovering what you can do to make it delight you.\n These sites and articles inspired this post and helped me implement webmentions!\n Why the Indie Web movement is so important, Dan Gillmor  Jamie Tanna  Max Böck  Microblogging, Paul Robert Lloyd  Zachary Dunn  Adding Webmentions to My Static Hugo Site, Ana Ulin  Adding Webmention Support to a Static Site, Keith J Grant  Webmention.rocks   ",url:"https://victoria.dev/blog/make-your-own-independent-website/"},"https:\/\/victoria.dev\/neofeed\/1350114283972124674\/":{title:"1350114283972124674",tags:[],content:"Just a friendly Friday reminder to write good docs. 👇\nHow to write good documentation\n",url:"https://victoria.dev/neofeed/1350114283972124674/"},"https:\/\/victoria.dev\/neofeed\/1350104957752549376\/":{title:"1350104957752549376",tags:[],content:"What would you keep track of in a personal CHANGELOG?\n",url:"https://victoria.dev/neofeed/1350104957752549376/"},"https:\/\/victoria.dev\/blog\/how-to-get-hired-as-a-software-developer\/":{title:"How to get hired as a software developer",tags:["life","coding","open-source"],content:"I\u0026rsquo;m asked this question a lot, so let me be the first to give you the good news: there\u0026rsquo;s no one right answer. As general tech-literacy increases, the culture of the coding industry is steadily, thankfully, moving away from a checklist approach. Instead of degrees and pre-requisites when it comes to deciding whether you\u0026rsquo;re qualified to be hired as a software developer, companies (including my own) are far more concerned with just one question. What can you do?\nThere are some general best practices that will make you a far more attractive hire than the majority of applicants, and I\u0026rsquo;ll discuss those in this post. For the most part, however, demonstrating what you\u0026rsquo;re capable of is the best way to increase your chances of getting to the interview and beyond. Here\u0026rsquo;s how to get hired as a software developer.\nFirst, build projects Companies who are primarily focused on getting products built want to see that you\u0026rsquo;ve built products. They don\u0026rsquo;t need to be flashy or for-profit, but they do need to work. I\u0026rsquo;m far more likely to consider a candidate with a colorful bouquet of working code in their GitHub or GitLab or CodePen portfolio. Here are some basic ideas to get you started:\n Command line utilities that help with tedious tasks, like renaming image files Themes for static site generators like Hugo or Jekyll Automation tools, such as for GitHub Actions  The best projects you could showcase are ones directly related to the specialty you want to apply for. Show that you have competency with the fundamentals. For instance, if you see yourself focusing on front end, demonstrate that you can build interactive web pages with no fancier tools than HTML, CSS, and vanilla JS. For back end focused developers, show that you know how to create a fundamental tool like an API for a service on a local server. Want to be well-rounded? Create an API with a web page UI.\nSpend some time creating a good README. Use screenshots, highlight code snippets, include well-written instructions for local set up. Show that you care about and fully understand your own work.\nExplore specific frameworks and libraries if they interest you, but keep in mind that those won\u0026rsquo;t be interesting to a company unless the company already wants to use that framework.\nYou maximize your chances of getting hired by demonstrating that you already have the ability to learn on your own, build, and then present projects to the world. Out of everything else in this article, this is the one fundamental trait that a company won\u0026rsquo;t want to have to teach you.\nNext, stand out Familiarity with the following topics, along with demonstrating that understanding in your own code, will put you miles ahead of most applicants.\nReusable code Companies that build products are concerned with getting the most bang for their buck. This is the same idea as wanting to save yourself time when you\u0026rsquo;re creating something individually. If you put a week of effort into building something, it would be nice if you could keep easily using it for a long time afterwards. How can you maximize the return on your efforts?\nBe familiar with DRY code. Avoid creating highly customized pieces that only fit a particular use case, peppered with hard-coded variables and dependent on a particular input structure. Avoid writing code that is hard to update in the future. Recognize when you\u0026rsquo;re writing a script or library that could apply in many different situations, and understand how to turn it into a reusable module.\nTypes and mutability Besides building projects, debugging them can be a company\u0026rsquo;s most expensive task. It takes a lot of time to hunt down and fix bugs, but you can help reduce that cost by understanding the subtler ways that a lot of bugs occur. Understanding types and mutability \u0026ndash; whether and how an object can be changed \u0026ndash; can help open the door to even greater technical proficiency.\nGet familiar with at least one type system. If there\u0026rsquo;s a linter available for your language, use it. Understand how immutable and mutable objects work in the language you use. Be able to describe specific use cases for either one. Understand at a general level how mutability impacts system resources, the difference between referencing and copying, and what it means to be thread-safe.\nFollow-on effects Keep in mind that organizations are made up of people. When you work together with colleagues, your work has an effect on someone else\u0026rsquo;s. Being aware of these effects and demonstrating conscientiousness in this area will help show potential employers that you\u0026rsquo;d benefit the team as a whole.\nConsider the second and third-order effects of code you write. Avoid writing code that will unnecessarily slow down a larger system. Understand what blocking means and how to use concurrency or parallelism in your project. Include your thoughts on follow-on effects in your READMEs. Show that you always have the larger project, effort, costs, or organization in mind.\nOther nice-to-haves If you\u0026rsquo;ve fully taken advantage of the points above, you\u0026rsquo;re most of the way to getting hired already. Seal the deal with these easy wins.\nBe a friendly open source participant There\u0026rsquo;s no better way to show a potential employer you can work well on a team than by providing plenty of examples. Get involved with open source projects, contribute features and fixes, and interact with contributors and maintainers. Create a real-life simulation for your future colleagues that leaves no doubt about what you\u0026rsquo;d be like to work with. The further back this history goes, the better, so start right away.\nCommunicate with care If you\u0026rsquo;re participating in open source or working remotely, most of your communication with your colleagues is going to take place online in text. Without facial expressions, tone or inflection, this form of communication leaves a lot to be desired. Some extra care on your part can help make sure your message always comes across as intended.\nGet into a habit of drafting most everything you write, especially for long-form communication. Putting yourself in the mindset of creating a draft first lets you take all the time you need to craft your message. You can make sure you\u0026rsquo;re choosing appropriate words and coming across with the emotions you intend to convey. Feeling hurried? Remember the golden rule of online communication: you never need to reply right away. Take a breath, then take your time.\nMy list of things I use has some great apps I use for drafting. While some people may disagree with me, I say: use emojis liberally. 🤩\nFinally, use your imagination Software developers are creative people by necessity. Before you can write code, build a project, or design a page, you first have to be able to imagine it! So put that skill to good use.\nIn every application, every email, every chat message with your potential employer, imagine yourself in their position. What do they care about right now? What current goals does the company have? What information about yourself can you share that would make them feel comfortable hiring you?\nTake your best guess, and then ask if you got it right. \u0026ldquo;I think the company is looking for someone to [insert guess here], is that accurate?\u0026rdquo; Show that you have both the capability to anticipate future needs and the desire to identify and solve them.\nGet yourself hired Admittedly, this post is my own wishlist. Good candidates for software development positions are hard to come by, and people who can rightly say they do everything above are rare. I don\u0026rsquo;t think the discrepancy is due to a lack of ability; perhaps just a lack of information.\nI\u0026rsquo;ve seen both sides of the virtual interview table, and this post is a result of me figuring things out the long and circuitous way. I hope this helps you to take a more direct route to getting yourself hired as a software developer.\n",url:"https://victoria.dev/blog/how-to-get-hired-as-a-software-developer/"},"https:\/\/victoria.dev\/neofeed\/a7051260-cff3-426c-bc90-7701469703a8\/":{title:"a7051260-cff3-426c-bc90-7701469703a8",tags:[],content:"When choosing your new product name, you can save yourself a lot of grief by asking, \u0026ldquo;What would this sound like if someone verbed it?\u0026rdquo;\n",url:"https://victoria.dev/neofeed/a7051260-cff3-426c-bc90-7701469703a8/"},"https:\/\/victoria.dev\/neofeed\/08c89b39-ee40-43ef-a9fd-5035bba76bb9\/":{title:"08c89b39-ee40-43ef-a9fd-5035bba76bb9",tags:[],content:"If you\u0026rsquo;re trying to turn on GitHub Pages for a Hugo theme repository, the configuration files in your theme might make GitHub Pages (Jekyll, really) confused. I successfully got GitHub Pages to serve the site using the README.md after adding a _config.yml file for Jekyll with:\nexclude:[\u0026#34;exampleSite\u0026#34;,\u0026#34;theme.toml\u0026#34;]",url:"https://victoria.dev/neofeed/08c89b39-ee40-43ef-a9fd-5035bba76bb9/"},"https:\/\/victoria.dev\/blog\/how-to-become-a-software-developer\/":{title:"How to become a software developer",tags:["coding","life"],content:"As a Director of Engineering, I’m a software developer who hires and leads other software developers. It’s not surprising then that I get asked this question a lot, in various forms:\n How do I become a software developer? What language or framework should I learn first? Where do I start?  While I’m certain there’s no one right answer for everyone, I’m also certain that the world needs more software developers and systems thinkers.\nThe best thing I can do to help you lead yourself, learn to code, and become a software developer is to share the most efficient parts of how I did it myself. This is the article I wish I had read when I started coding.\nDepth matters Software is exceedingly complex. Like a good novel that you wish you’d never finish reading, there’s always more to discover and learn. If you don’t want to miss the best parts, don’t be satisfied with surface-level explanations. Always go deeper! Ask why, why, and why again until you get to the fundamentals. Soon enough, you’ll start to see patterns.\nBy digging deeper, you’ll begin to understand the fundamentals of how things connect, what makes things “fast,” and facets of software operation that you probably can’t even imagine exist. It’s like peeking behind the curtain and seeing a whole world of systems and processes that most people are never aware of.\nGoing in-depth can expand your mind and your capacity for learning. Keep asking why. Follow every link. Let your curiosity guide you.\nHard stuff matters Giving yourself the chance to be delighted through discovery doesn’t come for free. It takes a lot of hard work to read and compress complicated ideas into your meat brain.\nIt’s important not to gloss over the hard stuff. In fact, if something seems too hard to understand, you might benefit from doing it first. You might have to get creative to find ways to explain things to yourself, but when you succeed, it makes everything else easier later on.\nAnalogies are helpful for understanding hard concepts, but they’ll only help you start to understand concepts at a surface level. Remember to go in-depth. Don\u0026rsquo;t stop at the analogy.\nWriting matters Write right away. Create a habit of explaining everything you learn to yourself in long-form writing. Better than bullet points, writing with a conversational tone engages parts of your brain that help you to process and remember new information. It’s why humans like and remember stories, and it’s a superpower you get for free.\nStart by writing for yourself. Write about what interests you. Try something new, even if it seems rudimentary, and write in-depth about what you learn. (One of my most popular posts is about iteration in Python. When I first wrote it, I considered myself a complete beginner.)\nIf you want to go a step further, share your writing with the world. Learn in public, like I do. I often get questions like, \u0026ldquo;how do I choose a theme for my blog?\u0026rdquo; or \u0026ldquo;what platform should I use?\u0026rdquo; or \u0026ldquo;what popular language/framework/topic should I focus on?\u0026rdquo; My answer is: don\u0026rsquo;t worry about it.\nDon\u0026rsquo;t fret too much about your blog theme or platform. Pick the easiest option for you to get started with for now. All of that will change and improve as you learn, practice, and find your focus. Just start writing, ideally, yesterday.\nWrite for yourself by explaining what you’re doing, as if it were past-you teaching future-you — because it is. You will be your first reader, and the first judge of how useful your blog can be. Seek to impress yourself!\nThe language, framework, or version doesn’t matter Why pigeonhole your abilities before you even start? Pick any software language, framework, or technology that seems to make sense to you when you first read it. Start there.\nRemember that it’s important to dig deep and understand the fundamentals. Basic concepts of software transcend languages. Whichever first language you choose, understand functions, variables, return values, iteration, and how immutability works. You’ll find that learning these concepts will make it easier to recognize them in your second language, and learn that too.\nYour portfolio doesn’t matter If your first objective is to build a portfolio, you may be trying to run before you walk. Building a portfolio to showcase to potential employers is a great goal, but a terrible first step.\nIf you think of creating a polished portfolio as a first step, you’re liable to spend too much time making it pretty and presentable before focusing on the content. As someone who hires software developers, I can tell you wholeheartedly that I’d rather see clean and well-written code than a flashy front page.\nDon’t confuse building a portfolio with building projects. Absolutely build projects, right from the beginning. There’s no better way to see the practical application of what you’re learning. Just treat them as first drafts, as training ground, and don’t worry about packaging them up for professional consumption.\nBy allowing yourself to build some draft projects first, you allow yourself the breathing room to learn from them. Focus on iteration, on making one small thing better each time, and you\u0026rsquo;ll build a portfolio without even realizing it.\nFocus on what matters Don’t follow this advice blindly; rather, incorporate it into your own systems. Experiment, make it work better than when you found it, then pay it forward by writing down what you\u0026rsquo;ve learned for someone else to read!\nHere are my favorite books for reading or listening to if you want to cultivate a learning mindset. See non-coding books for coders.\nIf this article benefits you in some way, I encourage you to write about it! The process of learning how to learn is never finished. You can be the next iteration.\n",url:"https://victoria.dev/blog/how-to-become-a-software-developer/"},"https:\/\/victoria.dev\/neofeed\/283dcb84-e66f-4a1a-9e10-1e3e435b5298\/":{title:"283dcb84-e66f-4a1a-9e10-1e3e435b5298",tags:[],content:"Here\u0026rsquo;s the thing. The thing. It\u0026rsquo;s a powerful thought, though it sneaks by quietly at first. Give it a little room to expand in your mind. When you get, you can feel it.\nYou will do precisely what you decide to do with your life.\n",url:"https://victoria.dev/neofeed/283dcb84-e66f-4a1a-9e10-1e3e435b5298/"},"https:\/\/victoria.dev\/neofeed\/458da2db-20a6-4213-8bf1-77f44f7f2899\/":{title:"458da2db-20a6-4213-8bf1-77f44f7f2899",tags:[],content:"If LiveJournal and GitHub Pages had a baby, it would\u0026hellip; wait no\u0026hellip; okay if Myspace custom CSS and Twitter had a baby\u0026hellip; no that\u0026rsquo;s not right\u0026hellip; okay if Twitter and Geocities had a baby\u0026hellip;\n",url:"https://victoria.dev/neofeed/458da2db-20a6-4213-8bf1-77f44f7f2899/"},"https:\/\/victoria.dev\/neofeed\/3e33a025-0fe4-463b-b1be-ac375406bc98\/":{title:"3e33a025-0fe4-463b-b1be-ac375406bc98",tags:[],content:"Consider that the biggest obstacle to your product’s cyber security may be embarrassment.\n",url:"https://victoria.dev/neofeed/3e33a025-0fe4-463b-b1be-ac375406bc98/"},"https:\/\/victoria.dev\/neofeed\/1345029519653167110\/":{title:"1345029519653167110",tags:[],content:"Many folks in the US are just trying to make ends meet, but many other folks are doing all right. If you\u0026rsquo;re in the latter category, here\u0026rsquo;s how you can help make things better for everyone.\nBuy local. Buy American. Ask if it\u0026rsquo;s made in the USA.\nPay for independent news sources. Pay for indie apps and games. Get paid subscriptions to your favorite newsletters and creators. Purchase products they advertise through their referrals.\nFrequent your local independent businesses for household staples like coffee and baked goods, and handmade items. Shop makers in your state by using sites like Etsy or Amazon Handmade. Pay more if it\u0026rsquo;s made next door.\nEat at local independent restaurants, or get takeout and food delivery. Double up on orders and put some in the freezer for a quick meal during the week.\nIf you need it, like it, and if something adds value to your life \u0026ndash; make the effort to get it locally, and give money to the people you want to succeed.\n",url:"https://victoria.dev/neofeed/1345029519653167110/"},"https:\/\/victoria.dev\/neofeed\/1344689821160251395\/":{title:"1344689821160251395",tags:[],content:"Year in numbers on Victoria.dev:\n 📰 36 articles ⌨ 40,329 words 🔠 278,810 characters  find . -name '2020*' | wc -l find . -name '2020*' | xargs wc -cw\nYear in numbers on GitHub:\n 📦 32 repositories 📬 188 issues  https://github.com/search?q=user%3Avictoriadrake+created%3A\u0026gt;2020-01-01\n",url:"https://victoria.dev/neofeed/1344689821160251395/"},"https:\/\/victoria.dev\/neofeed\/2020-12-25-160439\/":{title:"2020 12 25 16:04:39",tags:[],content:"I\u0026rsquo;ve considered writing a book. It\u0026rsquo;s one of the few things I\u0026rsquo;ve wanted to do since I was young, but haven\u0026rsquo;t yet done. This is because I have trouble convincing myself that putting in the effort to create a cohesive book would be more valuable than putting the same effort into writing articles on my site, which are read by thousands each week.\nI have roughly three books' worth of words written on victoria.dev, about 112,000 words today. It\u0026rsquo;s my personal database of what I\u0026rsquo;ve learned and how I\u0026rsquo;ve done it. I want it to be public, accessible, and helpful to those who find it. I refer folks I mentor to my own articles \u0026ndash; not because it\u0026rsquo;s my blog, but because I once found a great way to explain the thing I want to explain, and I\u0026rsquo;ve preserved it for anyone to reference.\nYes, I know I can sell a book and that it\u0026rsquo;s nigh impossible to sell an article on my blog. I have a page for buying me coffee where folks can show their appreciation, but it\u0026rsquo;s nothing close to a significant portion of my income. I feel like there\u0026rsquo;s something bigger to be had here than a side income stream.\nEvery significant source of income I\u0026rsquo;ve had in tech has come as a result of someone seeing an article I wrote on my site or an open source project I created. Had I folded any of that away in a book, I think it would have been far less likely to have been found.\nI want other people to benefit from what I\u0026rsquo;ve made and I think friction is key. The reason we love search engines is because they essentially eliminate friction between you and the whole Internet. When my articles are searchable, someone can find them \u0026ndash; someone who wouldn\u0026rsquo;t necessarily buy, or couldn\u0026rsquo;t afford, that same information in a book, yet who has all the necessary chutzpah to be a self-learner and succeed. Even a free book with no more friction than \u0026ldquo;click here to download\u0026rdquo; gives you one more button you have to press.\nI\u0026rsquo;ve seen countless how-to\u0026rsquo;s when it comes to self-marketing. How to get more Twitter followers, how to build your audience, how to get more newsletter subscribers. They all seem, to me, to boil down to one thing. Provide some value for free. Or, to put it another way that sounds better: provide a window to your value that is accessible to all.\nThis makes me think that setting out to make money is really missing the mark. The success stories I\u0026rsquo;ve read over this past helluva year are tales of independent creators and entrepreneurs \u0026ndash; upstarts who\u0026rsquo;ve discovered ways to provide value to people who needed something. Money, if there was any to be had, followed.\nIn my own case, the value I provide for free in turn attracts lucrative short-term freelance work and six-figure long-term positions. Had it just been a one-off experience, I\u0026rsquo;d have called it serendipitous. After multiple times, I think I have to call it strategy.\nIn summary, let me impress the importance I perceive in removing as much friction as possible from demonstrating the value you can provide. Give it away for free. Make it accessible to all. Be generous.\nMy theory is that this, in turn, attracts value to you. Perhaps it\u0026rsquo;s the value of effective altruism, or purely monetary gain. My own story supports this, but I\u0026rsquo;m just one data point. Will you help me experiment?\n",url:"https://victoria.dev/neofeed/2020-12-25-160439/"},"https:\/\/victoria.dev\/blog\/be-brave-and-build-in-public\/":{title:"Be brave and build in public",tags:["open-source"],content:"I used to think that when I wanted to make updates to a project, I ought to hold back and do a big re-launch with all the changes at once. I figured that seeing big changes would feel like Christmas!\nUnfortunately, Christmas only comes once a year. After years in the tech and cybersecurity world, my perspective has changed.\nI\u0026rsquo;ve found that people, including myself, value receiving small, constant, incremental improvements far more than big changes once or a few times a year. It makes sense if you think about it. The former constantly delights in small, unexpected ways that make the user experience better. The latter is invisible, except for a few times a year.\nThere are occasions when big changes make sense. Say you\u0026rsquo;re re-launching functionality, or coinciding with an event that deserves all the fanfare of an unveiling.\nOther than that, and for most of us, holding back doesn\u0026rsquo;t serve us at all. It may even come from something far more insidious: fear of judgement.\nBeing brave Thinking such as \u0026ldquo;I\u0026rsquo;ll show it to the world when it\u0026rsquo;s ready,\u0026rdquo; always leaves out the most important detail. What does \u0026ldquo;ready\u0026rdquo; mean?\nIf you haven\u0026rsquo;t written down your definition of \u0026ldquo;ready,\u0026rdquo; consider that you may be holding back for no good reason. What\u0026rsquo;s the worst that could happen, anyway, if you make your work public when it\u0026rsquo;s less than perfect?\nI decided to find out when I started to build in public. Instead of holding back work, I released a first version as soon as it functioned as intended. I leaned on v0.0.* tags as a way to say, \u0026ldquo;This is available, but still in progress.\u0026rdquo; Or, I\u0026rsquo;d say so outright, in the README.\nIn the world of open source, building in public can be scary stuff. It feels like making yourself vulnerable. It\u0026rsquo;s opening up part of yourself, a creative part, for scrutiny and nitpicking \u0026ndash; by strangers. Of course it\u0026rsquo;s not comfortable.\nOnce I overcame the discomfort, once I decided to be brave and appreciate even possibly negative feedback, something amazing happened.\nI suddenly had help.\nYes, there was scrutiny and nitpicking \u0026ndash; but I don\u0026rsquo;t think any of it was ill-intentioned. I found that there existed whole communities of people who wanted to help me build a project that they thought was interesting. In some cases, I was utterly amazed when people submitted pull requests for issues I\u0026rsquo;d opened on my own projects describing enhancements I\u0026rsquo;d like to have.\nI\u0026rsquo;ve been fortunate to have wonderful experiences with open source so far. Based on these experiences, I\u0026rsquo;d like to share with you what I\u0026rsquo;ve discovered to be the most effective ways to be brave and generous when it comes to open source.\n\u0026lsquo;Tis the season to share generously When unprompted strangers submit helpful comments, issues, and pull requests on your projects, it feels like Christmas. You can give the gift of helpfulness in your contributions as well.\nTreat comments like a face-to-face conversation. Greet the person you\u0026rsquo;re addressing. Use full sentences. Think about whether what you\u0026rsquo;re writing will make someone\u0026rsquo;s day better or worse, and be nice.\nWhen writing issues, include as much technical detail as possible. Screenshots, console logs, screenshots of console logs, your operating system, browser, screen resolution \u0026ndash; all these can help maintainers quickly diagnose a root cause.\nPull requests are the best presents ever. They make maintainers happy when well done, and it\u0026rsquo;s a gift that gives back when your contribution gets merged! 🎉 Give your PR the best chance of getting accepted by looking for and following any project contribution guidelines.\nRecognize the human Our brains are slightly lacking in an evolutionary sense when it comes to interacting with other humans through tiny screens. It can be easy to forget that the actions you put out there will eventually reach one or more other people.\nYou can help to maintain a great open source community by remembering the humans that make it exist. When commenting, take the time to do it well (see below) and recognize the time that someone else has put in. When closing a thread or merging a contribution, remember to say thank you to the people who pitched in to help. I try to use first names instead of screen names, whenever possible.\nYou can build personal relationships, too. If you\u0026rsquo;re a project maintainer, you may choose to give people a way to contact you directly to ask questions or hash out complicated plans. Establishing one-to-one communications with regular contributors is also a great way to build a community around your project.\nRecognizing the humans behind the open source community is a simple and meaningful way to give back.\nDon\u0026rsquo;t rush The vast majority of open source participants are volunteers, which means they don\u0026rsquo;t get paid for the time they spend building up projects. That sometimes means that other work takes priority. It\u0026rsquo;s okay if this describes you, too.\nIt\u0026rsquo;s important to remember that in most cases, a well-done contribution later is preferred over a half-done contribution sooner. If you\u0026rsquo;re too short on time now to write a thoughtful comment \u0026ndash; don\u0026rsquo;t! Either draft a quick note and set it aside for later, or comment something along the lines of:\n Hi there! Just wanted to let you know that I\u0026rsquo;ve seen this and I plan to help! I\u0026rsquo;ll respond in full as soon as I have the time to write a thoughtful comment.\n Showing that you think a comment is worth the time to do well is something that open source contributors and repository maintainers both appreciate.\nBuild generously When open source participants act with conscientiousness, every day feels like Christmas. Regardless of your type of contribution, you can help build this generous global community year-round.\nThe humans of open source, by self selection, mostly consist of good people who want to help. If you build openly, share feedback generously, and try to do good in general, I think you fit in here.\nI hope you have a very happy holiday season and give many gifts that keep on giving!\n",url:"https://victoria.dev/blog/be-brave-and-build-in-public/"},"https:\/\/victoria.dev\/neofeed\/1341054072204521475\/":{title:"1341054072204521475",tags:[],content:"When presented with an offer like \u0026ldquo;Get $10 when you sign up today,\u0026rdquo; the question you should be asking to make the proper value calculation is, \u0026ldquo;Am I willing to start this new habit for $10?\u0026rdquo;\n",url:"https://victoria.dev/neofeed/1341054072204521475/"},"https:\/\/victoria.dev\/blog\/so-youre-the-family-tech-support\/":{title:"So you\u0027re the family tech support",tags:["privacy","cybersecurity","life"],content:"🎄🌟 Happy holidays! 🌟🎄\nFor those of you seeing relatives this season, chances are that you’re the designated family tech support. If part of your time home for the holidays is spent on software updates and troubleshooting WiFi, here are a few other quick wins to help boost your family\u0026rsquo;s online privacy and security.\n1. Set up a VPN Using a VPN is Online Safety 101, but the biggest barrier to effectiveness is not remembering to turn it on. ExpressVPN\u0026rsquo;s Network Lock kill switch feature lets you set-it-and-forget-it, making it seamless to add VPN protection to your online activities. I go in-depth on this privacy-focused VPN here.\n2. Introduce a password manager If your family member uses the same password everywhere (\u0026lt;petname\u0026gt;+\u0026lt;house number\u0026gt;, same as last year) because passwords are hard to remember, introduce them to their new best friend, 1Password. Help your family get set up with secure passwords they don\u0026rsquo;t have to write down on Post-It notes \u0026ndash; just one master pass(phrase) is all you need.\nWhen choosing a passphrase, avoid using information easily found on social media accounts, like pet names, favorite sports teams, favorite brands, or birthdays.\n3. Switch to DuckDuckGo Help fight the Internet search monopoly by getting your family to use a search engine that respects their privacy. Go to your browser Settings and set your Default Search Engine (that uses the URL bar) to DuckDuckGo. Break the ice with an instant answer feature, like searching \u0026ldquo;calendar\u0026rdquo; so you can count down to Christmas.\n(You might want to search for \u0026ldquo;classic cocktails cheat sheet\u0026rdquo; after all this.)\n4. Install a better browser and blocker While I prefer a Pi-hole, setting one up can be complex. Instead, help set up a privacy-preserving browser like Firefox or a wide-spectrum blocking extension like uBlock Origin (GitHub source).\nYour family will get faster page load times, less advertisements interrupting articles and videos, and fewer sneaky trackers leaking browsing habits to big tech, all with near-zero maintenance.\nBe a home-for-the-holidays hero! Help improve your family\u0026rsquo;s security posture this holiday season. A little beefed-up cybersecurity may be one of the best gifts you can give!\nI\u0026rsquo;m keeping it short-and-sweet this week. My annual Christmas post drops on December 24, full of warm fuzzy goodness and a tech tip or two. Thank you for being a subscriber \u0026ndash; stay tuned!\n",url:"https://victoria.dev/blog/so-youre-the-family-tech-support/"},"https:\/\/victoria.dev\/neofeed\/2020-12-20-164104\/":{title:"2020 12 20 16:41:04",tags:[],content:"Hi mom.\n",url:"https://victoria.dev/neofeed/2020-12-20-164104/"},"https:\/\/victoria.dev\/neofeed\/2020-12-19-211137\/":{title:"2020 12 19 21:11:37",tags:[],content:"I\u0026rsquo;m experimenting.\n",url:"https://victoria.dev/neofeed/2020-12-19-211137/"},"https:\/\/victoria.dev\/neofeed\/1338882674556014594\/":{title:"1338882674556014594",tags:[],content:"The process of writing and editing your own work is to temporarily fall in love with your words and then break up with them repeatedly.\nDraft like you’re half-drunk. Edit sober.\n",url:"https://victoria.dev/neofeed/1338882674556014594/"},"https:\/\/victoria.dev\/blog\/how-to-write-good-documentation\/":{title:"How to write good documentation",tags:["docs","tech-team"],content:"If you\u0026rsquo;ve ever half-written a software project before taking a few days off, this is the article you\u0026rsquo;ll discover you needed when you reopen that IDE.\nIn the technology teams I lead, we make a constant effort to document all the things. Documentation lives alongside the code as an equal player. This helps ensure that no one needs to make assumptions about how something works, or is calling lengthy meetings to gain working knowledge of a feature. Good documentation saves us a lot of time and hassle.\nThat said, and contrary to popular belief, the most valuable software documentation is not primarily written for other people. As I said in this well-received tweet:\nThe secret to good documentation is to write it while you\u0026#39;re writing the code. You are your first audience. Explain what you\u0026#39;re doing to yourself. Future you will thank you!\n\u0026mdash; Victoria Drake November 24, 2020\n  With holidays around the corner, it\u0026rsquo;d be prudent to prepare for the possibility of some eggnog-induced programming paralysis. (Pecan pie and Python make a great combination.)\nHere are three concrete steps you can take to write good documentation before it\u0026rsquo;s too late.\n1. Start with accurate notes As you work out ideas in code, ensure you don’t soon forget important details by starting with accurate notes. While you will want to explain things to yourself in long-form later, short-form notes will suffice to capture details without interrupting your coding session flow.\nKeep a document open alongside your code and write down things like commands, decisions, and sources you use. This can include:\n Terminal commands you typed in Why you chose a particular method over another Links you visited for help or coughcopy-pastecough inspiration The order in which you did things  Don’t worry about full sentences at this point. Just ensure you accurately capture context, relevant code snippets, and helpful URLs. It can also be helpful to turn on any auto-save option available.\n2. Explain decisions in long form The ideal time to tackle this step is when you take a break from coding, but before you completely go out to lunch on whatever it is you’re working on at the moment. You want to ensure that context, ideas, and decisions are all still fresh in your mind when you explain them to yourself.\nGo over the short-form notes you took and start expanding them into conversational writing. Be your own rubber duck. Describe what you’re doing as if you were teaching it to someone else. You might cover topics such as:\n Quirky-looking decisions: \u0026ldquo;I would normally do it this way, but I chose to do something different because\u0026hellip;\u0026rdquo; Challenges you ran into and how you overcame them Architectural decisions that support your project goals  Stick to the main points. Long-form writing doesn’t mean you’ll be paid by the word! Just use full sentences, and write as if explaining your project to a colleague. You’re explaining to future you, after all.\n3. Don\u0026rsquo;t neglect prerequisite knowledge This step is best done after a long lunch break, or even the next day (but probably not two). Re-read your document and fill in any blanks that become apparent after putting some distance between yourself and the project.\nTake extra care to fill in or at least link to prerequisite knowledge, especially if you frequently use different languages or tools. Even an action as small as pasting in a link to the API documentation you used can save hours of future searching.\nWrite down or link to READMEs, installation steps, and relevant support issues. For frequently performed command-line actions, you can use a self-documenting Makefile to avoid having to man common tasks each time you come back to a project.\nIt’s easy to forget supporting details after even just a short break from your project. Capture anything you found helpful this time around.\nDocument all the things The next time you catch yourself thinking, “I’m sure I’ll remember this part, no need to write it down,” just recall this emoji: 🤦‍♀️\nSoftware projects are made up of a lot more than just their code. To best set up your future self for success, document all the things! Whether it’s a process you’ve established, Infrastructure as Code, or a fleeting future roadmap idea — write it down! Future you will thank you for it.\nIf you enjoyed this post, there\u0026rsquo;s a lot more where that came from! I write about computing, cybersecurity, and leading great technical teams. You can subscribe to see new articles first.\n",url:"https://victoria.dev/blog/how-to-write-good-documentation/"},"https:\/\/victoria.dev\/neofeed\/1336702163733590019\/":{title:"1336702163733590019",tags:[],content:"Quick example of how to weave small habits into systems for success in your life. 👇\nI drink out of my favorite Yeti mug all day. When I first pour in my hot drink, I know it takes 10-15 minutes to cool to a drinkable temperature.\nIf I walk away and start coding or writing, I know I\u0026rsquo;ll end up forgetting about my drink long enough for it to get cold.\nInstead, I always take that time to do some yoga or bodyweight exercises. Those short bursts of activity get my blood moving and help to refresh my mind.\nI also don\u0026rsquo;t lose track of time with a quick exercise the way I do when I\u0026rsquo;m in coding flow, so by the time 10-15 minutes pass, I feel ready to stop.\nIf I make a hot drink 2-4 times a day (nearly 100% of my days) that\u0026rsquo;s 30 minutes to an hour of exercise in total, without even thinking about it. Boom. Easy.\n",url:"https://victoria.dev/neofeed/1336702163733590019/"},"https:\/\/victoria.dev\/blog\/make-your-team-more-productive-by-literally-doing-one-thing\/":{title:"Make your team more productive by literally doing one thing",tags:["tech-team","leadership"],content:"In the tech teams I lead, \u0026ldquo;priority\u0026rdquo; has no plural form.\nWhether you\u0026rsquo;re leading a team of people or leading yourself, it\u0026rsquo;s important to take account of all the important things that need doing in your organization. This does not mean that everything can be equally important.\nLogically, everything can\u0026rsquo;t be. Tasks are typically interdependent, and there\u0026rsquo;s always one task on which another depends. Tasks can be time-sensitive. Certain tasks might block a logical path towards a goal.\nIt\u0026rsquo;s the duty of a leader to make hard calls and decide which tasks are most important out of everything that needs doing. This necessitates comparing one to another, which is much easier to do with a centralized to-do list.\nHere\u0026rsquo;s how this one simple change to your perspective on to-do lists can help to build happier and more productive teams.\nKeep a central prioritized to-do list Avoid working in silos. A single centralized list can make it easier for you and your team members to see what\u0026rsquo;s being worked on. With all tasks out in the open, it\u0026rsquo;s easier for people to spot opportunities for helping each other out and where they can contribute.\nEncouraging a culture of openness can help people feel more comfortable asking questions, asking for help, and proposing ideas and improvements. Tracking work in the open also means that no one is left wondering what status a task is currently in.\nFor team leaders, a single list makes it easier to compare and prioritize tasks. This benefits team members by providing a completely unambiguous and transparent accounting of what needs doing next. Whichever task is most important, for the whole organization, is on top.\nPriorities with autonomy A single priority doesn\u0026rsquo;t necessarily pigeonhole someone into doing a task they don\u0026rsquo;t feel cut out for. Each member of your team has different strengths, skill sets, and diverse ways of thinking. You can take full advantage of this by encouraging autonomy in task selection.\nHave people choose whichever task is nearest to the top that they\u0026rsquo;d like to tackle. They might pick the highest priority task that\u0026rsquo;s in their wheelhouse, or experiment with a higher one that\u0026rsquo;s in a domain they\u0026rsquo;d like to improve their skills at.\nEmbrace opportunities for cross-training. If tasks high up on the list fall in a category that only one or a few people on your team are experts in, have your experts partner up with another team member who\u0026rsquo;s taking on the task. By pooling your resources to cross-train across domains, you multiply the capabilities of each team member and your team as a result.\nWhen a task is especially time-sensitive, have several team members swarm on it and distribute the work according to their interests or strengths.\nMake yourself redundant Working off a single prioritized to-do list works best when your team members can take on tasks as independently as possible. This is especially important in remote teams where people work asynchronously.\nIf you\u0026rsquo;re a leader and find that your team members frequently ask you what they should do next, you could be making your team dependent on you. Ask yourself if you\u0026rsquo;re unnecessarily gatekeeping information that would let your team be more autonomous.\nA team that overly depends on their leader is not an efficient one. Individual people, such as yourself, don\u0026rsquo;t scale. Don\u0026rsquo;t become a bottleneck to your team\u0026rsquo;s productivity. A successful leader should be able to take several days off on short notice without productivity grinding to a halt.\nTo support your team\u0026rsquo;s ability to work without you, make your team, product, and company goals painfully available. Put them where people hang out \u0026ndash; your team\u0026rsquo;s message board, chat channel, or document repository, for example. No one should be at a loss when asked what the team wants to achieve next, and why.\nMake any applicable resources, style guides, product documents, or links to external documentation painfully available as well. If your team makes a decision about how something should be done, write it down. Don\u0026rsquo;t rely on yours or anyone else\u0026rsquo;s meat brain to remember an important decision, nor make yourself the only resource for recalling it.\nMake yourself redundant when it comes to day-to-day work. Doing so empowers your team members to do work without you, think through solutions on their own, and propose paths of action that you probably wouldn\u0026rsquo;t have thought of yourself.\nBuild happier and more productive teams From first-hand experience as both a team member and leader, I\u0026rsquo;ve seen how encouraging a culture of openness, cross-training, and autonomy makes for happier team members and more productive teams. A single prioritized to-do list, coupled with available documentation and resources, opens the gates to let your technical team be maximally productive.\nBy removing bottlenecks, you allow people to make more decisions on their own and take ownership of their work. That\u0026rsquo;s a technical team I\u0026rsquo;d be proud to lead.\n",url:"https://victoria.dev/blog/make-your-team-more-productive-by-literally-doing-one-thing/"},"https:\/\/victoria.dev\/neofeed\/1335977389810200581\/":{title:"1335977389810200581",tags:[],content:"While setting up a new phone recently, I couldn\u0026rsquo;t for the life of me remember how I\u0026rsquo;d gotten iAWriter and Working Copy to sync so I could write and push to Git from anywhere.\nI did remember writing about it some time in the past. So I searched my blog (site:https://victoria.dev ia writer working copy) and found my post from 20 months ago explaining the set up.\nA remote sync solution for iOS and Linux: Git and Working Copy\nI followed my own instructions but one of the steps didn\u0026rsquo;t make sense. Either I missed something the first time, or the process changed since I wrote it up.\nThat\u0026rsquo;s ok! I had a head start on my research already thanks to what I wrote, and in a few minutes I found the new answer. I decided to update the post and in so doing I realized it\u0026rsquo;s the second time I\u0026rsquo;ve updated it to add something new! It’s like past me leaving helpful tips for future me, months or years ago.\nMany people read my blog, and I’m flattered by the feedback I receive. I’m happy that I’m able to help so many other people, but I also don’t write it for other people primarily.\nI write down the things I do and what I learn from doing them because it’s like having a second, more-perfect brain. My meat-brain will forget the details, but my disk-brain can retain its accuracy indefinitely.\nI often get questions like, “how do I choose a theme for my blog?” or “what platform should I use?” or “what popular language/framework/topic should I focus on?” My answer is: don’t start with that. Start with you.\nChoose whatever publishing option is easiest for you right now, then start by writing for yourself. Write about what interests you. Try something new, even if it seems rudimentary, and write in-depth about what you learn. (One of my most popular posts is about iteration in Python. When I first wrote it, I considered myself a complete beginner.)\nWrite for yourself by explaining what you’re doing, as if it were past-you teaching future-you — because it is. You will be your first reader, and the first judge of how useful your blog can be. Seek to impress yourself!\nMake it a habit, and I think the rest will follow.\n",url:"https://victoria.dev/neofeed/1335977389810200581/"},"https:\/\/victoria.dev\/blog\/owasp-web-security-testing-guide-v4.2-released\/":{title:"OWASP Web Security Testing Guide v4.2 released",tags:["cybersecurity","open-source"],content:"I\u0026rsquo;m very happy and proud to share that the Open Web Application Security Project (OWASP) Web Security Testing Guide v4.2 is now available! This update is the result of a lot of hard work by the repository team and many dedicated contributors. With a team like this, I\u0026rsquo;m honored to be a core maintainer and co-author.\nHere\u0026rsquo;s a reprint of the announcement I wrote for owasp.org. If you\u0026rsquo;re interested in security testing for web applications and APIs, this is an update you\u0026rsquo;ll definitely want to check out!\nYou can become a contributor yourself by joining us on GitHub!\n Web Security Testing Guide v4.2 Released Thursday, December 3, 2020\nThe OWASP Web Security Testing Guide team is proud to announce version 4.2 of the Web Security Testing Guide (WSTG)! In keeping with a continuous delivery mindset, this new minor version adds content as well as improves the existing tests.\nIn recent years, the Web Security Testing Guide has sought to remain your foremost open source resource for web application testing. Our previous release marked a move from a cumbersome wiki platform to the highly collaborative world of GitHub. Since then, over 61 new contributors pushing over 600 commits have helped to make the WSTG better than ever.\nVersion 4.2 of the Web Security Testing Guide introduces new testing scenarios, updates existing chapters, and offers an improved reading experience with a clearer writing style and chapter layout. Readers will enjoy easier navigation and consistent testing instructions.\nWith new improvements to our development workflow, new contributors will find it easier than ever to help build future versions of the WSTG. A clear and concise contributor’s guide and style guide can help you write new tests or ensure existing scenarios stay current. Core maintainers Rick Mitchell, Elie Saad, Rejah Rehim, and Victoria Drake have implemented modern processes like continuous integration with GitHub Actions. New workflows help to build PDFs and make reviewing new additions and updates easier.\nWe couldn’t be happier to share this new version with you, and we don’t plan to slow down anytime soon. The dedicated volunteers who’ve made this release possible are already hard at work on the next major version of the WSTG. Come join us and become a contributor!\nYou can read the Web Security Testing Guide v4.2 online or download a PDF on our project page. We greatly appreciate all the authors, editors, reviewers, and readers who make this open source security endeavor worthwhile.\nThank you for being a part of the WSTG!\n",url:"https://victoria.dev/blog/owasp-web-security-testing-guide-v4.2-released/"},"https:\/\/victoria.dev\/neofeed\/1334126675836227584\/":{title:"1334126675836227584",tags:[],content:"To understand the world, travel.\nTo understand yourself, stay home.\n",url:"https://victoria.dev/neofeed/1334126675836227584/"},"https:\/\/victoria.dev\/blog\/what-is-tcp\/ip-layers-and-protocols-explained\/":{title:"What is TCP/IP? Layers and protocols explained",tags:["computing","protocols","data"],content:"A significant part of the process of creation is the ability to imagine things that do not yet exist. This skill was instrumental to the creation of the Internet. If no one had imagined the underlying technology that most now take for granted every day, there would be no cat memes.\nTo make the Internet possible, two things that needed imagining are layers and protocols. Layers are conceptual divides that group similar functions together. The word \u0026ldquo;protocol,\u0026rdquo; means \u0026ldquo;the way we\u0026rsquo;ve agreed to do things around here,\u0026rdquo; more or less. In short, both layers and protocols can be explained to a five-year-old as \u0026ldquo;ideas that people agreed sounded good, and then they wrote them down so that other people could do things with the same ideas.\u0026rdquo;\nThe Internet Protocol Suite is described in terms of layers and protocols. Collectively, the suite refers to the communication protocols that enable our endless scrolling. It\u0026rsquo;s often called by its foundational protocols: the Transmission Control Protocol (TCP) and the Internet Protocol (IP). Lumped together as TCP/IP, these protocols describe how data on the Internet is packaged, addressed, sent, and received.\nHere\u0026rsquo;s why the Internet Protocol Suite, or TCP/IP, is an imaginary rainbow layer cake.\nLayers are imaginary If you consider the general nature of a rainbow layer sponge cake, it\u0026rsquo;s mostly made up of soft, melt-in-your mouth vanilla-y goodness. This goodness is in itself comprised of something along the lines of eggs, butter, flour, and sweetener.\nThere isn\u0026rsquo;t much to distinguish one layer of a rainbow sponge cake from another. Often, the only difference between layers is the food-coloring and a bit of frosting. When you think about it, it\u0026rsquo;s all cake from top to bottom. The rainbow layers are only there because the baker thought they ought to be.\nSimilar to cake ingredients, layers in the context of computer networking are mostly composed of protocols, algorithms, and configurations, with some data sprinkled in. It can be easier to talk about computer networking if its many functions are split up into groups, so certain people came up with descriptions of layers, which we call network models. TCP/IP is just one network model among others. In this sense, layers are concepts, not things.\nSome of the people in question are part of the Internet Engineering Task Force (IETF). They created the RFC-1122 publication, discussing the Internet\u0026rsquo;s communications layers. Half of a whole, the standard:\n \u0026hellip;covers the communications protocol layers: link layer, IP layer, and transport layer; its companion RFC-1123 covers the application and support protocols.\n The layers described by RFC-1122 and RFC-1123 each encapsulate protocols that satisfy the layer\u0026rsquo;s functionality. Let\u0026rsquo;s look at each of these communications layers and see how TCP and IP stack up in this model of the Internet layer cake.\nLink layer protocols The link layer is the most basic, or lowest-level, classification of communication protocol. It deals with sending information between hosts on the same local network, and translating data from the higher layers to the physical layer. Protocols in the link layer describe how data interacts with the transmission medium, such as electronic signals sent over specific hardware. Unlike other layers, link layer protocols are dependent on the hardware being used.\nInternet layer protocols Protocols in the Internet layer describe how data is sent and received over the Internet. The process involves packaging data into packets, addressing and transmitting packets, and receiving incoming packets of data.\nThe most widely known protocol in this layer gives TCP/IP its last two letters. IP is a connectionless protocol, meaning that it provides no guarantee that packets are sent or received in the right order, along the same path, or even in their entirety. Reliability is handled by other protocols in the suite, such as in the transport layer.\nThere are currently two versions of IP in use: IPv4, and IPv6. Both versions describe how devices on the Internet are assigned IP addresses, which are used when navigating to cat memes. IPv4 is more widely used, but has only 32 bits for addressing, allowing for about 4.3 billion (ca. 4.3×109) possible addresses. These are running out, and IPv4 and will eventually suffer from address exhaustion as more and more people use more devices on the Internet.\nThe successor version IPv6 aims to solve address exhaustion by using 128 bits for addresses. This provides, um, a lot more address possibilities (ca. 3.4×1038).\nTransport layer protocols In May 1974, Vint Cerf and Bob Kahn (collectively often called \u0026ldquo;the fathers of the Internet\u0026rdquo;) published a paper entitled A Protocol for Packet Network Intercommunication. This paper contained the first description of a Transmission Control Program, a concept encompassing what would eventually be known as the Transmission Control Protocol (TCP) and User Datagram Protocol (UDP). (I had the pleasure of meeting Vint and can personally confirm that yes, he does look exactly like The Architect in the Matrix movies.)\nThe transport layer presently encapsulates TCP and UDP. Like IP, UDP is connectionless and can be used to prioritize time over reliability. TCP, on the other hand, is a connection-oriented transport layer protocol that prioritizes reliability over latency, or time. TCP describes transferring data in the same order as it was sent, retransmitting lost packets, and controls affecting the rate of data transmission.\nApplication layer protocols The application layer describes the protocols that software applications interact with most often. The specification includes descriptions of the remote login protocol Telnet, the File Transfer Protocol (FTP), and the Simple Mail Transfer Protocol (SMTP).\nAlso included in the application layer are the Hypertext Transfer Protocol (HTTP) and its successor, Hypertext Transfer Protocol Secure (HTTPS). HTTPS is secured by Transport Layer Security, or TLS, which can be said to be the top-most layer of the networking model described by the Internet protocol suite. If you\u0026rsquo;d like to further understand TLS and how this protocol secures your cat meme viewing, I invite you read my article about TLS and cryptography.\nThe Internet cake is still baking Like a still-rising sponge cake, descriptions of layers, better protocols, and new models are being developed every day. The Internet, or whatever it will become in the future, is still in the process of being imagined.\nIf you enjoyed learning from this post, there\u0026rsquo;s a lot more where this came from! I write about computing, cybersecurity, and building great technical teams. Subscribe to see new articles first.\n",url:"https://victoria.dev/blog/what-is-tcp/ip-layers-and-protocols-explained/"},"https:\/\/victoria.dev\/neofeed\/33342\/":{title:"33342",tags:[],content:"If you\u0026rsquo;re reading this, you found the secret room. 🎉\nThere are cookies. 🍪🍪🍪\n",url:"https://victoria.dev/neofeed/33342/"},"https:\/\/victoria.dev\/neofeed\/1331589960604979200\/":{title:"1331589960604979200",tags:[],content:"\u0026ldquo;You cannot push anyone up a ladder unless he be willing to climb a little himself.\u0026rdquo;\n\u0026ndash; Andrew Carnegie\n",url:"https://victoria.dev/neofeed/1331589960604979200/"},"https:\/\/victoria.dev\/neofeed\/1331262801797652483\/":{title:"1331262801797652483",tags:[],content:"The secret to good documentation is to write it while you\u0026rsquo;re writing the code. You are your first audience. Explain what you\u0026rsquo;re doing to yourself. Future you will thank you!\n",url:"https://victoria.dev/neofeed/1331262801797652483/"},"https:\/\/victoria.dev\/neofeed\/32723\/":{title:"32723",tags:[],content:"I occasionally revisit previous blog posts and update them with things I\u0026rsquo;ve learned (and why not? No dead trees here!). One of my top-visited posts on iteration in Python just got an update!\nRead it here 👉 Iteration in Python: for, list, and map\n",url:"https://victoria.dev/neofeed/32723/"},"https:\/\/victoria.dev\/neofeed\/32239\/":{title:"32239",tags:[],content:"First emails from SimpleSubscribe.org go out today! Hope they look good 😅 apparently CSS in emails is a whole iceberg of complexity.\n",url:"https://victoria.dev/neofeed/32239/"},"https:\/\/victoria.dev\/blog\/responsive-pages-and-color-themes-with-minimal-css\/":{title:"Responsive pages and color themes with minimal CSS",tags:["websites","coding"],content:"Hello, do come in! If you\u0026rsquo;re reading this on my website, you may notice I\u0026rsquo;ve spruced up a bit. Victoria.dev can now better respond to your devices and preferences!\nMost modern devices and web browsers allow users to choose either a light or dark theme for the user interface. With CSS media queries, you can have your own website\u0026rsquo;s styles change to match this user setting!\nMedia queries are also a common way to have elements on web pages change to suit different screen sizes. This is an especially powerful tool when combined with custom properties set on the root element.\nHere\u0026rsquo;s how to use CSS media queries and custom properties to improve your visitor\u0026rsquo;s browsing experience with just a few lines of CSS.\nCatering to color preferences The prefers-color-scheme media feature can be queried to serve up your user\u0026rsquo;s color scheme of choice. The light option is the go-to version if no active preference is set, and it has decent support across modern browsers.\nAdditionally, users reading on certain devices can also set light and dark color themes based on a schedule. For example, my phone uses light colors throughout its UI during the daytime, and dark colors at night. You can make your website follow suit!\nAvoid repeating a lot of CSS by setting custom properties for your color themes on your :root pseudo-class. You can specify the themes available with the color-scheme property (currently part of a draft specification, but I like to write my articles to age well). Create a version for each theme you wish to support. Here\u0026rsquo;s a quick example you can build on:\n:root { color-scheme: light dark; } @media (prefers-color-scheme: light) { :root { --text-primary: #24292e; --background: white; --shadow: rgba(0, 0, 0, 0.15) 0px 2px 5px 0px; } } @media (prefers-color-scheme: dark) { :root { --text-primary: white; --background: #24292e; --shadow: rgba(0, 0, 0, 0.35) 0px 2px 5px 0px; } } As you can see, you can use custom properties to set all kinds of values. To use these as variables with other CSS elements, use the var() function:\nheader { color: var(--text-primary); background-color: var(--background); box-shadow: var(--shadow); } In this quick example, the header element will now display your user\u0026rsquo;s preferred colors according to their browser settings!\nPreferred color schemes are set by the user in different ways, depending on the browser. Here are a couple examples.\nFirefox You can test out light and dark modes in Firefox by typing about:config into the address bar. Accept the warning if it pops up, then type ui.systemUsesDarkTheme into the search.\nChoose a Number value for the setting, then input a 1 for dark or 0 for light.\nBrave If you\u0026rsquo;re using Brave, find color theme settings in Settings \u0026gt; Appearance \u0026gt; Brave colors.\nVariable scaling You can also use a custom property to effortlessly adjust the size of text or other elements depending on your user\u0026rsquo;s screen size. The width media feature tests the width of the viewport. While width: _px will match an exact size, you can also use min and max to create ranges.\nQuery with min-width: _px to match anything over _ pixels, and max-width: _px to match anything up to _ pixels.\nUse these queries to set a custom property on the :root to create a ratio:\n@media (min-width: 360px) { :root { --scale: 0.8; } } @media (min-width: 768px) { :root { --scale: 1; } } @media (min-width: 1024px) { :root { --scale: 1.2; } } Then make an element responsive by using the calc() function. Here are a few examples:\nh1 { font-size: calc(42px * var(--scale)); } h2 { font-size: calc(26px * var(--scale)); } img { width: calc(200px * var(--scale)); } In this example, multiplying an initial value by your --scale custom property allows the size of headings and images to magically adjust to your user\u0026rsquo;s device width.\nThe relative unit rem will have a similar effect. You can use it to define sizes for elements relative to the font size declared at the root element.\nh1 { font-size: calc(5rem * var(--scale)); } h2 { font-size: calc(1.5rem * var(--scale)); } p { font-size: calc(1rem * var(--scale)); } Of course, you can also multiply two custom properties. For example, setting the --max-img as a custom property on the :root can help to save you time later on by not having to update a pixel value in multiple places:\nimg { max-width: calc(var(--max-img) * var(--scale)); } Raise your responsiveness game Try out these easy wins for a website that caters to your visitor\u0026rsquo;s devices and preferences. I\u0026rsquo;ve put them to good use now on victoria.dev. I invite you to let me know how you like it!\n",url:"https://victoria.dev/blog/responsive-pages-and-color-themes-with-minimal-css/"},"https:\/\/victoria.dev\/blog\/build-your-own-serverless-subscriber-list-with-go-and-aws\/":{title:"Build your own serverless subscriber list with Go and AWS",tags:["api","aws","coding","go","data","cybersecurity","websites"],content:"You can now subscribe to my email list on victoria.dev! Here\u0026rsquo;s how I lovingly built a subscription sign up flow with email confirmation that doesn\u0026rsquo;t suck. You can too.\nIntroducing Simple Subscribe If you\u0026rsquo;re interested in managing your own mailing list or newsletter, you can set up Simple Subscribe on your own AWS resources to collect email addresses. This open source API is written in Go, and runs on AWS Lambda. Visitors to your site can sign up to your list, which is stored in a DynamoDB table, ready to be queried or exported at your leisure.\nWhen someone signs up, they\u0026rsquo;ll receive an email asking them to confirm their subscription. This is sometimes called \u0026ldquo;double opt-in,\u0026rdquo; although I prefer the term \u0026ldquo;verified.\u0026rdquo; Simple Subscribe works on serverless infrastructure and uses an AWS Lambda to handle subscription, confirmation, and unsubscribe requests.\nYou can find the Simple Subscribe project, with its fully open-source code, on GitHub. I encourage you to pull up the code and follow along! In this post I\u0026rsquo;ll share each build step, the thought process behind the API\u0026rsquo;s single-responsibility functions, and security considerations for an AWS project like this one.\nBuilding a verified subscription flow A non-verified email sign up process is straightforward. Someone puts their email into a box on your website, then that email goes into your database. However, if I\u0026rsquo;ve taught you anything about not trusting user input, the very idea of a non-verified sign up process should raise your hackles. Spam may be great when fried in a sandwich, but no fun when it\u0026rsquo;s running up your AWS bill.\nWhile you can use a strategy like a CAPTCHA or puzzle for is-it-a-human verification, these can create enough friction to turn away your potential subscribers. Instead, a confirmation email can help to ensure both address correctness and user sentience.\nTo build a subscription flow with email confirmation, create single-responsibility functions that satisfy each logical step. Those are:\n Accept an email address and record it. Generate a token associated with that email address and record it. Send a confirmation email to that email address with the token. Accept a verification request that has both the email address and token.  To achieve each of these goals, Simple Subscribe uses the official AWS SDK for Go to interact with DynamoDB and SES.\nAt each stage, consider what the data looks like and how you store it. This can help to handle conundrums like, \u0026ldquo;What happens if someone tries to subscribe twice?\u0026rdquo; or even threat-modeling such as, \u0026ldquo;What if someone subscribes with an email they don\u0026rsquo;t own?\u0026rdquo;\nReady? Let\u0026rsquo;s break down each step and see how the magic happens.\nSubscribing The subscription process begins with a humble web form, like the one on my site\u0026rsquo;s main page. A form input with attributes type=\u0026quot;email\u0026quot; required helps with validation, thanks to the browser. When submitted, the form sends a GET request to the Simple Subscribe subscription endpoint.\nSimple Subscribe receives a GET request to this endpoint with a query string containing the intended subscriber\u0026rsquo;s email. It then generates an id value and adds both email and id to your DynamoDB table.\nThe table item now looks like:\n   email confirm id timestamp     subscriber@example.com false uuid-xxxxx 2020-11-01 00:27:39    The confirm column, which holds a boolean, indicates that the item is a subscription request that has not yet been confirmed. To verify an email address in the database, you\u0026rsquo;ll need to find the correct item and change confirm to true.\nAs you work with your data, consider the goal of each manipulation and how you might compare an incoming request to existing data.\nFor example, if someone made a subsequent subscription request for the same email address, how would you handle it? You might say, \u0026ldquo;Create a new line item with a new id,\u0026rdquo; however, this might not be best strategy when your serverless application database is paid for by request volume.\nSince DynamoDB Pricing depends on how much data you read and write to your tables, it\u0026rsquo;s advantageous to avoid piling on excess data.\nWith that in mind, it would be prudent to handle subscription requests for the same email by performing an update instead of adding a new line. Simple Subscribe actually uses the same function to either add or update a database item. This is typically referred to as, \u0026ldquo;update or insert.\u0026rdquo;\nIn a database like SQLite this is accomplished with the UPSERT syntax. In the case of DynamoDB, you use an update operation. For the Go SDK, its syntax is UpdateItem.\nWhen a duplicate subscription request is received, the database item is matched on the email only. If an existing line item is found, its id and timestamp are overridden, which updates the existing database record and avoids flooding your table with duplicate requests.\nVerifying email addresses After submitting the form, the intended subscriber then receives an email from SES containing a link. This link is built using the email and id from the table, and takes the format:\n\u0026lt;BASE_URL\u0026gt;\u0026lt;VERIFY_PATH\u0026gt;/?email=subscriber@example.com\u0026amp;id=uuid-xxxxx In this set up, the id is a UUID that acts as a secret token. It provides an identifier that you can match that is sufficiently complex and hard to guess. This approach deters people from subscribing with email addresses they don\u0026rsquo;t control.\nVisiting the link sends a request to your verification endpoint with the email and id in the query string. This time, it\u0026rsquo;s important to compare both the incoming email and id values to the database record. This verifies that the recipient of the confirmation email is initiating the request.\nThe verification endpoint ensures that these values match an item in your database, then performs another update operation to set confirm to true, and update the timestamp. The item now looks like:\n   email confirm id timestamp     subscriber@example.com true uuid-xxxxx 2020-11-01 00:37:39    Querying for emails You can now query your table to build your email list. Depending on your email sending solution, you might do this manually, with another Lambda, or even from the command line.\nSince data for requested subscriptions (where confirm is false) is stored in the table alongside confirmed subscriptions, it\u0026rsquo;s important to differentiate this data when querying for email addresses to send to. You\u0026rsquo;ll want to ensure you only return emails where confirm is true.\nProviding unsubscribe links Similar to verifying an email address, Simple Subscribe uses email and id as arguments to the function that deletes an item from your DynamoDB table in order to unsubscribe an email address. To allow people to remove themselves from your list, you\u0026rsquo;ll need to provide a URL in each email you send that includes their email and id as a query string to the unsubscribe endpoint. It would look something like:\n\u0026lt;BASE_URL\u0026gt;\u0026lt;UNSUBSCRIBE_PATH\u0026gt;/?email=subscriber@example.com\u0026amp;id=uuid-xxxxx When the link is clicked, the query string is passed to the unsubscribe endpoint. If the provided email and id match a database item, that item will be deleted.\nProving a method for your subscribers to automatically remove themselves from your list, without any human intervention necessary, is part of an ethical and respectful philosophy towards handling the data that\u0026rsquo;s been entrusted to you.\nCaring for your data Once you decide to accept other people\u0026rsquo;s data, it becomes your responsibility to care for it. This is applicable to everything you build. For Simple Subscribe, it means maintaining the security of your database, and periodically pruning your table.\nIn order to avoid retaining email addresses where confirm is false past a certain time frame, it would be a good idea to set up a cleaning function that runs on a regular schedule. This can be achieved manually, with an AWS Lambda function, or using the command line.\nTo clean up, find database items where confirm is false and timestamp is older than a particular point in time. Depending on your use case and request volumes, the frequency at which you choose to clean up will vary.\nAlso depending on your use case, you may wish to keep backups of your data. If you are particularly concerned about data integrity, you can explore On-Demand Backup or Point-in-Time Recovery for DynamoDB.\nBuild your independent subscriber base Building your own subscriber list can be an empowering endeavor! Whether you intend to start a newsletter, send out notifications for new content, or want to create a community around your work, there\u0026rsquo;s nothing more personal or direct than an email from me to you.\nI encourage you to start building your subscriber base with Simple Subscribe today! Like most of my work, it\u0026rsquo;s open source and free for your personal use. Dive into the code at the GitHub repository or learn more at SimpleSubscribe.org.\n",url:"https://victoria.dev/blog/build-your-own-serverless-subscriber-list-with-go-and-aws/"},"https:\/\/victoria.dev\/neofeed\/29941\/":{title:"29941",tags:[],content:"Nothing alleviates boredom snacking quite so well as centering things in CSS.\n",url:"https://victoria.dev/neofeed/29941/"},"https:\/\/victoria.dev\/neofeed\/29606\/":{title:"29606",tags:[],content:"Without iteration, we cease to exist.\n",url:"https://victoria.dev/neofeed/29606/"},"https:\/\/victoria.dev\/neofeed\/29518\/":{title:"29518",tags:[],content:"I 💜 @obsdmd. I\u0026rsquo;m back to using boring old local Markdown files for notes and I couldn\u0026rsquo;t be happier.\n",url:"https://victoria.dev/neofeed/29518/"},"https:\/\/victoria.dev\/neofeed\/29238\/":{title:"29238",tags:[],content:"❓: Why is an ewe like a privacy-focused VPN?\n˙uoᴉʇɐɔᴉldǝɹ ɹoɟ w∀ᴚ ǝsn ɥʇoq ʎǝɥʇ :∀\n",url:"https://victoria.dev/neofeed/29238/"},"https:\/\/victoria.dev\/blog\/wpa-key-wpa2-wpa3-and-wep-key-wi-fi-security-explained\/":{title:"WPA Key, WPA2, WPA3, and WEP Key: Wi-Fi security explained",tags:["computing","algorithms","protocols","cybersecurity"],content:"Setting up new Wi-Fi? Picking the type of password you need can seem like an arbitrary choice. After all, WEP, WPA, WPA2, and WPA3 all have mostly the same letters in them. A password is a password, so what\u0026rsquo;s the difference?\nAbout 60 seconds to billions of years, as it turns out.\nAll Wi-Fi encryption is not created equal. Let\u0026rsquo;s explore what makes these four acronyms so different, and how you can best protect your home and organization Wi-Fi.\nWired Equivalent Privacy (WEP) In the beginning, there was WEP.\n Not to be confused with the name of a certain rap song.\n  Wired Equivalent Privacy is a deprecated security algorithm from 1997 that was intended to provide equivalent security to a wired connection. \u0026ldquo;Deprecated\u0026rdquo; means, \u0026ldquo;Let\u0026rsquo;s not do that anymore.\u0026rdquo;\nEven when it was first introduced, it was known not to be as strong as it could have been, for two reasons: one, its underlying encryption mechanism; and two, World War II.\nDuring World War II, the impact of code breaking (or cryptanalysis) was huge. Governments reacted by attempting to keep their best secret-sauce recipes at home. Around the time of WEP, U.S. Government restrictions on the export of cryptographic technology caused access point manufacturers to limit their devices to 64-bit encryption. Though this was later lifted to 128-bit, even this form of encryption offered a very limited possible key size.\nThis proved problematic for WEP. The small key size resulted in being easier to brute-force, especially when that key doesn\u0026rsquo;t often change.\nWEP\u0026rsquo;s underlying encryption mechanism is the RC4 stream cipher. This cipher gained popularity due to its speed and simplicity, but that came at a cost. It\u0026rsquo;s not the most robust algorithm. WEP employs a single shared key among its users that must be manually entered on an access point device. (When\u0026rsquo;s the last time you changed your Wi-Fi password? Right.) WEP didn\u0026rsquo;t help matters either by simply concatenating the key with the initialization vector \u0026ndash; which is to say, it sort of mashed its secret-sauce bits together and hoped for the best.\n Initialization Vector (IV): fixed-size input to a low-level cryptographic algorithm, usually random.\n Combined with the use of RC4, this left WEP particularly susceptible to related-key attack. In the case of 128-bit WEP, your Wi-Fi password can be cracked by publicly-available tools in a matter of around 60 seconds to three minutes.\nWhile some devices came to offer 152-bit or 256-bit WEP variants, this failed to solve the fundamental problems of WEP\u0026rsquo;s underlying encryption mechanism.\nSo, yeah. Let\u0026rsquo;s not do that anymore.\nWi-Fi Protected Access (WPA) A new, interim standard sought to temporarily \u0026ldquo;patch\u0026rdquo; the problem of WEP\u0026rsquo;s (lack of) security. The name Wi-Fi Protected Access (WPA) certainly sounds more secure, so that\u0026rsquo;s a good start; however, WPA first started out with another, more descriptive name.\nRatified in a 2004 IEEE standard, Temporal Key Integrity Protocol (TKIP) uses a dynamically-generated, per-packet key. Each packet sent has a unique temporal 128-bit key, (See? Descriptive!) that solves the susceptibility to related-key attacks brought on by WEP\u0026rsquo;s shared key mashing.\nTKIP also implements other measures, such as a message authentication code (MAC). Sometimes known as a checksum, a MAC provides a cryptographic way to verify that messages haven\u0026rsquo;t been changed. In TKIP, an invalid MAC can also trigger rekeying of the session key. If the access point receives an invalid MAC twice within a minute, the attempted intrusion can be countered by changing the key an attacker is trying to crack.\nUnfortunately, in order to preserve compatibility with the existing hardware that WPA was meant to \u0026ldquo;patch,\u0026rdquo; TKIP retained the use of the same underlying encryption mechanism as WEP \u0026ndash; the RC4 stream cipher. While it certainly improved on the weaknesses of WEP, TKIP eventually proved vulnerable to new attacks that extended previous attacks on WEP. These attacks take a little longer to execute by comparison: for example, twelve minutes in the case of one, and 52 hours in another. This is more than sufficient, however, to deem TKIP no longer secure.\nWPA, or TKIP, has since been deprecated as well. So let\u0026rsquo;s also not do that anymore.\nWhich brings us to\u0026hellip;\nWi-Fi Protected Access II (WPA2) Rather than spend the effort to come up with an entirely new name, the improved Wi-Fi Protected Access II (WPA2) standard instead focuses on using a new underlying cipher. Instead of the RC4 stream cipher, WPA2 employs a block cipher called Advanced Encryption Standard (AES) to form the basis of its encryption protocol. The protocol itself, abbreviated CCMP, draws most of its security from the length of its rather long name (I\u0026rsquo;m kidding): Counter Mode Cipher Block Chaining Message Authentication Code Protocol, which shortens to Counter Mode CBC-MAC Protocol, or CCM mode Protocol, or CCMP. 🤷\nCCM mode is essentially a combination of a few good ideas. It provides data confidentiality through CTR mode, or counter mode. To vastly oversimplify, this adds complexity to plaintext data by encrypting the successive values of a count sequence that does not repeat. CCM also integrates CBC-MAC, a block cipher method for constructing a MAC.\nAES itself is on good footing. The AES specification was established in 2001 by the U.S. National Institute of Standards and Technology (NIST) after a five-year competitive selection process during which fifteen proposals for algorithm designs were evaluated. As a result of this process, a family of ciphers called Rijndael (Dutch) was selected, and a subset of these became AES. For the better part of two decades, AES has been used to protect every-day Internet traffic as well as certain levels of classified information in the U.S. Government.\nWhile possible attacks on AES have been described, none have yet been proven to be practical in real-world use. The fastest attack on AES in public knowledge is a key-recovery attack that improved on brute-forcing AES by a factor of about four. How long would it take? Some billions of years.\nWi-Fi Protected Access III (WPA3) The next installment of the WPA trilogy has been required for new devices since July 1, 2020. Expected to further enhance the security of WPA2, the WPA3 standard seeks to improve password security by being more resilient to word list or dictionary attacks.\nUnlike its predecessors, WPA3 will also offer forward secrecy. This adds the considerable benefit of protecting previously exchanged information even if a long-term secret key is compromised. Forward secrecy is already provided by protocols like TLS by using asymmetric keys to establish shared keys. You can learn more about TLS in this post.\nAs WPA2 has not been deprecated, both WPA2 and WPA3 remain your top choices for Wi-Fi security.\nIf the other ones suck, why are they still around? You may be wondering why your access point even allows you to choose an option other than WPA2 or WPA3. The likely reason is that you\u0026rsquo;re using legacy hardware, which is what tech people call your mom\u0026rsquo;s router.\nSince the deprecation of WEP and WPA occurred (in old-people terms) rather recently, it\u0026rsquo;s possible in large organizations as well as your parent\u0026rsquo;s house to find older hardware that still uses these protocols. Even newer hardware may have a business need to support these older protocols.\nWhile I may be able to convince you to invest in a shiny new top-of-the-line Wi-Fi appliance, most organizations are a different story. Unfortunately, many just aren\u0026rsquo;t yet cognizant of the important role cybersecurity plays in meeting customer needs and boosting that bottom line. Additionally, switching to newer protocols may require new internal hardware or firmware upgrades. Especially on complex systems in large organizations, upgrading devices can be financially or strategically difficult.\nBoost your Wi-Fi security If it\u0026rsquo;s an option, choose WPA2 or WPA3. Cybersecurity is a field that evolves by the day, and getting stuck in the past can have dire consequences.\nIf you can\u0026rsquo;t use WPA2 or WPA3, do the best you can to take additional security measures. The best bang for your buck is to use a Virtual Private Network (VPN). Using a VPN is a good idea no matter which type of Wi-Fi encryption you have. On open Wi-Fi (coffee shops) and using WEP, it\u0026rsquo;s plain irresponsible to go without a VPN. Kind of like shouting out your bank details as you order your second cappuccino.\n Choose a VPN provider that offers a feature like ExpressVPN's Network Lock with automatic connection. This is a kill switch that blocks your network traffic if your VPN becomes disconnected, preventing you from accidentally transmitting information on an insecure connection like open Wi-Fi or WEP. I personally use ExpressVPN. Their auto-connect feature works well on Linux, and I never worry about remembering to start it up first. I wrote more about choosing a VPN in this post.\nWhen possible, ensure you only connect to known networks that you or your organization control. Many cybersecurity attacks are executed when victims connect to an imitation public Wi-Fi access point, also called an evil twin attack, or Wi-Fi phishing. These fake hotspots are easily created using publicly accessible programs and tools. A VPN can help mitigate damage from these attacks as well, but it\u0026rsquo;s always better not to take the risk. If you travel often, consider purchasing a portable hotspot that uses a cellular data plan, or using data SIM cards for all your devices.\nMuch more than just acronyms WEP, WPA, WPA2, and WPA3 mean a lot more than a bunch of similar letters \u0026ndash; in some cases, it\u0026rsquo;s a difference of billions of years minus about 60 seconds.\nOn more of a now-ish timescale, I hope I\u0026rsquo;ve taught you something new about the security of your Wi-Fi and how you can improve it!\nIf you\u0026rsquo;d like to thank me, you can protect yourself with ExpressVPN with my referral. I only refer products I use personally, and this one is solid.\nKnow someone who\u0026rsquo;d benefit from some beefed up cybersecurity? Share the cybersecurity starter pack!\n",url:"https://victoria.dev/blog/wpa-key-wpa2-wpa3-and-wep-key-wi-fi-security-explained/"},"https:\/\/victoria.dev\/blog\/three-rules-for-choosing-a-vpn-that-takes-your-privacy-seriously\/":{title:"Three rules for choosing a VPN that takes your privacy seriously",tags:["privacy","cybersecurity","data","life","linux"],content:"Most people know that a VPN is meant to protect your privacy on public or open Wi-Fi. A lesser-known purpose is to protect your privacy right in your own home, from your own internet service provider (ISP).\nA set of Federal Communications Commission (FCC) rules entitled \u0026ldquo;Protecting the Privacy of Customers of Broadband and Other Telecommunications Services\u0026rdquo; were unfortunately struck down in 2017. These rules would have prevented ISPs from using and selling your sensitive personal data, such as precise geographic location, health and financial information, web browsing history, and even the content of the messages you send.\nI\u0026rsquo;m not comfortable having that data stored anywhere. Handing it over to my ISP makes me even less comfortable, since these treasure troves of sensitive personal data are a frequent and profitable target for ill-intentioned hackers as well.\nYour online activities shouldn\u0026rsquo;t be anyone\u0026rsquo;s business, and certainly not in a literal money-making sense. Using a VPN helps to keep your private information where it belongs: between you and the person you\u0026rsquo;re sending it to.\nOf course, if you type any flavor of \u0026ldquo;VPN vs VPN\u0026rdquo; into a search you\u0026rsquo;ll get a smorgasbord of comparison blogs and providers vying for your attention. How do you know what makes a VPN \u0026ldquo;good\u0026rdquo;? (Answer: lots of research.)\nIf you\u0026rsquo;re a regular reader, you know I\u0026rsquo;m big on security and privacy. (If you\u0026rsquo;re not yet, welcome! Hi!) Since I\u0026rsquo;ve built my career in the cybersecurity industry, I take my VPN fairly seriously. Here are the top three things I look for when choosing my own VPN provider:\n No DNS leaks A real commitment to privacy, with no logs Ease of use across all operating systems  I\u0026rsquo;ve written about why a VPN is important and even how to deal with the challenges of DNS leaks when using OpenVPN to set up my own. The response I often get to articles like these includes the question, \u0026ldquo;Which VPN do you use?\u0026rdquo;\nThe answer is ExpressVPN. Here\u0026rsquo;s how my privacy philosophy got me there, and why these three points matter so much.\n TL;DR: ExpressVPN is built on privacy. You can set-it-and-forget-it with three extra months free.\n Why you don\u0026rsquo;t want a DNS leak In a previous Linux-flavored adventure, I created my own VPN using OpenVPN and AWS EC2. While I\u0026rsquo;ve been told my post was helpful, this was definitely not a plug-and-play solution. After reinstalling a new OS, I once failed to follow my own guide to the letter. It took a few months before I discovered I had a DNS leak.\n Using a VPN prevents your ISP from collecting your sensitive personal data, including your web browsing history, but only as long as you don\u0026rsquo;t have a DNS leak. A DNS leak means that your ISP still sees all the URLs that you visit: their servers resolve them for you. This is plenty of information to build a picture of who you are, what your interests might be, any health issues you might have, what you like to spend money on, and much more.\nProtocols like DNS over HTTPS will help, but they rely on co-operation between many entities that is still in its early stages. In the meantime, I want my VPN to do everything it can to avoid using DNS servers that could collect or sell my browsing history.\nAt time of writing, there\u0026rsquo;s really only one fool-proof solution to ensuring that your browsing records aren\u0026rsquo;t accidentally shared: run your own private DNS server. So ExpressVPN did just that.\nOf course, this only works in my favor when the VPN itself doesn\u0026rsquo;t keep a record of my activities. Which is why\u0026hellip;\nA no-logs philosophy matters VPN providers do not all value your privacy, and some are no better than your ISP. Many VPN providers, especially free ones, elect to log your personal data and sell it to data brokers and marketers. Using a VPN that does any kind of logging simply transfers the risk from your ISP to the VPN provider.\nAt a minimum, you want a VPN provider to clearly state a strict no-logs policy. Of course, this still means you\u0026rsquo;ll have to trust that they aren\u0026rsquo;t being cagey with their definition of \u0026ldquo;logs,\u0026rdquo; and still writing your personal data to disk under a pretense.\nA more trustworthy solution would be to remove the possibility of writing any personal data to disk in the first place. So ExpressVPN got rid of the disks.\n Dad joke. I know.\n  I was pretty thrilled to learn about what ExpressVPN calls TrustedServer, which runs only on random-access memory, or RAM, and not on hard drives. Unlike a disk meant for long-term, fault-tolerant storage, RAM is volatile memory. It requires constant power to operate, which guarantees that all data is lost when the server is rebooted.\nWhile you wouldn\u0026rsquo;t want a laptop that runs entirely on RAM, volatile memory is perfectly suited to an ephemeral, no-logs VPN server. The entire software stack including the OS must be re-installed from a central, signed image each time the server boots. This also means it\u0026rsquo;s always installing the most up-to-date security patches and configuration. That\u0026rsquo;s clever.\nThis post goes into more technical detail on TrustedServer, which was independently audited by PricewaterhouseCoopers.\nAs a Director of Engineering myself, I have a deep appreciation for a company that builds its technology on its philosophy.\nThat said, the technology only works if you actually use it.\nThe best VPN is one you actually use None of what I\u0026rsquo;ve said so far would matter one iota if my chosen VPN was even just a little bit inconvenient to use.\nMy preferred platforms are Linux and iOS. I\u0026rsquo;ve had my fair share of struggles finding all kinds of software that works equally well on just these two. ExpressVPN seems to offer one of the few applications I\u0026rsquo;ve come across that isn\u0026rsquo;t trapped in an ecosystem.\nThere\u0026rsquo;s a dedicated app for every major platform, including even smart TVs and game consoles. Unlike my experiences with other VPNs, ExpressVPN's Linux app just works, out-of-the-box, the way they said it would.\n All the devices!\n  I especially appreciate the Network Lock kill switch feature, which prevents me from accidentally sending unprotected network traffic when I first open up my laptop and it reconnects to Wi-Fi. It prevents my ISP from seeing anything I do, and only takes a few seconds to reconnect.\nExpressVPN connects fast and then gets out of my way. I haven\u0026rsquo;t noticed any reduced speeds or blocked sites. I gave a lot of thought to choosing my VPN so I wouldn\u0026rsquo;t have to think about it on a day-to-day basis. I use ExpressVPN constantly, and it just works.\nPrivacy is more than personal When you protect yourself and your family with a VPN, you improve more than your own personal cybersecurity. The less data your ISP can collect, the less they have to lose, sell, or profit from. One day, the risk and cost for ISPs will outweigh the payoff. When you take action to prevent ISPs from scooping up your family\u0026rsquo;s sensitive personal data, everyone\u0026rsquo;s privacy can benefit.\nIf you found this article helpful, I invite you to help support my work by signing up for ExpressVPN. It only takes a few minutes (assuming you remember where you left your credit card) and will give you the best possible set-it-and-forget-it privacy protection that I can recommend.\n",url:"https://victoria.dev/blog/three-rules-for-choosing-a-vpn-that-takes-your-privacy-seriously/"},"https:\/\/victoria.dev\/neofeed\/28600\/":{title:"28600",tags:[],content:"Chop off the end of the string to make Python\u0026rsquo;s datetime.fromisoformat() accept datetime strings that end in \u0026ldquo;Z\u0026rdquo;:\ndatetime_obj = datetime.fromisoformat(input_time_str_var[:-1])\nBackground discussion and why this isn\u0026rsquo;t a thing:\n discuss.python.org bugs.python.org  ",url:"https://victoria.dev/neofeed/28600/"},"https:\/\/victoria.dev\/neofeed\/27830\/":{title:"27830",tags:[],content:"I’ve passed 100,000 words written on my blog! That’s enough for a book, or two. It’s also:\n The limit of most Ph.D. dissertations About 2.5 novels Dickens on an off day About 357 of today’s tweets, or 714 when Twitter was good  Can’t stop won’t stop.\n",url:"https://victoria.dev/neofeed/27830/"},"https:\/\/victoria.dev\/blog\/your-cybersecurity-starter-pack\/":{title:"Your cybersecurity starter pack",tags:["cybersecurity"],content:"Readers of my blog typically know more about technology and cybersecurity than most people. This article is for most people. If someone you know could benefit from a simple and straightforward introduction to cybersecurity tools, please share this article with them \u0026ndash; it benefits everyone!\nIf you\u0026rsquo;ve ever said to yourself:\n\u0026ldquo;There\u0026rsquo;s no one targeting lil ol' me.\u0026quot;\n\u0026ldquo;I have nothing to hide, anyway.\u0026quot;\n\u0026ldquo;I\u0026rsquo;m too busy to learn all this stuff. Why can\u0026rsquo;t someone just give me a simple summary of best practices that I can skim in approximately seven minutes?\u0026quot;\nFirst of all, you might want to stop talking to yourself in public. Secondly, here is a simple summary of best practices that you can skim in approximately seven minutes.\nIntroducing your three-step starter pack While there are many different degrees of security, privacy, and anonymity, these three basics are accessible to all:\n Use a VPN Use multifactor authentication Develop a healthy sense of skepticism  I\u0026rsquo;ll discuss each of these and help you get started with your security upgrade. But first\u0026hellip;\nWhy is cybersecurity important? Would you let just anyone walk into your house, or even look through your open doorway from across the street? If not, you might appreciate that the cybersecurity practices we\u0026rsquo;ll discuss today are not that different from locking your front door.\nCybersecurity isn\u0026rsquo;t about finding some magic spell that completely secures your online activities \u0026ndash; that would be nice, but it\u0026rsquo;s unrealistic. Good security practices are about employing some thoughtful habits that make your online activities more secure than the next guy, in much the same way as you learned to lock your front door.\nSecurity breaches and incidents happen every day. Most of them occur because an automated scanner cast a wide net and found a person or company with lax security that a hacker could then exploit. Don\u0026rsquo;t be that guy.\n1. Use a VPN Let\u0026rsquo;s say you send a lot of mail, but never bother to put your letters in envelopes or even fold them in half. Anyone who bothers to look can read all your dirty secrets (not that you have any).\nWhen you use a Virtual Private Network, or VPN, especially if you often connect to public WiFi, it\u0026rsquo;s like putting your letters into cryptographically-sealed envelopes and sending them via a special invisible courier service. No one but the intended recipient can read your letters, and no one but you and the courier know to whom the letters are sent.\n Encrypted mail still won\u0026rsquo;t stop you from the accidental reply all, unfortunately.\n  VPNs prevent others from reading your communications. This may include opportunistic attackers who scan open WiFi, and even your own internet service provider (ISP) who may sell your usage data for advertising dollars.\nChoosing a VPN A few important differentiating factors can help you choose a VPN provider.\n  Is it free? VPNs cost money to operate; if one is offered for free, consider what they might be doing in order to cover their costs. Generally, I recommend avoiding free VPN apps and services; they\u0026rsquo;ll typically cost you much more than you\u0026rsquo;ll know. Expect to pay between $5-$10 USD monthly for the service.\n  Where is it based? Understand where your VPN provider is based, and what that country\u0026rsquo;s laws allow them to do with your data.\n  Do they keep logs? Part of the philosophy of using a VPN is that no one has any business getting into your business when it comes to online activities. When a VPN provider keeps logs of your usage, that defeats the purpose. Instead of your ISP knowing just what you\u0026rsquo;re up to online, that knowledge is simply transferred to the logging VPN. Look for VPN providers with a strict no-logging policy.\n  I use ExpressVPN. I go into greater detail about choosing a VPN in this post.\n2. Use multifactor authentication Passwords are dead. Computationally, they are a solved problem. Cracking your password is just a matter of time.\nUnfortunately, many people still help to speed up the process by using the same compromised passwords for multiple accounts, putting themselves at further risk.\nThe answer, at least for now, is multifactor authentication (MFA). MFA is made up of three kinds of authentication factors:\n Something you know, like a pass phrase; Something you have, like a chip pin card or phone; and Something that you are, like your face or fingerprint.   Also the name of my next beatboxing team.\n  Two or more of these factors are infinitely better than a password alone, especially if your password is on this list.\nMultiple authentication factors are now widely supported by account providers and social media sites. If you have the choice, avoid using text messages, or SMS, as a way of receiving authentication codes. SMS authentication leaves you vulnerable to the SIM swap attack - please direct further questions to Jack Dorsey.\nInstead, use a One Time Password (OTP) app such as Authy to generate codes on your device. This ensures that you alone, using that particular device, will have the correct authentication code.\nYou can also use hardware authentication keys such as the YubiKey, but these aren\u0026rsquo;t yet as widely supported as OTP apps.\n3. Develop a healthy sense of skepticism Social engineering, sometimes SE, is the use of psychological persuasion to get an unwitting target to give up access or information. This can take the form of phishing emails, letters, or phone calls (vishing) as well as far more sophisticated spear-phishing attacks of high-value targets, like company executives.\nWhile some attacks are easier to spot, others use cognitive biases very effectively and are difficult even for security professionals to avoid. No human is immune.\nUltimately, the weakest link in your cybersecurity defense is you. All the VPNs and MFA on the Internet won\u0026rsquo;t protect you if a scam can trick you into opening the front gates. Always look a Trojan gift horse in the mouth.\n Yes, I know it\u0026rsquo;s a very nice looking wooden horse. Also free. Did you order it? No? Then it can stay outside.\n  Develop the habit of second-guessing things delivered to your virtual doorstep. Email, phone, and messaging scams range in sophistication. Even security professionals can fall for a good scam.\nOne way to protect yourself is to practice a healthy sense of skepticism. Question communications that ask you to click on links or visit a website, even if they come from someone you know or a company you use.\nIf you\u0026rsquo;re not certain that your bank or mother sent this email, pick up the phone and call them. Even if you think you are certain, pick up the phone and double check. You don\u0026rsquo;t call your mother enough, anyway.\nOh, and if the person on the phone is from your local tax office or the IRS or the CRA and they\u0026rsquo;re about to freeze your accounts because a case of mistaken identity has resulted in you being criminally charged for not repaying a loan on a 600-foot yacht in Malibu, just hang up. You know better than that. Tax agencies don\u0026rsquo;t have phones.\nA safer Internet Congratulations! You now have three tools to make your personal cybersecurity better than the next guy\u0026rsquo;s. If enough people do that, the whole neighborhood (or in this case, the Internet) will benefit as a result.\nIf this article piqued your interest, you can go further and outsource your security with a password manager and temporary virtual credit cards.\nCheat sheets and other resources I\u0026rsquo;ll leave you with a few resources that I\u0026rsquo;ve enjoyed:\n The Electronic Frontier Foundation website Surveillance Self Defense offers many great guides and how-to\u0026rsquo;s, such as setting up the encrypted messaging app Signal on your mobile device, and protecting yourself on social media. The Cybersecurity and Infrastructure Security Agency (CISA) offers many shareable starter resources. Working from home? The National Security Agency Central Security Service has Telework and Mobile Security Guides that discuss best practices for an unprecedented era of remote work.  ",url:"https://victoria.dev/blog/your-cybersecurity-starter-pack/"},"https:\/\/victoria.dev\/blog\/increase-developer-confidence-with-a-great-django-test-suite\/":{title:"Increase developer confidence with a great Django test suite",tags:["coding","tech-team","python"],content:"If you regard writing tests as a lame checkbox task, nothing could be farther from the truth. Done correctly, tests are one of your application\u0026rsquo;s most valuable assets.\nThe Django framework in particular offers your team the opportunity to create an efficient testing practice, based on the Python standard library unittest. Proper tests in Django are fast to write, faster to run, and can offer you a seamless continuous integration solution for taking the pulse of your developing application.\nWith comprehensive tests, developers have higher confidence when pushing changes. I\u0026rsquo;ve seen firsthand in my own teams that good tests can boost development velocity as a direct result of a better developer experience.\nIn this article, I\u0026rsquo;ll share my own experiences in building useful tests for Django applications, from the basics to the best possible execution. If you\u0026rsquo;re using Django or building with it in your organization, you might like to read the rest of my Django series.\nWhat to test Tests are extremely important. Far beyond simply letting you know if a function works, tests can form the basis of your team\u0026rsquo;s understanding of how your application is intended to work.\nHere\u0026rsquo;s the main goal: if you hit your head and forgot everything about how your application works tomorrow, you should be able to regain most of your understanding by reading and running the tests you write today.\nHere are some questions that may be helpful to ask as you decide what to test:\n What is our customer supposed to be able to do? What is our customer not supposed to be able to do? What should this method, view, or logical flow achieve? When, how, or where is this feature supposed to execute?  Tests that make sense for your application can help build developer confidence. With these sensible safeguards in place, developers make improvements more readily, and feel confident introducing innovative solutions to product needs. The result is an application that comes together faster, and features that are shipped often and with confidence.\nWhere to put tests If you only have a few tests, you may organize your test files similarly to Django\u0026rsquo;s default app template by putting them all in a file called tests.py. This straightforward approach is best for smaller applications.\nAs your application grows, you may like to split your tests into different files, or test modules. One method is to use a directory to organize your files, such as projectroot/app/tests/. The name of each test file within that directory should begin with test, for example, test_models.py.\nBesides being aptly named, Django will find these files using built-in test discovery based on the unittest module. All files in your application with names that begin with test will be collected into a test suite.\nThis convenient test discovery allows you to place test files anywhere that makes sense for your application. As long as they\u0026rsquo;re correctly named, Django\u0026rsquo;s test utility can find and run them.\nHow to document a test Use docstrings to explain what a test is intended to verify at a high level. For example:\ndef test_create_user(self): \u0026#34;\u0026#34;\u0026#34;Creating a new user object should also create an associated profile object\u0026#34;\u0026#34;\u0026#34; # ... These docstrings help you quickly understand what a test is supposed to be doing. Besides navigating the codebase, this helps to make it obvious when a test doesn\u0026rsquo;t verify what the docstring says it should.\nDocstrings are also shown when the tests are being run, which can be helpful for logging and debugging.\nWhat a test needs to work Django tests can be quickly set up using data created in the setUpTestData() method. You can use various approaches to create your test data, such as utilizing external files, or even hard-coding silly phrases or the names of your staff. Personally, I much prefer to use a fake-data-generation library, such as faker.\nThe proper set up of arbitrary testing data can help you ensure that you\u0026rsquo;re testing your application functionality instead of accidentally testing test data. Because generators like faker add some degree of unexpectedness to your inputs, it can be more representative of real-world use.\nHere is an example set up for a test:\nfrom django.test import TestCase from faker import Faker from app.models import MyModel, AnotherModel fake = Faker() class MyModelTest(TestCase): def setUpTestData(cls): \u0026#34;\u0026#34;\u0026#34;Quickly set up data for the whole TestCase\u0026#34;\u0026#34;\u0026#34; cls.user_first = fake.first_name() cls.user_last = fake.last_name() def test_create_models(self): \u0026#34;\u0026#34;\u0026#34;Creating a MyModel object should also create AnotherModel object\u0026#34;\u0026#34;\u0026#34; # In test methods, use the variables created above test_object = MyModel.objects.create( first_name=self.user_first, last_name=self.user_last, # ... ) another_model = AnotherModel.objects.get(my_model=test_object) self.assertEqual(another_model.first_name, self.user_first) # ... Tests pass or fail based on the outcome of the assertion methods. You can use Python\u0026rsquo;s unittest methods, and Django\u0026rsquo;s assertion methods.\nFor further guidance on writing tests, see Testing in Django.\nBest possible execution for running your tests Django\u0026rsquo;s test suite is manually run with:\n./manage.py test I rarely run my Django tests this way.\nThe best, or most efficient, testing practice is one that occurs without you or your developers ever thinking, \u0026ldquo;I need to run the tests first.\u0026rdquo; The beauty of Django\u0026rsquo;s near-effortless test suite set up is that it can be seamlessly run as a part of regular developer activities. This could be in a pre-commit hook, or in a continuous integration or deployment workflow.\nI\u0026rsquo;ve previously written about how to use pre-commit hooks to improve your developer ergonomics and save your team some brainpower. Django\u0026rsquo;s speedy tests can be run this way, and they become especially efficient if you can run tests in parallel.\nTests that run as part of a CI/CD workflow, for example, on pull requests with GitHub Actions, require no regular effort from your developers to remember to run tests at all. I\u0026rsquo;m not sure how plainly I can put it \u0026ndash; this one\u0026rsquo;s literally a no-brainer.\nTesting your way to a great Django application Tests are extremely important, and underappreciated. They can catch logical errors in your application. They can help explain and validate how concepts and features of your product actually function. Best of all, tests can boost developer confidence and development velocity as a result.\nThe best tests are ones that are relevant, help to explain and define your application, and are run continuously without a second thought. I hope I\u0026rsquo;ve now shown you how testing in Django can help you to achieve these goals for your team!\n",url:"https://victoria.dev/blog/increase-developer-confidence-with-a-great-django-test-suite/"},"https:\/\/victoria.dev\/neofeed\/27423\/":{title:"27423",tags:[],content:"I won\u0026rsquo;t be convinced that Apple really understands the current world until they start putting the higher MP camera on the front side of the phone.\n",url:"https://victoria.dev/neofeed/27423/"},"https:\/\/victoria.dev\/neofeed\/27329\/":{title:"27329",tags:[],content:"A programmer and a mathematician draw a cartoon together\u0026hellip;\n",url:"https://victoria.dev/neofeed/27329/"},"https:\/\/victoria.dev\/neofeed\/26935\/":{title:"26935",tags:[],content:"If you want to be taken seriously as a developer, stop using these phrases:\n 🧸 “Playing around with” 🔩 “Tinker” 🤷 “Some stuff”  ",url:"https://victoria.dev/neofeed/26935/"},"https:\/\/victoria.dev\/blog\/django-project-best-practices-to-keep-your-developers-happy\/":{title:"Django project best practices to keep your developers happy",tags:["python","tech-team","leadership","coding","docs"],content:"Do you want your team to enjoy your development workflow? Do you think building software should be fun and existentially fulfilling? If so, this is the post for you!\nI\u0026rsquo;ve been developing with Django for years, and I\u0026rsquo;ve never been happier with my Django project set up than I am right now. Here\u0026rsquo;s how I\u0026rsquo;m making a day of developing with Django the most relaxing and enjoyable development experience possible for myself and my engineering team.\nA custom CLI tool for your Django project Instead of typing:\npython3 -m venv env source env/bin/activate pip install -r requirements.txt python3 manage.py makemigrations python3 manage.py migrate python3 manage.py collectstatic python3 manage.py runserver Wouldn\u0026rsquo;t it be much nicer to type:\nmake start \u0026hellip;and have all that happen for you? I think so!\nWe can do that with a self-documenting Makefile! Here\u0026rsquo;s one I frequently use when developing my Django applications, like ApplyByAPI.com:\nVENV := env BIN := $(VENV)/bin PYTHON := $(BIN)/python SHELL := /bin/bash include .env .PHONY: help help: ## Show this help  @egrep -h \u0026#39;\\s##\\s\u0026#39; $(MAKEFILE_LIST) | awk \u0026#39;BEGIN {FS = \u0026#34;:.*?## \u0026#34;}; {printf \u0026#34;\\033[36m%-20s\\033[0m %s\\n\u0026#34;, $$1, $$2}\u0026#39; .PHONY: venv venv: ## Make a new virtual environment  python3 -m venv $(VENV) \u0026amp;\u0026amp; source $(BIN)/activate .PHONY: install install: venv ## Make venv and install requirements  $(BIN)/pip install --upgrade -r requirements.txt freeze: ## Pin current dependencies  $(BIN)/pip freeze \u0026gt; requirements.txt migrate: ## Make and run migrations  $(PYTHON) manage.py makemigrations $(PYTHON) manage.py migrate db-up: ## Pull and start the Docker Postgres container in the background  docker pull postgres docker-compose up -d db-shell: ## Access the Postgres Docker database interactively with psql. Pass in DBNAME=\u0026lt;name\u0026gt;.  docker exec -it container_name psql -d $(DBNAME) .PHONY: test test: ## Run tests  $(PYTHON) manage.py test application --verbosity=0 --parallel --failfast .PHONY: run run: ## Run the Django server  $(PYTHON) manage.py runserver start: install migrate run ## Install requirements, apply migrations, then start development server You\u0026rsquo;ll notice the presence of the line include .env above. This ensures make has access to environment variables stored in a file called .env. This allows Make to utilize these variables in its commands, for example, the name of my virtual environment, or to pass in $(DBNAME) to psql.\nWhat\u0026rsquo;s with that weird \u0026ldquo;##\u0026rdquo; comment syntax? A Makefile like this gives you a handy suite of command-line aliases you can check in to your Django project. It\u0026rsquo;s very useful so long as you\u0026rsquo;re able to remember what all those aliases are.\nThe help command above, which runs by default, prints a helpful list of available commands when you run make or make help:\nhelp Show this help venv Make a new virtual environment install Make venv and install requirements migrate Make and run migrations db-up Pull and start the Docker Postgres container in the background db-shell Access the Postgres Docker database interactively with psql test Run tests run Run the Django server start Install requirements, apply migrations, then start development server All the usual Django commands are covered, and we\u0026rsquo;ve got a test command that runs our tests with the options we prefer. Brilliant.\nYou can read my full post about self-documenting Makefiles here, which also includes an example Makefile using pipenv.\nSave your brainpower with pre-commit hooks I previously wrote about some technical ergonomics that can make it a lot easier for teams to develop great software.\nOne area that\u0026rsquo;s a no-brainer is using pre-commit hooks to lint code prior to checking it in. This helps to ensure the quality of the code your developers check in, but most importantly, ensures that no one on your team is spending time trying to remember if it should be single or double quotes or where to put a line break.\nThe confusingly-named pre-commit framework is an otherwise fantastic way to keep hooks (which are not included in cloned repositories) consistent across local environments.\nHere is my configuration file, .pre-commit-config.yaml, for my Django projects:\nfail_fast:truerepos:- repo:https://github.com/pre-commit/pre-commit-hooksrev:v3.1.0hooks:- id:detect-aws-credentials- repo:https://github.com/psf/blackrev:19.3b0hooks:- id:black- repo:https://github.com/asottile/blacken-docsrev:v1.7.0hooks:- id:blacken-docsadditional_dependencies:[black==19.3b0]- repo:localhooks:- id:markdownlintname:markdownlintdescription:\u0026#34;Lint Markdown files\u0026#34;entry:markdownlint \u0026#39;**/*.md\u0026#39; --fix --ignore node_modules --config \u0026#34;./.markdownlint.json\u0026#34;language:nodetypes:[markdown]These hooks check for accidental secret commits, format Python files using Black, format Python snippets in Markdown files using blacken-docs, and lint Markdown files as well. To install them, just type pre-commit install.\nThere are likely even more useful hooks available for your particular use case: see supported hooks to explore.\nUseful gitignores An underappreciated way to improve your team\u0026rsquo;s daily development experience is to make sure your project uses a well-rounded .gitignore file. It can help prevent files containing secrets from being committed, and can additionally save developers hours of tedium by ensuring you\u0026rsquo;re never sifting through a git diff of generated files.\nTo efficiently create a gitignore for Python and Django projects, Toptal\u0026rsquo;s gitignore.io can be a nice resource for generating a robust .gitignore file.\nI still recommend examining the generated results yourself to ensure that ignored files suit your use case, and that nothing you want ignored is commented out.\nContinuous testing with GitHub Actions If your team works on GitHub, setting up a testing process with Actions is low-hanging fruit.\nTests that run in a consistent environment on every pull request can help eliminate \u0026ldquo;works on my machine\u0026rdquo; conundrums, as well as ensure no one\u0026rsquo;s sitting around waiting for a test to run locally.\nA hosted CI environment like GitHub Actions can also help when running integration tests that require using managed services resources. You can use encrypted secrets in a repository to grant the Actions runner access to resources in a testing environment, without worrying about creating testing resources and access keys for each of your developers to use.\nI\u0026rsquo;ve written on many occasions about setting up Actions workflows, including using one to run your Makefile, and how to integrate GitHub event data. GitHub even interviewed me about Actions once.\nFor Django projects, here\u0026rsquo;s a GitHub Actions workflow that runs tests with a consistent Python version whenever someone opens a pull request in the repository.\nname:Run Django testson:pull_requestjobs:test:runs-on:ubuntu-lateststeps:- uses:actions/checkout@v2- name:Set up Pythonuses:actions/setup-python@v2with:python-version:\u0026#39;3.8\u0026#39;- name:Install dependenciesrun:make install- name:Run testsrun:make testFor the installation and test commands, I\u0026rsquo;ve simply utilized the Makefile that\u0026rsquo;s been checked in to the repository. A benefit of using your Makefile commands in your CI test workflows is that you only need to keep them updated in one place \u0026ndash; your Makefile! No more \u0026ldquo;why is this working locally but not in CI??!?\u0026rdquo; headaches.\nIf you want to step up your security game, you can add Django Security Check as an Action too.\nSet up your Django project for success Want to help keep your development team happy? Set them up for success with these best practices for Django development. Remember, an ounce of brainpower is worth a pound of software!\n",url:"https://victoria.dev/blog/django-project-best-practices-to-keep-your-developers-happy/"},"https:\/\/victoria.dev\/blog\/manipulating-data-with-django-migrations\/":{title:"Manipulating data with Django migrations",tags:["coding","data","python"],content:"Growing, successful applications are a lovely problem to have. As a product develops, it tends to accumulate complication the way your weekend cake project accumulates layers of frosting. Thankfully, Django, my favorite batteries-included framework, handles complexity pretty well.\nDjango models help humans work with data in a way that makes sense to our brains, and the framework offers plenty of classes you can inherit to help you rapidly develop a robust application from scratch. As for developing on existing Django applications, there\u0026rsquo;s a feature for that, too. In this article, we\u0026rsquo;ll cover how to use Django migrations to update your existing models and database.\nWhat\u0026rsquo;s under the hood Django migrations are Python files that help you add and change things in your database tables to reflect changes in your Django models. To understand how Django migrations help you work with data, it may be helpful to understand the underlying structures we\u0026rsquo;re working with.\nWhat\u0026rsquo;s a database table If you\u0026rsquo;ve laid eyes on a spreadsheet before, you\u0026rsquo;re already most of the way to understanding a database table. In a relational database, for example, a PostgreSQL database, you can expect to see data organized into columns and rows. A relational database table may have a set number of columns and any number of rows.\nIn Django, each model is its own table. For example, here\u0026rsquo;s a Django model:\nfrom django.db import models class Lunch(models.Model): left_side = models.CharField(max_length=100, null=True) center = models.CharField(max_length=100, null=True) right_side = models.CharField(max_length=100, null=True) Each field is a column, and each row is a Django object instance of that model. Here\u0026rsquo;s a representation of a database table for the Django model \u0026ldquo;Lunch\u0026rdquo; above. In the database, its name would be lunch_table.\n   id left_side center right_side     1 Fork Plate Spoon    The model Lunch has three fields: left_side, center, and right-side. One instance of a Lunch object would have \u0026ldquo;Fork\u0026rdquo; for the left_side, a \u0026ldquo;Plate\u0026rdquo; for the center, and \u0026ldquo;Spoon\u0026rdquo; for the right_side. Django automatically adds an id field if you don\u0026rsquo;t specify a primary key.\nIf you wanted to change the name of your Lunch model, you would do so in your models.py code. For example, change \u0026ldquo;Lunch\u0026rdquo; to \u0026ldquo;Dinner,\u0026rdquo; then run python manage.py makemigrations. You\u0026rsquo;ll see:\npython manage.py makemigrations Did you rename the backend.Lunch model to Dinner? [y/N] y Migrations for \u0026#39;backend\u0026#39;: backend/migrations/0003_auto_20200922_2331.py - Rename model Lunch to Dinner Django automatically generates the appropriate migration files. The relevant line of the generated migrations file in this case would look like:\nmigrations.RenameModel(old_name=\u0026#34;Lunch\u0026#34;, new_name=\u0026#34;Dinner\u0026#34;), This operation would rename our \u0026ldquo;Lunch\u0026rdquo; model to \u0026ldquo;Dinner\u0026rdquo; while keeping everything else the same. But what if you also wanted to change the structure of the database table itself, its schema, as well as make sure that existing data ends up in the right place on your Dinner table?\nLet\u0026rsquo;s explore how to turn our Lunch model into a Dinner model that looks like this:\nfrom django.db import models class Dinner(models.Model): top_left = models.CharField(max_length=100, null=True) top_center = models.CharField(max_length=100, null=True) top_right = models.CharField(max_length=100, null=True) bottom_left = models.CharField(max_length=100, null=True) bottom_center = models.CharField(max_length=100, null=True) bottom_right = models.CharField(max_length=100, null=True) \u0026hellip;with a database table that would look like this:\n   id top_left top_center top_right bottom_left bottom_center bottom_right     1 Bread plate Spoon Glass Fork Plate Knife    Manipulating data with Django migrations Before you begin to manipulate your data, it\u0026rsquo;s always a good idea to create a backup of your database that you can restore in case something goes wrong. There are various ways to do this depending on the database you\u0026rsquo;re using. You can typically find instructions by searching for \u0026lt;your database name\u0026gt; and keywords like backup, recovery, or snapshot.\nIn order to design your migration, it\u0026rsquo;s helpful to become familiar with the available migration operations. Migrations are run step-by-step, and each operation is some flavor of adding, removing, or altering data. Like a strategic puzzle, it\u0026rsquo;s important to make model changes one step at a time so that the generated migrations have the correct result.\nWe\u0026rsquo;ve already renamed our model successfully. Now, we\u0026rsquo;ll rename the fields that hold the data we want to retain:\nclass Dinner(models.Model): bottom_left = models.CharField(max_length=100, null=True) bottom_center = models.CharField(max_length=100, null=True) top_center = models.CharField(max_length=100, null=True) Django is sometimes smart enough to determine the old and new field names correctly. You\u0026rsquo;ll be asked for confirmation:\npython manage.py makemigrations Did you rename dinner.center to dinner.bottom_center (a CharField)? [y/N] y Did you rename dinner.left_side to dinner.bottom_left (a CharField)? [y/N] y Did you rename dinner.right_side to dinner.top_center (a CharField)? [y/N] y Migrations for \u0026#39;backend\u0026#39;: backend/migrations/0004_auto_20200914_2345.py - Rename field center on dinner to bottom_center - Rename field left_side on dinner to bottom_left - Rename field right_side on dinner to top_center In some cases, you\u0026rsquo;ll want to try renaming the field and running makemigrations one at a time.\nNow that the existing fields have been migrated to their new names, add the remaining fields to the model:\nclass Dinner(models.Model): top_left = models.CharField(max_length=100, null=True) top_center = models.CharField(max_length=100, null=True) top_right = models.CharField(max_length=100, null=True) bottom_left = models.CharField(max_length=100, null=True) bottom_center = models.CharField(max_length=100, null=True) bottom_right = models.CharField(max_length=100, null=True) Running makemigrations again now gives us:\npython manage.py makemigrations Migrations for \u0026#39;backend\u0026#39;: backend/migrations/0005_auto_20200914_2351.py - Add field bottom_right to dinner - Add field top_left to dinner - Add field top_right to dinner You\u0026rsquo;re done! By generating Django migrations, you\u0026rsquo;ve successfully set up your dinner_table and moved existing data to its new spot.\nAdditional complexity You\u0026rsquo;ll notice that our Lunch and Dinner models are not very complex. Out of Django\u0026rsquo;s many model field options, we\u0026rsquo;re just using CharField. We also set null=True to let Django store empty values as NULL in the database.\nDjango migrations can handle additional complexity, such as changing field types, and whether a blank or null value is permitted. I keep Django\u0026rsquo;s model field reference handy as I work with varying types of data and different use cases.\nDe-mystified migrations I hope this article has helped you better understand Django migrations and how they work!\nNow that you can change models and manipulate existing data in your Django application, be sure to use your powers wisely! Backup your database, research and plan your migrations, and always run tests before working with customer data. By doing so, you have the potential to enable your application to grow \u0026ndash; with manageable levels of complexity.\n",url:"https://victoria.dev/blog/manipulating-data-with-django-migrations/"},"https:\/\/victoria.dev\/neofeed\/25724\/":{title:"25724",tags:[],content:"Setting up a new remote branch? Use git push -u origin HEAD to automatically push the current branch to a new remote branch with the same name.\nAnnoyingly, git push doesn\u0026rsquo;t automatically set the upstream branch.\nhttps://git-scm.com/docs/git-push#Documentation/git-push.txt\u0026ndash;u\nfatal: The current branch \u0026lt;branch name\u0026gt; has no upstream branch. To push the current branch and set the remote as upstream, use git push --set-upstream origin \u0026lt;branch name\u0026gt; ",url:"https://victoria.dev/neofeed/25724/"},"https:\/\/victoria.dev\/neofeed\/25556\/":{title:"25556",tags:[],content:"I can\u0026rsquo;t tell you how happy I am about this setting! Automatic format on save in Visual Studio Code, but ONLY for the lines you modified. Heaven-sent change for PR reviewers 🙏\nhttps://code.visualstudio.com/updates/v1_49#_only-format-modified-text\n",url:"https://victoria.dev/neofeed/25556/"},"https:\/\/victoria.dev\/neofeed\/24906\/":{title:"24906",tags:[],content:"Tried to come up with a way to remember the sudo tee hack for when I forget to start Vim with sudo:\nSave with the ! shell command using sudo to tee-split to the % current file name.\n:w !sudo tee %",url:"https://victoria.dev/neofeed/24906/"},"https:\/\/victoria.dev\/blog\/what-is-tls-transport-layer-security-encryption-explained-in-plain-english\/":{title:"What is TLS? Transport Layer Security encryption explained in plain english",tags:["cybersecurity","algorithms","protocols","computing"],content:"If you want to have a confidential conversation with someone you know, you might meet up in person and find a private place to talk. If you want to send data confidentially over the Internet, you might have a few more considerations to cover.\nTLS, or Transport Layer Security, refers to a protocol. \u0026ldquo;Protocol\u0026rdquo; is a word that means, \u0026ldquo;the way we\u0026rsquo;ve agreed to do things around here,\u0026rdquo; more or less. The \u0026ldquo;transport layer\u0026rdquo; part of TLS simply refers to host-to-host communication, such as how a client and a server interact, in the Internet protocol suite model.\nThe TLS protocol attempts to solve these fundamental problems:\n How do I know you are who you say you are? How do I know this message from you hasn\u0026rsquo;t been tampered with? How can we communicate securely?  Here\u0026rsquo;s how TLS works, explained in plain English. As with many successful interactions, it begins with a handshake.\nGetting to know you The basic process of a TLS handshake involves a client, such as your web browser, and a server, such as one hosting a website, establishing some ground rules for communication. It begins with the client saying hello. Literally. It\u0026rsquo;s called a ClientHello message.\nThe ClientHello message tells the server which TLS protocol version and cipher suites it supports. While \u0026ldquo;cipher suite\u0026rdquo; sounds like a fancy hotel upgrade, it just refers to a set of algorithms that can be used to secure communications. The server, in a similarly named ServerHello message, chooses the protocol version and cipher suite to use from the choices offered. Other data may also be sent, for example, a session ID if the server supports resuming a previous handshake.\nDepending on the cipher suite chosen, the client and server exchange further information in order to establish a shared secret. Often, this process moves the exchange from asymmetric cryptography to symmetric cryptography with varying levels of complexity. Let\u0026rsquo;s explore these concepts at a general level and see why they matter to TLS.\nAsymmetric beginnings This is asymmetry:\n Small egg, big egg.\n  Asymmetric cryptography is one method by which you can perform authentication. When you authenticate yourself, you answer the fundamental question, \u0026ldquo;How do I know you are who you say you are?\u0026rdquo;\nIn an asymmetric cryptographic system, you use a pair of keys in order to achieve authentication. These keys are asymmetric. One key is your public key, which, as you would guess, is public. The other is your private key, which \u0026ndash; well, you know.\nTypically, during the TLS handshake, the server will provide its public key via its digital certificate, sometimes still called its SSL certificate, though TLS replaces the deprecated Secure Sockets Layer (SSL) protocol. Digital certificates are provided and verified by trusted third parties known as Certificate Authorities (CA), which are a whole other article in themselves.\nWhile anyone may encrypt a message using your public key, only your private key can then decrypt that message. The security of asymmetric cryptography relies only on your private key staying private, hence the asymmetry. It\u0026rsquo;s also asymmetric in the sense that it\u0026rsquo;s a one-way trip. Alice can send messages encrypted with your public key to you, but neither of your keys will help you send an encrypted message to Alice.\nSymmetric secrets Asymmetric cryptography also requires more computational resources than symmetric cryptography. Thus when a TLS handshake begins with an asymmetric exchange, the client and server will use this initial communication to establish a shared secret, sometimes called a session key. This key is symmetric, meaning that both parties use the same shared secret and must maintain that secrecy for the encryption to be secure.\n Wise man say: share your public key, but keep your shared keys private.\n  By using the initial asymmetric communication to establish a session key, the client and server can rely on the session key being known only to them. For the rest of the session, they\u0026rsquo;ll both use this same shared key to encrypt and decrypt messages, which speeds up communication.\nSecure sessions A TLS handshake may use asymmetric cryptography or other cipher suites to establish the shared session key. Once the session key is established, the handshaking portion is complete and the session begins.\nThe session is the duration of encrypted communication between the client and server. During this time, messages are encrypted and decrypted using the session key that only the client and server have. This ensures that communication is secure.\nThe integrity of exchanged information is maintained by using a checksum. Messages exchanged using session keys have a message authentication code (MAC) attached. This is not the same thing as your device\u0026rsquo;s MAC address. The MAC is generated and verified using the session key. Because of this, either party can detect if a message has been changed before being received. This solves the fundamental question, \u0026ldquo;How do I know this message from you hasn\u0026rsquo;t been tampered with?\u0026rdquo;\nSessions can end deliberately, due to network disconnection, or from the client staying idle for too long. Once a session ends, it must be re-established via a new handshake or through previously established secrets called session IDs that allow resuming a session.\nTLS and you Let\u0026rsquo;s recap:\n TLS is a cryptographic protocol for providing secure communication. The process of creating a secure connection begins with a handshake. The handshake establishes a shared session key that is then used to secure messages and provide message integrity. Sessions are temporary, and once ended, must be re-established or resumed.  This is just a surface-level skim of the very complex cryptographic systems that help to keep your communications secure. For more depth on the topic, I recommend exploring cipher suites and the various supported algorithms.\nThe TLS protocol serves a very important purpose in your everyday life. It helps to secure your emails to family, your online banking activities, and the connection by which you\u0026rsquo;re reading this article. The HTTPS communication protocol is encrypted using TLS. Every time you see that little lock icon in your URL bar, you\u0026rsquo;re experiencing firsthand all the concepts you\u0026rsquo;ve just read about in this article. Now you know the answer to the last question: \u0026ldquo;How can we communicate securely?\u0026rdquo;\n",url:"https://victoria.dev/blog/what-is-tls-transport-layer-security-encryption-explained-in-plain-english/"},"https:\/\/victoria.dev\/blog\/deceptively-simple-search-and-replace-across-multiple-files\/":{title:"Deceptively simple search-and-replace across multiple files",tags:["terminal","linux"],content:"While a multitude of methods exist to search for and replace words in a single file, what do you do when you\u0026rsquo;ve got a string to update across multiple unrelated files, all with different names? You harness the power of command line tools, of course!\nFirst, you\u0026rsquo;ll need to find all the files you want to change. Stringing together what are effectively search queries for find is really only limited by your imagination. Here\u0026rsquo;s a simple example that finds Python files:\nfind . -name \u0026#39;*.py\u0026#39; The -name test searches for a pattern, such as all files ending in .py, but find can do a lot more with other test conditions, including -regex tests. Run find --help to see the multitude of options.\nFurther tune your search by using grep to get only the files that contain the string you want to change, such as by adding:\ngrep -le \u0026#39;\\\u0026lt;a whale\\\u0026gt;\u0026#39; The -l option gives you just the file names for all files containing a pattern (denoted with -e) that match \u0026ldquo;a whale\u0026rdquo;.\nUsing Vim\u0026rsquo;s impressive :bufdo lets you run the same command across multiple buffers, interactively working with all of these files without the tedium of opening, saving, and closing each file, one at a time.\nLet\u0026rsquo;s plug your powerful find+grep results into Vim with:\nvim `find . -name \u0026#39;*.py\u0026#39; \\ -exec grep -le \u0026#39;\\\u0026lt;a whale\\\u0026gt;\u0026#39; {} \\;` Using backtick-expansion to pass our search to Vim opens up multiple buffers ready to go. (Do :h backtick-expansion in Vim for more.) Now you can apply the Vim command :bufdo to all of these files and perform actions such as interactive search-and-replace:\n:bufdo %s/a whale/a bowl of petunias/gceThe g for \u0026ldquo;global\u0026rdquo; will change occurrences of the pattern on all lines. The e will omit errors if the pattern is not found. The c option makes this interactive; if you\u0026rsquo;re feeling confident, you can omit it to make the changes without reviewing each one.\nIf one of the patterns contains a / character, you can substitute the separator in the above command to make it more readable. Vim will assume the character following the %s is the separator, so for example:\n:bufdo %s_a whale_a bowl of peonies/petunias_gceWhen you\u0026rsquo;ve finished going through all the buffers, save all the work you\u0026rsquo;ve completed with:\n:bufdo wq!Then bask in the glory of your saved time and effort.\n",url:"https://victoria.dev/blog/deceptively-simple-search-and-replace-across-multiple-files/"},"https:\/\/victoria.dev\/neofeed\/23510\/":{title:"23510",tags:[],content:"Use shell parameter expansion to replace all the newlines in a text file with %0A URL encoding of a newline:\nOUTPUT=$(cat output.txt) FORMATTED=${OUTPUT//$\u0026#39;\\n\u0026#39;/%0A} ",url:"https://victoria.dev/neofeed/23510/"},"https:\/\/victoria.dev\/neofeed\/22937\/":{title:"22937",tags:[],content:"I like deadlines and living documents.\n",url:"https://victoria.dev/neofeed/22937/"},"https:\/\/victoria.dev\/blog\/how-github-codespaces-increases-productivity-and-lowers-barriers\/":{title:"How GitHub Codespaces increases productivity and lowers barriers",tags:["open-source","coding","tech-team"],content:"The most recent integration between Visual Studio Code and GitHub can help make development accessible and welcoming: Codespaces in GitHub!\nNow in beta, GitHub Codespaces provide an online, in-the-browser IDE powered by Visual Studio Code. This lets you use this full-featured IDE, complete with extensions, terminal, Git commands, and all the settings you\u0026rsquo;re accustomed to, on any machine. You can now bring your development workflow anywhere using a tablet or other browser-based device.\nCodespaces is great news for open source contributors, too. Adding a codespace configuration to your project is a great way to invite new folks to easily start contributing.\nA new open source contributor or new hire at your organization can quickly fire up a codespace and get hacking on a good first issue with no local environment set up or installations necessary!\nWe\u0026rsquo;ve added codespace configuration settings over at the OWASP Web Security Testing Guide (WSTG). Want to take it for a spin? See our open issues.\nConfiguring Codespaces You can use Visual Studio Code\u0026rsquo;s .devcontainer folder to configure a development container for your repository as well.\nMany pre-built containers are available \u0026ndash; just copy the .devcontainer you need to your repository root. If your repository doesn\u0026rsquo;t have one, a default base Linux image will be used.\nHere\u0026rsquo;s a reason to remove .vscode from your .gitignore file. Any new codespaces created in your repository will now respect settings found at .vscode/settings.json. This means that your online IDE can have the same Workspace configuration as you have on your local machine. Isn\u0026rsquo;t that useful!\nMaking Codespaces personal For next-level dotfiles personalization, consider committing relevant files from your local dotfiles folder as a public GitHub repository at yourusername/dotfiles.\nWhen you create a new codespace, this brings in your configurations, such as shell aliases and preferences, by creating symlinks to dotfiles in your codespace $HOME. This personalizes all the codespaces you create in your account.\nNeed some inspiration? Browse my dotfiles repository on GitHub.\nDeveloping in a codespace is a familiar experience for Visual Studio Code users, right down to running an application locally.\nThanks to port forwarding, when I run an application in a codespace terminal, clicking on the resulting localhost URL takes me to the appropriate port as output from my codespace.\nWhen I\u0026rsquo;m working on this website in my codespace, for example, I run hugo serve then click the provided localhost:1313 link to see a preview of my changes in another browser tab.\nWant to stay in sync between devices? There\u0026rsquo;s an extension for that. You can connect to your codespace from Visual Studio Code on your local machine so you can always pick up right where you left off.\nDevelop anywhere Codespaces is a super exciting addition to my GitHub workflow. It allows me to access my full development process pretty much anywhere, using devices like my iPad.\nIt\u0026rsquo;ll also make it easier for new open source contributors or new hires at your organization to hit the ground running with a set-up IDE. If you have access to the limited beta, I invite you to spin up a codespace and try contributing to the WSTG, or to an issue on one of my open source projects.\nI\u0026rsquo;m looking forward to general availability and seeing what the open source community will dream up for GitHub Codespaces next!\nAnd yes \u0026ndash; codespaces support your favorite Visual Studio Code theme. 😈\n Screenshot of a codespace with the Kabukichō theme for Visual Studio Code\n  ",url:"https://victoria.dev/blog/how-github-codespaces-increases-productivity-and-lowers-barriers/"},"https:\/\/victoria.dev\/blog\/the-opportunity-cost-canary\/":{title:"The opportunity cost canary",tags:[""],content:"It\u0026rsquo;s hard to see something you\u0026rsquo;re ignoring.\nOpportunity costs are incurred when you choose one thing over another. If you can only choose one of two options for what to do with your Saturday \u0026ndash; say, attend an industry conference and win new clients, or go on a hike with your kids \u0026ndash; your opportunity cost is whichever of the two you didn\u0026rsquo;t do.\nSaturday activities are a simplified example, for which most people can make up their minds based on their current priorities and personalities. However, opportunity costs span a lifetime of choices that aren\u0026rsquo;t always so clearly presented.\nHave you got a to-do list of ideas to act on? Companies to start? Products to invent?\nIf you knew your time and energy to tackle that list was limited (spoiler alert: it is), how would you go about crossing things off?\nSystems What you can achieve in a day is a direct result of what you make the time to do. Part of being your own boss (or an adult out of school) is that no one else will give you set periods of time for activities. You have to make them out for yourself. You may base these decisions on whatever factors you prefer, but if your main factor is whether or not you feel like it at the moment \u0026ndash; well, you know the rest.\nIf your list is really important to you, if it\u0026rsquo;s more than a entertaining thought of \u0026ldquo;someday maybe,\u0026rdquo; then make the time for it. It might be a half-hour every morning or every night. It may be every other Saturday. If it\u0026rsquo;s not on the same calendar you use for everything else, it doesn\u0026rsquo;t exist.\nChoices Now that you have a dedicated period of time, you\u0026rsquo;ll have to choose which list item to work on. Our brains can only truly do one thing at a time (see: multitasking myth), so choose one thing, and work on it for your allotted time.\nBut what about opportunity cost?\nHere\u0026rsquo;s a simple way to help you figure out the opportunity costs you\u0026rsquo;re willing to accept. Each time you choose something from your list, move it to the top of the list. Assuming you\u0026rsquo;re a human, you probably like some amount of variety in your work \u0026ndash; let\u0026rsquo;s say, three things.\nAfter three iterations, everything on your list from the fourth position down is likely an opportunity cost you\u0026rsquo;re willing to pay.\nYou should adapt this principle to what works for you. Maybe five top spots is more your thing. Maybe it\u0026rsquo;s just one. You don\u0026rsquo;t necessarily need to toss the rest of the list. Instead, let it be informative.\nRevelations What happens when an item that isn\u0026rsquo;t in the top spot strikes you as something you really should be doing? Or something you thought you\u0026rsquo;d want to do by now?\nYou now have a canary. It\u0026rsquo;s a warning that something is off-balance in your life. Maybe your priorities aren\u0026rsquo;t being addressed, or need to be rethought. Maybe you don\u0026rsquo;t possess the skills you require, and need to learn them or outsource the task.\nIt\u0026rsquo;s hard to see something you\u0026rsquo;re ignoring, but now you can. Now, you can start to ask the right questions.\n",url:"https://victoria.dev/blog/the-opportunity-cost-canary/"},"https:\/\/victoria.dev\/neofeed\/21522\/":{title:"21522",tags:[],content:"I feel like watching someone trash and pillage an entire island should be adequate grounds for retracting your agreement to go and live with them.\n",url:"https://victoria.dev/neofeed/21522/"},"https:\/\/victoria.dev\/blog\/it-didnt-happen-exactly-like-that\/":{title:"It didn\u0027t happen exactly like that",tags:[""],content:"The first people you sell to are the ones who work for you.\nFor a very brief moment a bajillion years ago I was an office manager at a very new marketing startup.\nThe CEO had just signed the lease for the office. It smelled like dust and packing tape. There was no furniture yet besides a folding table and chairs, but one wall was lined with boxes of swag, so there was that.\n“As our office manager, you\u0026rsquo;ll be responsible for our office,” said the CEO, tossing me a foam stress ball with the new company\u0026rsquo;s logo printed on it. “You\u0026rsquo;ll provide any administrative support we need.”\n“Okay,” I said, giving my new swag an obligatory squeeze. “And who will I be working with?”\n“You’ll report to the Manager. She’ll mostly be telling you what to do.” he said, opening another box. “Do you like hoodies?” It didn\u0026rsquo;t happen exactly like that, but it may as well have.\nI met the Manager. “The CEO says you have a lot of drive and ideas,” she said. “That’s great. You should always feel free to tell me about any ideas you have, so I can bring them to the CEO.\u0026quot;\n\u0026ldquo;Okay,\u0026rdquo; I said, \u0026ldquo;that\u0026rsquo;s great, because I have some ideas about how we can be more productive and I\u0026rsquo;d like to work my way up in the company.\u0026rdquo;\n\u0026ldquo;That\u0026rsquo;s nice,\u0026rdquo; said the Manager. \u0026ldquo;Here, I have some papers for you to shuffle.\u0026rdquo; It didn\u0026rsquo;t happen exactly like that, but it may as well have.\nSome time later the Manager came by my desk. \u0026ldquo;I\u0026rsquo;d like you to start making some cold calls to prospective customers,\u0026rdquo; she said. \u0026ldquo;Here\u0026rsquo;s a list.\u0026rdquo;\n\u0026ldquo;Isn\u0026rsquo;t that more in the sales teams' wheelhouse?\u0026rdquo; I asked. \u0026ldquo;I\u0026rsquo;m not very interested in being a salesperson and making cold calls.\u0026rdquo;\n\u0026ldquo;Oh, don\u0026rsquo;t worry, they aren\u0026rsquo;t really cold calls. They\u0026rsquo;re more like a non-zero-probability-of-warmth calls,\u0026rdquo; said the Manager.\n\u0026ldquo;Did you get my email about making our onboarding process more efficient?\u0026rdquo; I asked. \u0026ldquo;I can start working on that if you think it makes sense.\u0026rdquo;\n\u0026ldquo;Sure,\u0026rdquo; said the Manager, leaving the call list on my desk.\nMost of the logo had worn off my company stress ball when I went to see the CEO. \u0026ldquo;I have some concerns about making these cold calls.\u0026rdquo; I said.\n\u0026ldquo;Oh?\u0026rdquo; said the CEO, looking up from a mug of coffee with the company logo printed on it. \u0026ldquo;What\u0026rsquo;s that?\u0026rdquo;\n\u0026ldquo;Shouldn\u0026rsquo;t the sales team be doing this? I imagine they\u0026rsquo;d be better at it, having had sales training.\u0026rdquo;\n\u0026ldquo;Oh, sure,\u0026rdquo; said the CEO. \u0026ldquo;Don\u0026rsquo;t worry about it. I\u0026rsquo;ll talk to the sales team.\u0026rdquo;\n\u0026ldquo;\u0026hellip;Do we even have a sales team?\u0026rdquo; I asked.\n\u0026ldquo;Oh, excuse me, I have a phone call.\u0026rdquo; said the CEO, waving me out of his office.\nOn my last day there, I went to see the Manager. \u0026ldquo;I don\u0026rsquo;t feel I\u0026rsquo;m making much of an impact here,\u0026rdquo; I said, \u0026ldquo;I\u0026rsquo;ve decided to resign.\u0026rdquo;\n\u0026ldquo;That\u0026rsquo;s fine,\u0026rdquo; said the Manager, \u0026ldquo;because the CEO wants to fire you anyway.\u0026rdquo;\n\u0026ldquo;Oh,\u0026rdquo; I said, surprised. \u0026ldquo;What for?\u0026rdquo;\n\u0026ldquo;What for?\u0026rdquo; she said, surprised. \u0026ldquo;For what you said. You didn\u0026rsquo;t make any cold calls.\u0026rdquo;\nIt didn\u0026rsquo;t happen exactly like that, but it may as well have.\n",url:"https://victoria.dev/blog/it-didnt-happen-exactly-like-that/"},"https:\/\/victoria.dev\/blog\/how-to-create-a-self-documenting-makefile\/":{title:"How to create a self-documenting Makefile",tags:["coding","ci/cd","docs","tech-team"],content:"My new favorite way to completely underuse a Makefile? Creating personalized, per-project repository workflow command aliases that you can check in.\nCan a Makefile improve your DevOps and keep developers happy? How awesome would it be if a new developer working on your project didn\u0026rsquo;t start out by copying and pasting commands from your README? What if instead of:\npip3 install pipenv pipenv shell --python 3.8 pipenv install --dev npm install pre-commit install --install-hooks # look up how to install Framework X... # copy and paste from README... npm run serve \u0026hellip; you could just type:\nmake start \u0026hellip;and then start working?\nMaking a difference I use make every day to take the tedium out of common development activities like updating programs, installing dependencies, and testing. To do all this with a Makefile (GNU make), we use Makefile rules and recipes. Similar parallels exist for POSIX flavor make, like Target Rules; here\u0026rsquo;s a great article on POSIX-compatible Makefiles.\nHere\u0026rsquo;s some examples of things we can make easier (sorry):\nupdate: ## Do apt upgrade and autoremove  sudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y sudo apt autoremove -y env: pip3 install pipenv pipenv shell --python 3.8 install: ## Install or update dependencies  pipenv install --dev npm install pre-commit install --install-hooks serve: ## Run the local development server  hugo serve --enableGitInfo --disableFastRender --environment development initial: update env install serve ## Install tools and start development server Now we have some command-line aliases that you can check in! Great idea! If you\u0026rsquo;re wondering what\u0026rsquo;s up with that weird ## comment syntax, it gets better.\nA self-documenting Makefile Aliases are great, if you remember what they all are and what they do without constantly typing cat Makefile. Naturally, you need a help command:\n.PHONY: help help: ## Show this help  @egrep -h \u0026#39;\\s##\\s\u0026#39; $(MAKEFILE_LIST) | sort | awk \u0026#39;BEGIN {FS = \u0026#34;:.*?## \u0026#34;}; {printf \u0026#34;\\033[36m%-20s\\033[0m %s\\n\u0026#34;, $$1, $$2}\u0026#39; With a little command-line magic, this egrep command takes the output of MAKEFILE_LIST, sorts it, and uses awk to find strings that follow the ## pattern. It then prints a helpful formatted version of the comments.\nWe\u0026rsquo;ll put it at the top of the file so it\u0026rsquo;s the default target. Now to see all our handy shortcuts and what they do, we just run make, or make help:\nhelp Show this help initial Install tools and start development server install Install or update dependencies serve Run the local development server update Do apt upgrade and autoremove Now we have our very own personalized and project-specific CLI tool!\nThe possibilities for improving your DevOps flow with a self-documenting Makefile are almost endless. You can use one to simplify any workflow and produce some very happy developers.\nPlease enjoy the (live!) Makefile I use to manage and develop this Hugo site. I hope it inspires you!\n My Hugo site Makefile SHELL := /bin/bash .POSIX: .PHONY: help env install upgrade-hugo serve build start initial help: ## Show this help 	@egrep -h \u0026#39;\\s##\\s\u0026#39; $(MAKEFILE_LIST) | sort | awk \u0026#39;BEGIN {FS = \u0026#34;:.*?## \u0026#34;}; {printf \u0026#34;\\033[36m%-20s\\033[0m %s\\n\u0026#34;, $$1, $$2}\u0026#39; env: pip3 install pipenv shell: ## Enter the virtual environment 	pipenv shell install: ## Install or update dependencies 	pipenv install --dev npm install npm install -g markdownlint-cli pre-commit install --install-hooks HUGO_VERSION:=$(shell curl -s https://api.github.com/repos/gohugoio/hugo/releases/latest | grep \u0026#39;tag_name\u0026#39; | cut -d \u0026#39;\u0026#34;\u0026#39; -f 4 | cut -c 2-) upgrade-hugo: ## Get the latest Hugo 	mkdir tmp/ \u0026amp;\u0026amp; \\ 	cd tmp/ \u0026amp;\u0026amp; \\ 	curl -sSL https://github.com/gohugoio/hugo/releases/download/v$(HUGO_VERSION)/hugo_extended_$(HUGO_VERSION)_Linux-64bit.tar.gz | tar -xvzf- \u0026amp;\u0026amp; \\ 	sudo mv hugo /usr/local/bin/ \u0026amp;\u0026amp; \\ 	cd .. \u0026amp;\u0026amp; \\ 	rm -rf tmp/ hugo version serve: ## Run the local development server 	hugo serve --enableGitInfo --disableFastRender --environment development future: ## Run the local development server in the future 	hugo serve --enableGitInfo --buildFuture --disableFastRender --environment development build: ## Lock dependencies and build site 	pipenv lock hugo --minify --cleanDestination start: upgrade-hugo serve ## Update Hugo and start development server  initial: env install upgrade-hugo serve ## Install tools and start development server   ",url:"https://victoria.dev/blog/how-to-create-a-self-documenting-makefile/"},"https:\/\/victoria.dev\/neofeed\/21539\/":{title:"21539",tags:[],content:"Let\u0026rsquo;s rename “soft skills” to “basic skills” and see if that helps.\n",url:"https://victoria.dev/neofeed/21539/"},"https:\/\/victoria.dev\/blog\/climbing-mt.-fuji\/":{title:"Climbing Mt. Fuji",tags:[""],content:"In 2017, I climbed Mt. Fuji, in Japan.\nMt. Fuji is, some folks would say, the cakewalk of mountain climbing. Physically, the hardest portions amount to scrambling over some big boulders; most of it is no more taxing than a hike or climbing a set of stairs. For spiritual reasons, some Japanese folks make the climb at ages upwards of 80 years. There are huts to stop at along the way where you can rent a sleeping bag inside, and buy food and water. Naturally, having done this research and deciding it sounded like a fun outing, I arrived at basecamp in sneakers.\nMost of the way up was amazing and thoroughly enjoyable. I saw sights I\u0026rsquo;d never seen before, like the glow of a city under the sun through a break in the clouds, from above. Walking a path through a cloud was like taking a road into nothingness, with blank grey on all sides that weren\u0026rsquo;t a mountain. Every time we hit a station marker, I felt pride and accomplishment.\nUntil it was time to summit.\nMost of the people who climb Mt. Fuji wish to reach the summit at sunrise. Some for spiritual reasons, others for Instagram, and for those like myself, it just seemed like the thing to do. Regardless, it was because of these other 5,000 average daily climbers that I found myself in an actual queue that snaked the entire path from the last station hut to the summit \u0026ndash; in the pitch black pre-dawn cold. It took hours, for most of which, we stood stock-still, going nowhere. I took to doing calisthenics to stave off frostbite from the cold that threatened my sneaker-shod toes.\nWe did, eventually, reach the summit, and before sunrise. It remains one of the most beautiful sunrises I\u0026rsquo;ve seen \u0026ndash; a pink-gold light that lit up the peak like breathing life into a painting, and that brought, mercifully, a degree of warmth. I was extremely happy, and felt pride and accomplishment.\nUntil it was time to descend.\nThere is a Japanese proverb: “A wise man will climb Mt Fuji once; a fool will climb Mt Fuji twice.” It is my own suspicion that this saying is based entirely on the difficulty of climbing down. The descent is essentially a loosely-packed, dirt and gravel road \u0026ndash; on a decline. It is not, I imagine, significantly taxing with proper hiking boots, maybe snow tread, and a couple good spiked hiking poles thrown in. Wearing a pair of flat-soled street shoes, however, I fell. I fell often, and hard, about every three steps, for hours. I tried to take larger steps; it didn\u0026rsquo;t help. I tried to take smaller steps; that didn\u0026rsquo;t help, either. I tried cunningly to find a way to surf-slide my way down the mountainside and nearly ended up with a mouthful of dirt. As if literally rubbing salt into my wounds, without the gaiters I hadn\u0026rsquo;t brought, sand found its way into my shoes. It was without a doubt the most stupefyingly discouraging experience of my life.\nOn several occasions, more seasoned (smarter? well-prepared?) hikers passed me, a good many of them at least twice my age. I\u0026rsquo;m hard-pressed to remember another time in my life where I have been so thoroughly shown up by someone who might have been my grandmother, plunking hiking poles into the earth and sauntering past at a steady pace while I picked myself up, elbows scratched and covered in dirt, for the umpteenth time.\nEventually, we reached the bottom. At a tiny basecamp gift shop, I ate a delicious bowl of ramen and the tastiest sponge cake in the shape of a mountain that I\u0026rsquo;ll likely ever have.\nThe experience drove home two lessons that have gone on to serve me well: one, that all the good research in the world will not guarantee your experience; and two, that even when faced with a discouraging situation that you can\u0026rsquo;t seem to think yourself out of and thus the only way is \u0026ldquo;through,\u0026rdquo; there may still be something to learn from it, and there may be really good cake at the bottom.\n",url:"https://victoria.dev/blog/climbing-mt.-fuji/"},"https:\/\/victoria.dev\/neofeed\/20954\/":{title:"20954",tags:[],content:"Do not fail silent into that good night. Rage, rage, against the passing of \u0026ndash;quiet.\n",url:"https://victoria.dev/neofeed/20954/"},"https:\/\/victoria.dev\/neofeed\/20754\/":{title:"20754",tags:[],content:"Add to your .bashrc for a more useful tree output:\nalias tree=\u0026quot;tree -aI 'test*|.git|node_modules'\u0026quot;\n",url:"https://victoria.dev/neofeed/20754/"},"https:\/\/victoria.dev\/blog\/go-automate-your-github-profile-readme\/":{title:"Go automate your GitHub profile README",tags:["ci/cd","go"],content:"GitHub\u0026rsquo;s new profile page README feature is having the wonderful effect of bringing some personality to the Myspace pages of the developer Internet. Though Markdown lends itself best to standard static text content, that\u0026rsquo;s not stopping creative folks from working to create a next-level README. You can include GIFs and images to add some motion and pizazz (they\u0026rsquo;re covered in GitHub Flavor Markdown), but I\u0026rsquo;m thinking of something a little more dynamic.\nAt front-and-center on your GitHub profile, your README is a great opportunity to let folks know what you\u0026rsquo;re about, what you find important, and to showcase some highlights of your work. You might like to show off your latest repositories, tweet, or blog post. Keeping it up to date doesn\u0026rsquo;t have to be a pain either, thanks to continuous delivery tools like GitHub Actions.\nMy current README refreshes itself daily with a link to my latest blog post. Here\u0026rsquo;s how I\u0026rsquo;m creating a self-updating README.md with Go and GitHub actions.\nReading and writing files with Go I\u0026rsquo;ve been writing a lot of Python lately, but for some things I really like using Go. You could say it\u0026rsquo;s my go-to language for just-for-func projects. Sorry. Couldn\u0026rsquo;t stop myself.\nTo create my README.md, I\u0026rsquo;m going to get some static content from an existing file, mash it together with some new dynamic content that we\u0026rsquo;ll generate with Go, then bake the whole thing at 400 degrees until something awesome comes out.\nHere\u0026rsquo;s how we read in a file called static.md and put it in string form:\n// Unwrap Markdown content content, err := ioutil.ReadFile(\u0026#34;static.md\u0026#34;) if err != nil { log.Fatalf(\u0026#34;cannot read file: %v\u0026#34;, err) return err } // Make it a string stringyContent := string(content) The possibilities for your dynamic content are only limited by your imagination! Here, I\u0026rsquo;ll use the github.com/mmcdole/gofeed package to read the RSS feed from my blog and get the newest post.\nfp := gofeed.NewParser() feed, err := fp.ParseURL(\u0026#34;https://victoria.dev/index.xml\u0026#34;) if err != nil { log.Fatalf(\u0026#34;error getting feed: %v\u0026#34;, err) } // Get the freshest item rssItem := feed.Items[0] To join these bits together and produce stringy goodness, we use fmt.Sprintf() to create a formatted string.\n// Whisk together static and dynamic content until stiff peaks form blog := \u0026#34;Read my latest blog post: **[\u0026#34; + rssItem.Title + \u0026#34;](\u0026#34; + rssItem.Link + \u0026#34;)**\u0026#34; data := fmt.Sprintf(\u0026#34;%s\\n%s\\n\u0026#34;, stringyContent, blog) Then to create a new file from this mix, we use os.Create(). There are more things to know about deferring file.Close(), but we don\u0026rsquo;t need to get into those details here. We\u0026rsquo;ll add file.Sync() to ensure our README gets written.\n// Prepare file with a light coating of os file, err := os.Create(\u0026#34;README.md\u0026#34;) if err != nil { return err } defer file.Close() // Bake at n bytes per second until golden brown _, err = io.WriteString(file, data) if err != nil { return err } return file.Sync() View the full code here in my README repository.\nMmmm, doesn\u0026rsquo;t that smell good? 🍪 Let\u0026rsquo;s make this happen on the daily with a GitHub Action.\nRunning your Go program on a schedule with Actions You can create a GitHub Action workflow that triggers both on a push to your master branch as well as on a daily schedule. Here\u0026rsquo;s a slice of the .github/workflows/update.yaml that defines this:\non:push:branches:- masterschedule:- cron:\u0026#39;0 11 * * *\u0026#39;To run the Go program that rebuilds our README, we first need a copy of our files. We use actions/checkout for that:\nsteps:- name:🍽️ Get working copyuses:actions/checkout@masterwith:fetch-depth:1This step runs our Go program:\n- name:🍳 Shake \u0026amp; bake READMErun:|cd ${GITHUB_WORKSPACE}/update/ go run main.goFinally, we push the updated files back to our repository. Learn more about the variables shown at Using variables and secrets in a workflow.\n- name:🚀 Deployrun:|git config user.name \u0026#34;${GITHUB_ACTOR}\u0026#34; git config user.email \u0026#34;${GITHUB_ACTOR}@users.noreply.github.com\u0026#34; git add . git commit -am \u0026#34;Update dynamic content\u0026#34; git push --all -f https://${{ secrets.GITHUB_TOKEN }}@github.com/${GITHUB_REPOSITORY}.gitView the full code for this Action workflow here in my README repository.\nGo forth and auto-update your README Congratulations and welcome to the cool kids' club! You now know how to build an auto-updating GitHub profile README. You may now go forth and add all sorts of neat dynamic elements to your page \u0026ndash; just go easy on the GIFs, okay?\n",url:"https://victoria.dev/blog/go-automate-your-github-profile-readme/"},"https:\/\/victoria.dev\/blog\/goals-are-for-losers\/":{title:"Goals are for Losers",tags:["life","coding",""],content:"I needed to grow my skill set, and fast.\nI had been working as a freelance software developer. At the time my now-husband and I traveled constantly, living nomadically out of one or two carry-on bags each. All my worldly possessions fit in my 40L backpack. Like a fine wine, I paired this lifestyle with an equally freedom-filled career. I worked on short contracts, hustled for clients, and generally had the time of my life.\nCareer-wise, I was about to hit a wall.\nI was a capable software developer; but in an increasingly tech-literate world, capable software developers are a dime a dozen. It\u0026rsquo;s a great time to be a tech employer, and a highly competitive time to work in tech. If I was going to stand out in any way, I was going to have to do more than be capable. I needed to add both depth and breadth to my skill set, and to stay ahead of the curve I saw coming, I needed to do it on a shorter time scale than \u0026ldquo;years.\u0026rdquo;\nTo my great fortune, I had a good mentor. Among other advice I took, I did not set a goal to become a better software developer.\nGoals are for losers.\nFailing every day Zig Zigler was a pioneer in his own right. His work inspired a new class of motivational speakers. I especially enjoy this quote:\n For 24 years of my adult life, by choice, I weighed well over 200 pounds. I say \u0026ldquo;by choice\u0026rdquo; because I have never accidentally eaten anything.\n Zigler\u0026rsquo;s Seven Steps of Goal Setting introduces important paradigms for making achievements in life. It helps you to enumerate goals, identify motivations and obstacles, and develop a plan with a deadline. It misses the mark in just one respect: implementation.\nFor some goals, enumerating the benefits may be enough motivation to get you up that hill of changing your behavior. Unfortunately, attempting to solve most challenges with positive motivation alone is a Sisyphean effort. Humans don\u0026rsquo;t change their behavior based purely on benefits \u0026ndash; if we did, I venture more of us would be public speakers.\nThe human brain is a pattern-recognition machine, and we\u0026rsquo;re hard-wired through evolution to pay more attention to bad things. Before we had 5G-connected smartphones and could worry about how many Twitter followers our last tweet turned away, we were dealing with the pressing concern of whether that movement in the bushes was lunch or a creature that wanted us for lunch.\nTomes have been written on why this is the case, and the topic is far too complex to summarize here. For a deeper dive, I highly recommend Thinking, Fast and Slow by Daniel Kahneman. This quote sums up the point concisely:\n The brains of humans and other animals contain a mechanism that is designed to give priority to bad news. By shaving a few hundredths of a second from the time needed to detect a predator, this circuit improves the animal’s odds of living long enough to reproduce.\n In the frame of goals, this inclination towards negativity is a baked-in disadvantage. Why? Say you set a goal to lose weight. Let\u0026rsquo;s pretend you want to lose ten pounds. You say to yourself, \u0026ldquo;I have a goal to lose ten pounds.\u0026rdquo; You go through Zig Zigler\u0026rsquo;s Seven Steps of Goal Setting. You list benefits, obstacles, learn about calories and exercise, join a healthy eating support group on Twitter, and plan to eat boiled chicken and broccoli until you hit your goal. What happens tomorrow? Do you wake up, having lost ten pounds?\nNo, of course not. Any goal worth setting isn\u0026rsquo;t, in all likelihood, going to be achieved in a day. Tomorrow you\u0026rsquo;ll exercise, eat your boiled chicken and broccoli, check in with your Twitter group, and go to bed. And all the while, your silly, wonderful, negatively-inclined brain is whispering to you, \u0026ldquo;I failed to achieve my goal today.\u0026rdquo;\nThe day after that, you\u0026rsquo;ll go on failing. Every day until the day you\u0026rsquo;ve lost ten pounds, you will have failed to meet your goal. I don\u0026rsquo;t know about you, but I think that sounds terrible. Why would anyone do that to themselves?\nIn my experience, only one thing changes human behavior with enough consistency for me to call it a reliable interface to our unreliable brain.\nWinning every day To become a better software developer, I created a system that resulted in the rapid growth of my skills and knowledge in the fields of computing and cybersecurity. It resulted in me writing one in-depth technical article on a topic that was new to me. Each week. For six months.\nMy system was simple. It was July. I decided I was going to make a small change to each week until the end of the year. On Sunday, I looked for a new technical topic that I was unfamiliar with. For at least an hour a day during the week following that, I researched and wrote about it. The next week, I posted my article to my website and two other technical communities I\u0026rsquo;m part of \u0026ndash; whether or not I thought it was done. I was learning. In public.\nA system is sustainable. I picked an hour a day of writing because for me, that was a sufficiently small task that I thought, oh sure, I can do that. Systems are highly individual; to be sustainable, it must be appealing to you and your lifestyle.\nA system is a practice. Unlike a goal, you don\u0026rsquo;t need to \u0026ldquo;achieve\u0026rdquo; a system. There is no pass or fail grading. It\u0026rsquo;s a behavior so precise that you can boil it down to the simple fact of whether or not you do it. On Sunday, I either found a topic, or I didn\u0026rsquo;t. Every day, I either spent some time writing, or I didn\u0026rsquo;t. The next week, I either posted my article, or I didn\u0026rsquo;t. Usually, I did.\nInstead of failing to meet a goal of building a better skill stack on day one, I spent the first Sunday with my new system as a winner. The next day, I wrote for an hour and won again. In fact, every day for the next six months, I won.\nI produced 23 technical articles in 23 weeks. Plus an extra one, for Christmas. I got feedback from folks that helped me learn, or gave me new topics to write about. I got feedback from folks who said my articles helped them learn, too.\nIt\u0026rsquo;s hard to learn and write about two-dozen new technical topics and not improve the breadth and depth of your skill set; so, yes, I became a better software developer. As it turned out, my articles helped inspire my current employer to send me an email and ask if I\u0026rsquo;d like to interview for a position.\nThough I\u0026rsquo;ve reduced the frequency of my published articles, my system is still in place. A few months after starting at my new company, I was promoted to their Director of Engineering.\nI\u0026rsquo;m still winning.\n",url:"https://victoria.dev/blog/goals-are-for-losers/"},"https:\/\/victoria.dev\/blog\/do-hard-work\/":{title:"Do hard work",tags:[""],content:"For a few weeks of my childhood - which at the time, of course, meant forever - I had a single most prized possession. It was a blue 150-page spiral bound notebook that I carried with me everywhere. In it, with a painstakingly-chosen professional artists' mechanical pencil, I was writing my first novel. In a suspenseful twisting epic, a young enterprising female detective would solve the mystery of a lost crystal of unfathomable monetary worth and historical significance. I knew that the resilience of my super sleuth heroine would lead her to succeed. Writing out how she learned and grew by overcoming the trials and challenges along the way was the fulfilling part.\nOne day, in a life-altering turn of events somewhere between recess and a game of dodge ball in gym class, I lost my notebook.\nI spent what was probably up to a whole hour, otherwise known as forever, running from classroom to classroom in search of the physical representation of my entire future career as a famous author. Eventually, I found what was left of it: a barely-recognizable tattered mess of torn pages. Some of them were scribbled over with red crayon. It may as well have been my own blood. I was crushed. I may have cried.\nAfter an appropriate period of mourning that may have lasted a whole day, I got another notebook. In it, with my professional artists' mechanical pencil, I started my second novel.\nI don\u0026rsquo;t remember what the second one was about - nor my third, fourth, or the ones after those. As you can imagine, the topics matter little. I kept working, and the complexity of the work grew up with me. I developed a habit of perseverance that serves me well in daily life and in my profession as a software engineer; one that has stood the test of far more trying circumstances than the weekly dramas of childhood. To be fair, I had never seen it in that light until I discovered books and resources that helped me think of grit as a skill to cultivate. One recent book put so fine a point on it (through a story far more dire than any of my own) that it has become my top-shelf recommendation on the topic. Though it is an entirely worthwhile read, I will summarize.\nEverything seems serious when you\u0026rsquo;re young and inexperienced. As a kid with a comparatively smaller frame of reference for the trials and exaltations of living, everything that happens to you is the worst or best thing you can imagine. Historically, the passing of years would help to grow this frame of reference, but today, it\u0026rsquo;s not enough. Today, the vast majority of people on the planet enjoy far more comfortable lives than any of our ancestors could have imagined. Running water alone provided our forefathers with the mental capacity to invent and industrialize on a never-before-seen scale; now we\u0026rsquo;ve got Internet. The dark shadow of this bright future? We grow up with less appreciation for hardship, having had less hardship to beat.\nPersonal success, no matter what you decide that means to you, doesn\u0026rsquo;t come without doing the hard work that your soul is calling out for you to do. Exemplary success comes when you do the hard work that others are not willing to do. It takes mental toughness, resilience, and grit to keep doing hard work. It takes every type of effortful endurance in the face of every type of adversity.\nIt is only by intentionally deciding to do and then doing hard work that you can develop a maturity of perception. This allows you to separate that which is trivial distraction from that which is truly fulfilling. This allows you to evaluate, rehash, rebuild, iterate, and become better at your craft. People who cultivate this ability are the ones who succeed. People who cultivate this ability have made the world a better place.\nI appreciated Fortitude, by Dan Crenshaw, for many reasons. It is a timely amalgamation of philosophies, pragmatic suggestions for systems that encourage mental toughness, and moving personal stories that remind me of the irreplaceable things in life. Books like Fortitude exemplify a mindset that makes success inevitable. By cultivating this mindset, like my super sleuth heroine, I know my resilience will lead me to succeed. Learning and growing by overcoming the trials and challenges along the way is the fulfilling part.\n",url:"https://victoria.dev/blog/do-hard-work/"},"https:\/\/victoria.dev\/neofeed\/19427\/":{title:"19427",tags:[],content:"Faker and fixtures are my favorite f-words.\n",url:"https://victoria.dev/neofeed/19427/"},"https:\/\/victoria.dev\/neofeed\/19137\/":{title:"19137",tags:[],content:"I can hardly imagine a more relaxing start to a development workday than typing make start.\n",url:"https://victoria.dev/neofeed/19137/"},"https:\/\/victoria.dev\/blog\/writing-efficient-django\/":{title:"Writing efficient Django",tags:["coding","python"],content:"I like Django. It\u0026rsquo;s a well-considered and intuitive framework with a name I can pronounce out loud. You can use it to to quickly spin up a weekend-sized project, and you can still use it to run full-blown production applications at scale. I\u0026rsquo;ve done both these things, and over the years I\u0026rsquo;ve discovered how to use some of Django\u0026rsquo;s features for maximum efficiency. These are:\n Class-based versus function-based views Django models Retrieving objects with queries  Let\u0026rsquo;s look at how these tools let you create a performant Django application that\u0026rsquo;s pleasant to build and maintain.\nClass-based versus function-based views Remember that Django is all Python under the hood. When it comes to views, you\u0026rsquo;ve got two choices: view functions (sometimes called \u0026ldquo;function-based views\u0026rdquo;), or class-based views.\nYears ago when I first built ApplyByAPI, it was initially composed entirely of function-based views. These offer granular control, and are good for implementing complex logic; just as in a Python function, you have complete control (for better or worse) over what the view does. With great control comes great responsibility, and function-based views can be a little tedious to use. You\u0026rsquo;re responsible for writing all the necessary methods for the view to work - this is what allows you to completely tailor your application.\nIn the case of ApplyByAPI, there were only a sparse few places where that level of tailored functionality was really necessary. Everywhere else, function-based views began making my life harder. Writing what is essentially a custom view for run-of-the-mill operations like displaying data on a list page became tedious, repetitive, and error-prone.\nWith function-based views, you\u0026rsquo;ll need figure out which Django methods to implement in order to handle requests and pass data to views. Unit testing can take some work to write. In short, the granular control that function-based views offer also requires some granular tedium to properly implement.\nI ended up holding back ApplyByAPI while I refactored the majority of the views into class-based views. This was not a small amount of work and refactoring, but when it was done, I had a bunch of tiny views that made a huge difference. I mean, just look at this one:\nclass ApplicationsList(ListView): model = Application template_name = \u0026#34;applications.html\u0026#34; It\u0026rsquo;s three lines. My developer ergonomics, and my life, got a lot easier.\nYou may think of class-based views as templates that cover most of the functionality any app needs. There are views for displaying lists of things, for viewing a thing in detail, and editing views for performing CRUD (Create, Read, Update, Delete) operations. Because implementing one of these generic views takes only a few lines of code, my application logic became dramatically succinct. This gave me less repeated code, fewer places for something to go wrong, and a more manageable application in general.\nClass-based views are fast to implement and use. The built-in class-based generic views may require less work to test, since you don\u0026rsquo;t need to write tests for the base view Django provides. (Django does its own tests for that; no need for your app to double-check.) To tweak a generic view to your needs, you can subclass a generic view and override attributes or methods. In my case, since I only needed to write tests for any customizations I added, my test files became dramatically shorter, as did the time and resources it took to run them.\nWhen you\u0026rsquo;re weighing the choice between function-based or class-based views, consider the amount of customization the view needs, and the future work that will be necessary to test and maintain it. If the logic is common, you may be able to hit the ground running with a generic class-based view. If you need sufficient granularity that re-writing a base view\u0026rsquo;s methods would make it overly complicated, consider a function-based view instead.\nDjango models Models organize your Django application\u0026rsquo;s central concepts to help make them flexible, robust, and easy to work with. If used wisely, models are a powerful way to collate your data into a definitive source of truth.\nLike views, Django provides some built-in model types for the convenience of implementing basic authentication, including the User and Permission models. For everything else, you can create a model that reflects your concept by inheriting from a parent Model class.\nclass StaffMember(models.Model): user = models.OneToOneField(User, on_delete=models.CASCADE) company = models.OneToOneField(Company, on_delete=models.CASCADE) def __str__(self): return self.company.name + \u0026#34; - \u0026#34; + self.user.email When you create a custom model in Django, you subclass Django\u0026rsquo;s Model class and take advantage of all its power. Each model you create generally maps to a database table. Each attribute is a database field. This gives you the ability to create objects to work with that humans can better understand.\nYou can make a model useful to you by defining its fields. Many built-in field types are conveniently provided. These help Django figure out the data type, the HTML widget to use when rendering a form, and even form validation requirements. If you need to, you can write custom model fields.\nDatabase relationships can be defined using a ForeignKey field (many-to-one), or a ManyToManyField (give you three guesses). If those don\u0026rsquo;t suffice, there\u0026rsquo;s also a OneToOneField. Together, these allow you to define relations between your models with levels of complexity limited only by your imagination. (Depending on the imagination you have, this may or may not be an advantage.)\nRetrieving objects with queries Use your model\u0026rsquo;s Manager (objects by default) to construct a QuerySet. This is a representation of objects in your database that you can refine, using methods, to retrieve specific subsets. All available methods are in the QuerySet API and can be chained together for even more fun.\nPost.objects.filter( type=\u0026#34;new\u0026#34; ).exclude( title__startswith=\u0026#34;Blockchain\u0026#34; ) Some methods return new QuerySets, such as filter(), or exclude(). Chaining these can give you powerful queries without affecting performance, as QuerySets aren\u0026rsquo;t fetched from the database until they are evaluated. Methods that evaluate a QuerySet include get(), count(), len(), list(), or bool().\nIterating over a QuerySet also evaluates it, so avoid doing so where possible to improve query performance. For instance, if you just want to know if an object is present, you can use exists() to avoid iterating over database objects.\nUse get() in cases where you want to retrieve a specific object. This method raises MultipleObjectsReturned if something unexpected happens, as well as the DoesNotExist exception, if, take a guess.\nIf you\u0026rsquo;d like to get an object that may not exist in the context of a user\u0026rsquo;s request, use the convenient get_object_or_404() or get_list_or_404() which raises Http404 instead of DoesNotExist. These helpful shortcuts are suited to just this purpose. To create an object that doesn\u0026rsquo;t exist, there\u0026rsquo;s also the convenient get_or_create().\nEfficient essentials You\u0026rsquo;ve now got a handle on these three essential tools for building your efficient Django application \u0026ndash; congratulations! There\u0026rsquo;s a lot more that Django can do for you, so stay tuned for future articles. If you\u0026rsquo;re going to build on GitHub, you may like to set up my django-security-check GitHub Action. In the meantime, you\u0026rsquo;re well on your way to building a beautiful software project.\n",url:"https://victoria.dev/blog/writing-efficient-django/"},"https:\/\/victoria.dev\/neofeed\/17912\/":{title:"17912",tags:[],content:"Try this one simple trick to avoid having to build your own custom tools: use ones that already exist! 🤯\n",url:"https://victoria.dev/neofeed/17912/"},"https:\/\/victoria.dev\/blog\/look-mom-im-a-github-action-hero\/":{title:"Look mom, I\u0027m a GitHub Action Hero",tags:["ci/cd","open-source","cybersecurity","life","coding","terminal","api"],content:"GitHub recently interviewed me for their blog editorial entitled GitHub Action Hero: Victoria Drake. Here\u0026rsquo;s a behind-the-scenes peek at the original interview questions and my answers.\nWhat is the name of your Action? Please include a link too. Among the several Actions I\u0026rsquo;ve built, I have two current favorites. One is hugo-remote, which lets you continuously deploy a Hugo static site from a private source repository to a public GitHub Pages repository. This keeps the contents of the source repository private, such as your unreleased drafts, while still allowing you to have a public open source site using GitHub Pages.\nThe second is django-security-check. It\u0026rsquo;s an effortless way to continuously check that your production Django application is free from a variety of security misconfigurations. You can think of it as your little CI/CD helper for busy projects \u0026ndash; a security linter!\nTell us a little bit more about yourself—how did you get started in software tools? When I was a kid, I spent several summer vacations coding a huge medieval fantasy world MUD (Multi-User Dungeon, like a multiplayer role-playing game) written in LPC, with friends. It was entirely text-based, and built and played via Telnet. I fell in love with the terminal and learned a lot about object-oriented programming and prototype-based programming early on.\nI became a freelance developer and had the privilege of working on a wide variety of client projects. Realizing the difficulty that companies have with hiring experienced developers, I built ApplyByAPI.com to help. As you might imagine, it allows candidates to apply for jobs via API, instead of emailing a resume. It\u0026rsquo;s based on the Django framework, so in the process, I learned even more about building reusable units of software.\nWhen I became a co-author and a core maintainer for the Open Web Application Security Project (OWASP) Web Security Testing Guide (WSTG), I gained an even broader appreciation for how a prototype-based, repeatable approach can help build secure web applications. Organizations worldwide consider the WSTG the foremost open source resource for testing the security of web applications. We\u0026rsquo;ve applied this thinking via the use of GitHub Actions in our repository \u0026ndash; I\u0026rsquo;ll tell you more about that later.\nWhether I\u0026rsquo;m creating an open source tool or leading a development team, my childhood experience still informs how I think about programming today. I strive to create repeatable units of software like GitHub Actions \u0026ndash; only now, I make them for large enterprises in the real world!\nWhat is the story behind your built GitHub Action? (Why did you build this?) Developers take on a lot of responsibility when it comes to building secure applications these days. I\u0026rsquo;m a full-time senior software developer at a cybersecurity company. I\u0026rsquo;ve found that I\u0026rsquo;m maximally productive when I create systems and processes that help myself and my team make desired outcomes inevitable. So I spend my free time building tools that make it easy for other developers to build secure software as well. My Actions help to automate contained, repeatable units of work that can make a big difference in a developer\u0026rsquo;s day.\nDo you have future plans for this or other Actions? Yes! I\u0026rsquo;m always finding ways for tools like GitHub Actions to boost the velocity of technical teams, whether at work or in my open source projects. Remember the Open Web Application Security Project? In the work I\u0026rsquo;ve lead with OWASP, I\u0026rsquo;ve championed the effort to increase automation using GitHub Actions to maintain quality, securely deploy new versions to the web, and even build PDFs of the WSTG. We\u0026rsquo;re constantly looking into new ways that GitHub Actions can make our lives easier and our readers' projects more secure.\nWhat has been your favorite feature of GitHub Actions? I like that I can build an Action using familiar and portable technologies, like Docker. Actions are easy for collaborators to work on too, since in the case of a Dockerized Action, you can use any language your team is comfortable with. This is especially useful in large organizations with polyglot teams and environments. There aren\u0026rsquo;t any complicated dependencies for running these portable tasks, and you don\u0026rsquo;t need to learn any special frameworks to get started.\nOne of my first blog posts about GitHub Actions even describes how I used an Action to run a Makefile! This is especially useful for large legacy applications that want to modernize their pipeline by using GitHub Actions.\nWhat are the biggest challenges you’ve faced while building your GitHub Action? The largest challenge of GitHub Actions isn\u0026rsquo;t really in GitHub Actions, but in the transition of legacy software and company culture.\nMigrating legacy software is always challenging, particularly with large legacy applications. Moving to modern CI/CD processes requires changes at the software level, team level, and even a shift in thinking when it comes to individual developers. It can help to have a tool like GitHub Actions, which is at once seamlessly modern and familiar, when transitioning legacy code to a modern pipeline.\nAnything else you would like to share about your experience? Any stories or lessons learned through building your Action? I\u0026rsquo;m happiest when I\u0026rsquo;m solving a challenge that makes developing secure software less challenging in the future, both for myself and for the technology organization I\u0026rsquo;m leading. With tools like GitHub Actions, a lot of mental overhead can be offloaded to automatic processes \u0026ndash; like getting a whole other brain, for free! This can massively help organizations that are ready to scale up their development output.\nIn the realm of cybersecurity, not only does creating portable and reusable software make developers' lives easier, it helps to make whole workflows repeatable, which in turn makes software development processes more secure. With smart processes in place, technical teams are happier. As an inevitable result, they\u0026rsquo;ll build better software for customers, too.\n",url:"https://victoria.dev/blog/look-mom-im-a-github-action-hero/"},"https:\/\/victoria.dev\/neofeed\/17418\/":{title:"17418",tags:[],content:"It\u0026rsquo;s the small decisions that define you.\n",url:"https://victoria.dev/neofeed/17418/"},"https:\/\/victoria.dev\/blog\/technical-ergonomics-for-the-efficient-developer\/":{title:"Technical ergonomics for the efficient developer",tags:["coding","life"],content:'This article isn\u0026rsquo;t going to tell you about saving your neck with a Roost stand, or your wrists with a split keyboard - I\u0026rsquo;ve already done that. This article is about saving your brain.\nWhen I first began to program full time, I found myself constantly tired from the mental exertion. Programming is hard! Thankfully, you can take some solace in knowing it gets easier with practice, and with a great supporting cast. Some very nice folks who preceded us both came up with tools to make the difficult bits of communicating with computers much easier on our poor human meatbrains.\nI invite you to explore these super helpful technical tools. They\u0026rsquo;ll improve your development set up and alleviate much of the mental stress of programming. You soon won\u0026rsquo;t believe you could have done without them.\nNot your average syntax highlighting If you\u0026rsquo;re still working with syntax highlighting that just picks out variable and class names for you, that\u0026rsquo;s cute. Time to turn it up a notch.\nIn all seriousness, syntax highlighting can make it much easier to find what you\u0026rsquo;re looking for on your screen: the current line, where your current code block starts and ends, or the absolute game-changing which-bracket-set-am-I-in highlight. I primarily use Visual Studio Code, but similar extensions can be found for the major text editors.\nHere are my favorites:\n Bracket Pair Colorizer highlights sequential bracket pairs in different matching colors, making the pain of picking through nested brackets and parentheses a thing of the past. TODO Highlight effectively removes any excuse you may have had for unintentionally committing TODO and FIXME comments by making them really easy to see. You can even add your own custom keywords to be highlighted (I suggest, wtf, but you didn\u0026rsquo;t hear it from me.) Indented Block Highlighting puts an easy-to-distinguish but unobtrusive highlight behind your current indented code block, so you can see just where that if ends and why that last else isn\u0026rsquo;t doing anything at all. Highlight Line puts a (slightly too) bright line where you last left your cursor. You can customize the line\u0026rsquo;s appearance - I set the borderWidth of mine to 1px.  The theme pictured in Visual Studio Code above is Kabukichō. I made it.\nUse Git hooks I previously brought you an interactive pre-commit checklist in the style of infomercials that\u0026rsquo;s both fun and useful for reinforcing the quality of your commits. But that\u0026rsquo;s not all!\nGit hooks are scripts that run automatically at pre-determined points in your workflow. Use them well, and you can save a ton of brainpower. A pre-commit hook remembers to do things like lint and format code, and even runs local tests for you before you indelibly push something embarrassing. Hooks can be a little annoying to share (the .git/hooks directory isn\u0026rsquo;t tracked and thus omitted when you clone or fork a repository) but there\u0026rsquo;s a framework for that: the confusingly-named pre-commit framework, which allows you to create a sharable configuration file of Git hook plugins, not just for pre-commit.\nI spend a majority of my time these days coding in Python, so here is my current .pre-commit-config.yaml:\nfail_fast: true repos: - repo: https://github.com/DavidAnson/markdownlint-cli2 rev: v0.1.3 hooks: - id: markdownlint-cli2 name: markdownlint-cli2 description: "Checks the style of Markdown/CommonMark files." entry: markdownlint-cli2 language: node types: [markdown] minimum_pre_commit_version: 0.15.0There are tons of supported hooks to explore.\nUse a type system If you write in languages like Python and JavaScript, get yourself an early birthday present and start using a static type system. Not only will this help improve the way you think about code, it can help make type errors clear before running a single line.\nFor Python, I like using mypy for static type checking. You can set it up as a pre-commit hook (see above) and it\u0026rsquo;s supported in Visual Studio Code too.\nTypeScript is my preferred way to write JavaScript. You can run the compiler on the command line using Node.js (see instructions in the repo), it works pretty well with Visual Studio Code out of the box, and of course there are multiple options for extension integrations.\nQuit unnecessarily beating up your meatbrain I mean, you wouldn\u0026rsquo;t stand on your head all day to do your work. It would be rather inconvenient to read things upside down all the time (at least until your brain adjusted), and in any case you\u0026rsquo;d likely get uncomfortably congested in short order. Working without taking advantage of the technical ergonomic tools I\u0026rsquo;ve given you today is a little like unnecessary inversion - why would you, if you don\u0026rsquo;t have to?\n',url:"https://victoria.dev/blog/technical-ergonomics-for-the-efficient-developer/"},"https:\/\/victoria.dev\/neofeed\/17218\/":{title:"17218",tags:[],content:"Does your repo rely on @djangoproject?\nI now have a GitHub Action that helps you continuously monitor and fix common security misconfigurations in your Django application. 💚 https://github.com/victoriadrake/django-security-check\n",url:"https://victoria.dev/neofeed/17218/"},"https:\/\/victoria.dev\/neofeed\/16128\/":{title:"16128",tags:[],content:"All code made perfect sense to someone at some point.\n",url:"https://victoria.dev/neofeed/16128/"},"https:\/\/victoria.dev\/neofeed\/16144\/":{title:"16144",tags:[],content:"You’d think pant pockets would have evolved to comfortably hold our phones by now.\n",url:"https://victoria.dev/neofeed/16144/"},"https:\/\/victoria.dev\/blog\/how-to-choose-and-care-for-a-secure-open-source-project\/":{title:"How to choose and care for a secure open source project",tags:["open-source","cybersecurity","leadership"],content:"There is a rather progressive sect of the software development world that believes that most people would be a lot happier and get a lot more work done if they just stopped building things that someone else has already built and is offering up for free use. They\u0026rsquo;re called the open source community. They want you to take their stuff.\nBesides existing without you having to lift a finger, open source tools and software have some distinct advantages. Especially in the case of well-established projects, it\u0026rsquo;s highly likely that someone else has already worked out all the most annoying bugs for you. Thanks to the ease with which users can view and modify source code, it\u0026rsquo;s also more likely that a program has been tinkered with, improved, and secured over time. When many developers contribute, they bring their own unique expertise and experiences. This can result in a product far more robust and capable than one a single developer can produce.\nOf course, being as varied as the people who build them, not all open source projects are created equal, nor maintained to be equally secure. There are many factors that affect a project\u0026rsquo;s suitability for your use case. Here are a few general considerations that make a good starting point when choosing an open source project.\nHow to choose an open source project As its most basic requirements, a good software project is reliable, easy to understand, and has up-to-date components and security. There are several indicators that can help you make an educated guess about whether an open source project satisfies these criteria.\nWho\u0026rsquo;s using it Taken in context, the number of people already using an open source project may be indicative of how good it is. If a project has a hundred users, for instance, it stands to reason that someone has tried to use it at least a hundred times before you found it. Thus by the ancient customs of \u0026ldquo;I don\u0026rsquo;t know what\u0026rsquo;s in that cave, you go first,\u0026rdquo; it\u0026rsquo;s more likely to be fine.\nYou can draw conclusions about a project\u0026rsquo;s user base by looking at available statistics. Depending on your platform, these may include the number of downloads, reviews, issues or tickets, comments, contributions, forks, or \u0026ldquo;stars,\u0026rdquo; whatever those are.\nEvaluate social statistics on platforms like GitHub with a grain of salt. They can help you determine how popular a project may be, but only in the same way that restaurant review apps can help you figure out if you should eat at Foo\u0026rsquo;s Grill \u0026amp; Bar. Depending on where Foo\u0026rsquo;s Grill \u0026amp; Bar is, when it opened, and how likely people are to be near it when the invariable steak craving should call, having twenty-six reviews may be a good sign or a terrible one. While you would not expect a project that addresses a very obscure use case or technology to have hundreds of users, having a few active users is, in such a case, just as confidence-inspiring.\nExternal validation can also be useful. For example, packages that are included in a Linux operating system distribution (distro) must conform to stringent standards and undergo vetting. Choosing software that is included in a distro\u0026rsquo;s default repositories can mean it\u0026rsquo;s more likely to be secure.\nPerhaps one of the best indications to look for is whether a project\u0026rsquo;s development team is using their own project. Look for issues, discussions, or blog posts that show that the project\u0026rsquo;s creators and maintainers are using what they\u0026rsquo;ve built themselves. Commonly referred to as \u0026ldquo;eating your own dog food,\u0026quot; or \u0026ldquo;dogfooding,\u0026rdquo; it\u0026rsquo;s an indicator that the project is most likely to be well-maintained by its developers.\nWho\u0026rsquo;s building it The main enemy of good open source software is usually a lack of interest. The parties involved in an open source project can make the difference between a flash-in-the-pan library and a respected long-term utility. Multiple committed maintainers, even making contributions in their spare time, have a much higher success rate of sustaining a project and generating interest.\nProjects with healthy interest are usually supported by, and in turn cultivate, a community of contributors and users. New contributors may be actively welcomed, clear guides are available explaining how to help, and project maintainers are available and approachable when people have inevitable questions. Some communities even have chat rooms or forums where people can interact outside of contributions. Active communities help sustain project interest, relevance, and its ensuing quality.\nIn a less organic fashion, a project can also be sustained through organizations that sponsor it. Governments and companies with financial interest are open source patrons too, and a project that enjoys public sector use or financial backing has added incentive to remain relevant and useful.\nHow alive is it The recency and frequency of an open source project\u0026rsquo;s activity is perhaps the best indicator of how much attention is likely paid to its security. Look at releases, commit history, changelogs, or documentation revisions to determine if a project is active. As projects vary in size and scope, here are some general things to look for.\nMaintaining security is an ongoing endeavor that requires regular monitoring and updates, especially for projects with third-party components. These may be libraries or any part of the project that relies on something outside itself, such as a payment gateway integration. An inactive project is more likely to have outdated code or use outdated versions of components. For a more concrete determination, you can research a project\u0026rsquo;s third-party components and compare their most recent patches or updates with the project\u0026rsquo;s last updates.\nProjects without third-party components may have no outside updates to apply. In these cases, you can use recent activity and release notes to determine how committed a project\u0026rsquo;s maintainers may be. Generally, active projects should show updates within the last months, with a notable release within the last year. This can be a good indication of whether the project is using an up-to-date version of its language or framework.\nYou can also judge how active a project may be by looking at the project maintainers themselves. Active maintainers quickly respond to feedback or new issues, even if it\u0026rsquo;s just to say, \u0026ldquo;We\u0026rsquo;re on it.\u0026rdquo; If the project has a community, its maintainers are a part of it. They may have a dedicated website or write regular blogs. They may offer ways to contact them directly and privately, especially to raise security concerns.\nCan you understand it Having documentation is a baseline requirement for a project that\u0026rsquo;s intended for anyone but its creator to use. Good open source projects have documentation that is easy to follow, honest, and thorough.\nHaving well-written documentation is one way a project can stand out and demonstrate the thoughtfulness and dedication of its maintainers. A \u0026ldquo;Getting Started\u0026rdquo; section may detail all the requirements and initial set up for running the project. An accurate list of topics in the documentation enables users to quickly find the information they need. A clear license statement leaves no doubt as to how the project can be used, and for what purposes. These are characteristic aspects of documentation that serves its users.\nA project that is following sound coding practices likely has code that is as readable as its documentation. Code that is easy to read lends itself to being understood. Generally, it has clearly defined and appropriately-named functions and variables, a logical flow, and apparent purpose. Readable code is easier to fix, secure, and build upon.\nHow compatible is it A few factors will determine how compatible a project is with your goals. These are objective qualities, and can be determined by looking at a project\u0026rsquo;s repository files. They include:\n Code language Specific technologies or frameworks License compatibility  Compatibility doesn\u0026rsquo;t necessarily mean a direct match. Different code languages can interact with each other, as can various technologies and frameworks. You should carefully read a project\u0026rsquo;s license to understand if it permits usage for your goal, or if it is compatible with a license you would like to use.\nUltimately, a project that satisfies all these criteria may still not quite suit your use case. Part of the beauty of open source software, however, is that you may still benefit from it by making alterations that better suit your usage. If those alterations make the project better for everyone, you can pay it back and pay it forward by contributing your work to the project.\nProper care and feeding of an open source project Once you adopt an open source project, a little attention is required to make sure it continues to be a boon to your goals. While its maintainers will look after the upstream project files, you alone are responsible for your own copy. Like all software, your open source project must be well-maintained in order to remain as secure and useful as possible.\nHave a system that provides you with notifications when updates for your software are made available. Update software promptly, treating each patch as if it were vital to security; it may well be. Keep in mind that open source project creators and maintainers are, in most cases, acting only out of the goodness of their own hearts. If you\u0026rsquo;ve got a particularly awesome one, its developers may make updates and security patches available on a regular basis. It\u0026rsquo;s up to you to keep tabs on updates and promptly apply them.\nAs with most things in software, keeping your open source additions modular can come in handy. You might use git submodules, branches, or environments to isolate your additions. This can make it easier to apply updates or pinpoint the source of any bugs that arise.\nSo although an open source project may cost no money, caveat emptor, which means, \u0026ldquo;Jimmy, if we get you a puppy, it\u0026rsquo;s your responsibility to take care of it.\u0026rdquo;\n",url:"https://victoria.dev/blog/how-to-choose-and-care-for-a-secure-open-source-project/"},"https:\/\/victoria.dev\/neofeed\/13849\/":{title:"13849",tags:[],content:"New badge, who dis?\nThanks for three years of an amazing DEV community! @ThePracticalDev 🙌🤗👩🏻‍💻\n",url:"https://victoria.dev/neofeed/13849/"},"https:\/\/victoria.dev\/blog\/if-you-want-to-build-a-treehouse-start-at-the-bottom\/":{title:"If you want to build a treehouse, start at the bottom",tags:["cybersecurity","coding"],content:"If you\u0026rsquo;ve ever watched a kid draw a treehouse, you have some idea of how applications are built when security isn\u0026rsquo;t made a priority. It\u0026rsquo;s far more fun to draw the tire swing, front porch, and swimming pool than to worry about how a ten-thousand-gallon bucket of water stays suspended in midair. With too much attention spent on fun and flashy features, foundations suffer.\nOf course, spending undue hours building a back end like Fort Knox may not be necessary for your application, either. Being an advocate for security doesn\u0026rsquo;t mean always wearing your tinfoil hat (although you do look dashing in it) but does mean building in an appropriate amount of security.\nHow much security is appropriate? The answer, frustratingly, is, \u0026ldquo;it depends.\u0026rdquo; The right amount of security for your application depends on who\u0026rsquo;s using it, what it does, and most importantly, what undesirable things it could be made to do. It takes some analysis to make decisions about the kinds of risks your application faces and how you\u0026rsquo;ll prepare to handle them. Okay, now\u0026rsquo;s a good time to don your tinfoil hat. Let\u0026rsquo;s imagine the worst.\nThreat modeling: what\u0026rsquo;s the worst that could happen A threat model is a stuffy term for the result of trying to imagine the worst things that could happen to an application. Using your imagination to assess risks (fittingly called risk assessment) is a conveniently non-destructive method for finding ways an application can be attacked. You won\u0026rsquo;t need any tools; just an understanding of how the application might work, and a little imagination. You\u0026rsquo;ll want to record your results with pen and paper. For the younger folks, that means the notes app on your phone.\nA few different methodologies for application risk assessment can be found in the software world, including the in-depth NIST Special Publication 800-30. Each method\u0026rsquo;s framework has specific steps and output, and will go into various levels of detail when it comes to defining threats. If following a framework, first choose the one you\u0026rsquo;re most likely to complete. You can always add more depth and detail from there.\nEven informal risk assessments are beneficial. Typically taking the form of a set of questions, they may be oriented around possible threats, the impact to assets, or ways a vulnerability could be exploited. Here are some examples of questions addressing each orientation:\n What kind of adversary would want to break my app? What would they be after? If the control of x fell into the wrong hands, what could an attacker do with it? Where could a x vulnerability occur in my app?  A basic threat model explains the technical, business, and human considerations for each risk. It will typically detail:\n The vulnerabilities or components that can cause the risk The impact that a successful execution of the risk would have on the application The consequences for the application\u0026rsquo;s users or organization  The result of a risk assessment exercise is your threat model; in other words, a list of things you would very much like not to occur. It is usually sorted in a hierarchy of risks, from the worst to the mildest. The worst risks have the most negative impact, and are most important to protect against. The mildest risks are the most acceptable - while still an undesirable outcome, they have the least negative impact on the application and users.\nYou can use this resulting hierarchy as a guide to determine how much of your cybersecurity efforts to apply to each risk area. An appropriate amount of security for your application will eliminate (where possible) or mitigate the worst risks.\nPushing left Although it sounds like a dance move meme, pushing left refers instead to building in as much of your planned security as possible in the early stages of software development.\nBuilding software is a lot like building a treehouse, just without the pleasant fresh air. You start with the basic supporting components, such as attaching a platform to a tree. Then comes the framing, walls, and roof, and finally, your rustic-modern Instagram-worthy wall hangings and deer bust.\nThe further along in the build process you are, the harder and more costly it becomes to make changes to a component that you\u0026rsquo;ve already installed. If you discover a problem with the walls only after the roof is put in place, you may need to change or remove the roof in order to fix it. Similar parallels can be drawn for software components, only without similar ease in detangling the attached parts.\nIn the case of a treehouse, it\u0026rsquo;s rather impossible to start with decorations or even a roof, since you can\u0026rsquo;t really suspend them in midair. In the case of software development, it is, unfortunately, possible to build many top-layer components and abstractions without a sufficient supporting architecture. A push-left approach views each additional layer as adding cost and complication. Pushing left means attempting to mitigate security risks as much as possible at each development stage before proceeding to the next.\nBuilding bottom-to-top By considering your threat model in the early stages of developing your application, you reduce the chances of necessitating a costly remodel later on. You can make choices about architecture, components, and code that support the main security goals of your particular application.\nWhile it\u0026rsquo;s not possible to foresee all the functionality your application may one day need to support, it is possible to prepare a solid foundation that allows additional functionality to be added more securely. Building in appropriate security from the bottom to the top will help make mitigating security risks much easier in the future.\n",url:"https://victoria.dev/blog/if-you-want-to-build-a-treehouse-start-at-the-bottom/"},"https:\/\/victoria.dev\/neofeed\/13032\/":{title:"13032",tags:[],content:"New workstation upgrade!\n",url:"https://victoria.dev/neofeed/13032/"},"https:\/\/victoria.dev\/neofeed\/12747\/":{title:"12747",tags:[],content:"“Your best and wisest refuge from all troubles is in your science” -Ada Lovelace\n",url:"https://victoria.dev/neofeed/12747/"},"https:\/\/victoria.dev\/neofeed\/12558\/":{title:"12558",tags:[],content:"If you love playing #AnimalCrossingNewHorizons, you’ll love a career of refactoring legacy code bases. Become an enterprise software developer today!\n",url:"https://victoria.dev/neofeed/12558/"},"https:\/\/victoria.dev\/neofeed\/12314\/":{title:"12314",tags:[],content:"Did you know that it tastes just like a pb\u0026amp;j sandwich if you put the peanut butter and jelly on the same slice of bread first?!\n",url:"https://victoria.dev/neofeed/12314/"},"https:\/\/victoria.dev\/neofeed\/12348\/":{title:"12348",tags:[],content:"“Pour-over” is just French for “human coffee machine”\n",url:"https://victoria.dev/neofeed/12348/"},"https:\/\/victoria.dev\/blog\/hugo-vs-jekyll-an-epic-battle-of-static-site-generator-themes\/":{title:"Hugo vs Jekyll: an epic battle of static site generator themes",tags:["websites","coding","go","open-source"],content:"I recently took on the task of creating a documentation site theme for two projects. Both projects needed the same basic features, but one uses Jekyll while the other uses Hugo.\nIn typical developer rationality, there was clearly only one option. I decided to create the same theme in both frameworks, and to give you, dear reader, a side-by-side comparison.\nThis post isn\u0026rsquo;t a comprehensive theme-building guide, but intended to familiarize you with the process of building a theme in either generator. Here\u0026rsquo;s what we\u0026rsquo;ll cover:\n How theme files are organized Where to put content How templating works Creating a top-level menu with the pages object Creating a menu with nested links from a data list Putting the template together Create a stylesheet  Sass and CSS in Jekyll Sass and Hugo Pipes in Hugo   Configure and deploy to GitHub Pages  Configure Jekyll Configure Hugo Deploy to GitHub Pages   Showtime Wait who won  Here\u0026rsquo;s a crappy wireframe of the theme I\u0026rsquo;m going to create.\nIf you\u0026rsquo;re planning to build-along, it may be helpful to serve the theme locally as you build it; both generators offer this functionality. For Jekyll, run jekyll serve, and for Hugo, hugo serve.\nThere are two main elements: the main content area, and the all-important sidebar menu. To create them, you\u0026rsquo;ll need template files that tell the site generator how to generate the HTML page. To organize theme template files in a sensible way, you first need to know what directory structure the site generator expects.\nHow theme files are organized Jekyll supports gem-based themes, which users can install like any other Ruby gems. This method hides theme files in the gem, so for the purposes of this comparison, we aren\u0026rsquo;t using gem-based themes.\nWhen you run jekyll new-theme \u0026lt;name\u0026gt;, Jekyll will scaffold a new theme for you. Here\u0026rsquo;s what those files look like:\n. ├── assets ├── Gemfile ├── _includes ├── _layouts │ ├── default.html │ ├── page.html │ └── post.html ├── LICENSE.txt ├── README.md ├── _sass └── \u0026lt;name\u0026gt;.gemspec The directory names are appropriately descriptive. The _includes directory is for small bits of code that you reuse in different places, in much the same way you\u0026rsquo;d put butter on everything. (Just me?) The _layouts directory contains templates for different types of pages on your site. The _sass folder is for Sass files used to build your site\u0026rsquo;s stylesheet.\nYou can scaffold a new Hugo theme by running hugo new theme \u0026lt;name\u0026gt;. It has these files:\n. ├── archetypes │ └── default.md ├── layouts │ ├── 404.html │ ├── _default │ │ ├── baseof.html │ │ ├── list.html │ │ └── single.html │ ├── index.html │ └── partials │ ├── footer.html │ ├── header.html │ └── head.html ├── LICENSE ├── static │ ├── css │ └── js └── theme.toml You can see some similarities. Hugo\u0026rsquo;s page template files are tucked into layouts/. Note that the _default page type has files for a list.html and a single.html. Unlike Jekyll, Hugo uses these specific file names to distinguish between list pages (like a page with links to all your blog posts on it) and single pages (like one of your blog posts). The layouts/partials/ directory contains the buttery reusable bits, and stylesheet files have a spot picked out in static/css/.\nThese directory structures aren\u0026rsquo;t set in stone, as both site generators allow some measure of customization. For example, Jekyll lets you define collections, and Hugo makes use of page bundles. These features let you organize your content multiple ways, but for now, lets look at where to put some simple pages.\nWhere to put content To create a site menu that looks like this:\nIntroduction Getting Started Configuration Deploying Advanced Usage All Configuration Settings Customizing Help and Support You\u0026rsquo;ll need two sections (\u0026ldquo;Introduction\u0026rdquo; and \u0026ldquo;Advanced Usage\u0026rdquo;) containing their respective subsections.\nJekyll isn\u0026rsquo;t strict with its content location. It expects pages in the root of your site, and will build whatever\u0026rsquo;s there. Here\u0026rsquo;s how you might organize these pages in your Jekyll site root:\n. ├── 404.html ├── assets ├── Gemfile ├── _includes ├── index.markdown ├── intro │ ├── config.md │ ├── deploy.md │ ├── index.md │ └── quickstart.md ├── _layouts │ ├── default.html │ ├── page.html │ └── post.html ├── LICENSE.txt ├── README.md ├── _sass ├── \u0026lt;name\u0026gt;.gemspec └── usage ├── customizing.md ├── index.md ├── settings.md └── support.md You can change the location of the site source in your Jekyll configuration.\nIn Hugo, all rendered content is expected in the content/ folder. This prevents Hugo from trying to render pages you don\u0026rsquo;t want, such as 404.html, as site content. Here\u0026rsquo;s how you might organize your content/ directory in Hugo:\n. ├── _index.md ├── intro │ ├── config.md │ ├── deploy.md │ ├── _index.md │ └── quickstart.md └── usage ├── customizing.md ├── _index.md ├── settings.md └── support.md To Hugo, _index.md and index.md mean different things. It can be helpful to know what kind of Page Bundle you want for each section: Leaf, which has no children, or Branch.\nNow that you have some idea of where to put things, let\u0026rsquo;s look at how to build a page template.\nHow templating works Jekyll page templates are built with the Liquid templating language. It uses braces to output variable content to a page, such as the page\u0026rsquo;s title: {{ page.title }}.\nHugo\u0026rsquo;s templates also use braces, but they\u0026rsquo;re built with Go Templates. The syntax is similar, but different: {{ .Title }}.\nBoth Liquid and Go Templates can handle logic. Liquid uses tags syntax to denote logic operations:\n{% if user %} Hello {{ user.name }}! {% endif %} And Go Templates places its functions and arguments in its braces syntax:\n{{ if .User }} Hello {{ .User }}! {{ end }} Templating languages allow you to build one skeleton HTML page, then tell the site generator to put variable content in areas you define. Let\u0026rsquo;s compare two possible default page templates for Jekyll and Hugo.\nJekyll\u0026rsquo;s scaffold default theme is bare, so we\u0026rsquo;ll look at their starter theme Minima. Here\u0026rsquo;s _layouts/default.html in Jekyll (Liquid):\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;{{ page.lang | default: site.lang | default: \u0026#34;en\u0026#34; }}\u0026#34;\u0026gt; {%- include head.html -%} \u0026lt;body\u0026gt; {%- include header.html -%} \u0026lt;main class=\u0026#34;page-content\u0026#34; aria-label=\u0026#34;Content\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;wrapper\u0026#34;\u0026gt; {{ content }} \u0026lt;/div\u0026gt; \u0026lt;/main\u0026gt; {%- include footer.html -%} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Here\u0026rsquo;s Hugo\u0026rsquo;s scaffold theme layouts/_default/baseof.html (Go Templates):\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; {{- partial \u0026#34;head.html\u0026#34; . -}} \u0026lt;body\u0026gt; {{- partial \u0026#34;header.html\u0026#34; . -}} \u0026lt;div id=\u0026#34;content\u0026#34;\u0026gt; {{- block \u0026#34;main\u0026#34; . }}{{- end }} \u0026lt;/div\u0026gt; {{- partial \u0026#34;footer.html\u0026#34; . -}} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Different syntax, same idea. Both templates pull in reusable bits for head.html, header.html, and footer.html. These show up on a lot of pages, so it makes sense not to have to repeat yourself. Both templates also have a spot for the main content, though the Jekyll template uses a variable ({{ content }}) while Hugo uses a block ({{- block \u0026quot;main\u0026quot; . }}{{- end }}). Blocks are just another way Hugo lets you define reusable bits.\nNow that you know how templating works, you can build the sidebar menu for the theme.\nCreating a top-level menu with the pages object You can programmatically create a top-level menu from your pages. It will look like this:\nIntroduction Advanced Usage Let\u0026rsquo;s start with Jekyll. You can display links to site pages in your Liquid template by iterating through the site.pages object that Jekyll provides and building a list:\n\u0026lt;ul\u0026gt; {% for page in site.pages %} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ page.url | absolute_url }}\u0026#34;\u0026gt;{{ page.title }}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {% endfor %} \u0026lt;/ul\u0026gt; This returns all of the site\u0026rsquo;s pages, including all the ones that you might not want, like 404.html. You can filter for the pages you actually want with a couple more tags, such as conditionally including pages if they have a section: true parameter set:\n\u0026lt;ul\u0026gt; {% for page in site.pages %} {%- if page.section -%} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ page.url | absolute_url }}\u0026#34;\u0026gt;{{ page.title }}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {%- endif -%} {% endfor %} \u0026lt;/ul\u0026gt; You can achieve the same effect with slightly less code in Hugo. Loop through Hugo\u0026rsquo;s .Pages object using Go Template\u0026rsquo;s range action:\n\u0026lt;ul\u0026gt; {{ range .Pages }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;{{.Permalink}}\u0026#34;\u0026gt;{{.Title}}\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; This template uses the .Pages object to return all the top-level pages in content/ of your Hugo site. Since Hugo uses a specific folder for the site content you want rendered, there\u0026rsquo;s no additional filtering necessary to build a simple menu of site pages.\nCreating a menu with nested links from a data list Both site generators can use a separately defined data list of links to render a menu in your template. This is more suitable for creating nested links, like this:\nIntroduction Getting Started Configuration Deploying Advanced Usage All Configuration Settings Customizing Help and Support Jekyll supports data files in a few formats, including YAML. Here\u0026rsquo;s the definition for the menu above in _data/menu.yml:\nsection:- page:Introductionurl:/introsubsection:- page:Getting Startedurl:/intro/quickstart- page:Configurationurl:/intro/config- page:Deployingurl:/intro/deploy- page:Advanced Usageurl:/usagesubsection:- page:Customizingurl:/usage/customizing- page:All Configuration Settingsurl:/usage/settings- page:Help and Supporturl:/usage/supportHere\u0026rsquo;s how to render the data in the sidebar template:\n{% for a in site.data.menu.section %} \u0026lt;a href=\u0026#34;{{ a.url }}\u0026#34;\u0026gt;{{ a.page }}\u0026lt;/a\u0026gt; \u0026lt;ul\u0026gt; {% for b in a.subsection %} \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{ b.url }}\u0026#34;\u0026gt;{{ b.page }}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; {% endfor %} \u0026lt;/ul\u0026gt; {% endfor %} This method allows you to build a custom menu, two nesting levels deep. The nesting levels are limited by the for loops in the template. For a recursive version that handles further levels of nesting, see Nested tree navigation with recursion.\nHugo does something similar with its menu templates. You can define menu links in your Hugo site config, and even add useful properties that Hugo understands, like weighting. Here\u0026rsquo;s a definition of the menu above in config.yaml:\nsectionPagesMenu:mainmenu:main:- identifier:introname:Introductionurl:/intro/weight:1- name:Getting Startedparent:introurl:/intro/quickstart/weight:1- name:Configurationparent:introurl:/intro/config/weight:2- name:Deployingparent:introurl:/intro/deploy/weight:3- identifier:usagename:Advanced Usageurl:/usage/- name:Customizingparent:usageurl:/usage/customizing/weight:2- name:All Configuration Settingsparent:usageurl:/usage/settings/weight:1- name:Help and Supportparent:usageurl:/usage/support/weight:3Hugo uses the identifier, which must match the section name, along with the parent variable to handle nesting. Here\u0026rsquo;s how to render the menu in the sidebar template:\n\u0026lt;ul\u0026gt; {{ range .Site.Menus.main }} {{ if .HasChildren }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;{{ .URL }}\u0026#34;\u0026gt;{{ .Name }}\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;ul class=\u0026#34;sub-menu\u0026#34;\u0026gt; {{ range .Children }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;{{ .URL }}\u0026#34;\u0026gt;{{ .Name }}\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{ end }} \u0026lt;/ul\u0026gt; {{ else }} \u0026lt;li\u0026gt; \u0026lt;a href=\u0026#34;{{ .URL }}\u0026#34;\u0026gt;{{ .Name }}\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; {{ end }} {{ end }} \u0026lt;/ul\u0026gt; The range function iterates over the menu data, and Hugo\u0026rsquo;s .Children variable handles nested pages for you.\nPutting the template together With your menu in your reusable sidebar bit (_includes/sidebar.html for Jekyll and partials/sidebar.html for Hugo), you can add it to the default.html template.\nIn Jekyll:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;{{ page.lang | default: site.lang | default: \u0026#34;en\u0026#34; }}\u0026#34;\u0026gt; {%- include head.html -%} \u0026lt;body\u0026gt; {%- include sidebar.html -%} {%- include header.html -%} \u0026lt;div id=\u0026#34;content\u0026#34; class=\u0026#34;page-content\u0026#34; aria-label=\u0026#34;Content\u0026#34;\u0026gt; {{ content }} \u0026lt;/div\u0026gt; {%- include footer.html -%} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; In Hugo:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; {{- partial \u0026#34;head.html\u0026#34; . -}} \u0026lt;body\u0026gt; {{- partial \u0026#34;sidebar.html\u0026#34; . -}} {{- partial \u0026#34;header.html\u0026#34; . -}} \u0026lt;div id=\u0026#34;content\u0026#34; class=\u0026#34;page-content\u0026#34; aria-label=\u0026#34;Content\u0026#34;\u0026gt; {{- block \u0026#34;main\u0026#34; . }}{{- end }} \u0026lt;/div\u0026gt; {{- partial \u0026#34;footer.html\u0026#34; . -}} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; When the site is generated, each page will contain all the code from your sidebar.html.\nCreate a stylesheet Both site generators accept Sass for creating CSS stylesheets. Jekyll has Sass processing built in, and Hugo uses Hugo Pipes. Both options have some quirks.\nSass and CSS in Jekyll To process a Sass file in Jekyll, create your style definitions in the _sass directory. For example, in a file at _sass/style-definitions.scss:\n$background-color: #eef !default; $text-color: #111 !default; body { background-color: $background-color; color: $text-color; } Jekyll won\u0026rsquo;t generate this file directly, as it only processes files with front matter. To create the end-result filepath for your site\u0026rsquo;s stylesheet, use a placeholder with empty front matter where you want the .css file to appear. For example, assets/css/style.scss. In this file, simply import your styles:\n--- --- @import \u0026#34;style-definitions\u0026#34;; This rather hackish configuration has an upside: you can use Liquid template tags and variables in your placeholder file. This is a nice way to allow users to set variables from the site _config.yml, for example.\nThe resulting CSS stylesheet in your generated site has the path /assets/css/style.css. You can link to it in your site\u0026rsquo;s head.html using:\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ \u0026#34;/assets/css/style.css\u0026#34; | relative_url }}\u0026#34; media=\u0026#34;screen\u0026#34;\u0026gt; Sass and Hugo Pipes in Hugo Hugo uses Hugo Pipes to process Sass to CSS. You can achieve this by using Hugo\u0026rsquo;s asset processing function, resources.ToCSS, which expects a source in the assets/ directory. It takes the SCSS file as an argument. With your style definitions in a Sass file at assets/sass/style.scss, here\u0026rsquo;s how to get, process, and link your Sass in your theme\u0026rsquo;s head.html:\n{{ $style := resources.Get \u0026#34;/sass/style.scss\u0026#34; | resources.ToCSS }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;{{ $style.RelPermalink }}\u0026#34; media=\u0026#34;screen\u0026#34;\u0026gt; Hugo asset processing requires extended Hugo, which you may not have by default. You can get extended Hugo from the releases page.\nConfigure and deploy to GitHub Pages Before your site generator can build your site, it needs a configuration file to set some necessary parameters. Configuration files live in the site root directory. Among other settings, you can declare the name of the theme to use when building the site.\nConfigure Jekyll Here\u0026rsquo;s a minimal _config.yml for Jekyll:\ntitle:Your awesome titledescription:\u0026gt;-# this means to ignore newlines until \u0026#34;baseurl:\u0026#34;Write an awesome description for your new site here. You can edit thisline in _config.yml. It will appear in your document head meta (forGoogle search results) and in your feed.xml site description.baseurl:\u0026#34;\u0026#34;# the subpath of your site, e.g. /blogurl:\u0026#34;\u0026#34;# the base hostname \u0026amp; protocol for your site, e.g. http://example.comtheme:# for gem-based themesremote_theme:# for themes hosted on GitHub, when used with GitHub PagesWith remote_theme, any Jekyll theme hosted on GitHub can be used with sites hosted on GitHub Pages.\nJekyll has a default configuration, so any parameters added to your configuration file will override the defaults. Here are additional configuration settings.\nConfigure Hugo Here\u0026rsquo;s a minimal example of Hugo\u0026rsquo;s config.yml:\nbaseURL:https://example.com/# The full domain your site will live atlanguageCode:en-ustitle:Hugo Docs Sitetheme:# theme nameHugo makes no assumptions, so if a necessary parameter is missing, you\u0026rsquo;ll see a warning when building or serving your site. Here are all configuration settings for Hugo.\nDeploy to GitHub Pages Both generators build your site with a command.\nFor Jekyll, use jekyll build. See further build options here.\nFor Hugo, use hugo. You can run hugo help or see further build options here.\nYou\u0026rsquo;ll have to choose the source for your GitHub Pages site; once done, your site will update each time you push a new build. Of course, you can also automate your GitHub Pages build using GitHub Actions. Here\u0026rsquo;s one for building and deploying with Hugo, and one for building and deploying Jekyll.\nShowtime All the substantial differences between these two generators are under the hood; all the same, let\u0026rsquo;s take a look at the finished themes, in two color variations.\nHere\u0026rsquo;s Hugo:\nHere\u0026rsquo;s Jekyll:\nSpiffy!\nWait who won 🤷\nBoth Hugo and Jekyll have their quirks and conveniences.\nFrom this developer\u0026rsquo;s perspective, Jekyll is a workable choice for simple sites without complicated organizational needs. If you\u0026rsquo;re looking to render some one-page posts in an available theme and host with GitHub Pages, Jekyll will get you up and running fairly quickly.\nPersonally, I use Hugo. I like the organizational capabilities of its Page Bundles, and it\u0026rsquo;s backed by a dedicated and conscientious team that really seems to strive to facilitate convenience for their users. This is evident in Hugo\u0026rsquo;s many functions, and handy tricks like Image Processing and Shortcodes. They seem to release new fixes and versions about as often as I make a new cup of coffee.\nIf you still can\u0026rsquo;t decide, don\u0026rsquo;t worry. Many themes are available for both Hugo and Jekyll! Start with one, switch later if you want. That\u0026rsquo;s the benefit of having options.\n",url:"https://victoria.dev/blog/hugo-vs-jekyll-an-epic-battle-of-static-site-generator-themes/"},"https:\/\/victoria.dev\/neofeed\/11533\/":{title:"11533",tags:[],content:"‪Today in yoga news, if you never thought a half moon pose would come in handy in real life, try turning on the shower then getting in before realizing your hot water’s on the fritz.‬\n",url:"https://victoria.dev/neofeed/11533/"},"https:\/\/victoria.dev\/neofeed\/8353\/":{title:"8353",tags:[],content:"The real evidence of technological advances made in our time will be when we don’t start video meetings with, “Hi. Can you hear me?”\n",url:"https://victoria.dev/neofeed/8353/"},"https:\/\/victoria.dev\/neofeed\/7954\/":{title:"7954",tags:[],content:"I have a new desk friend! There will be much debugging together!\nThanks @github @GitHubSecurity @GHSecurityLab 😄\n",url:"https://victoria.dev/neofeed/7954/"},"https:\/\/victoria.dev\/neofeed\/7836\/":{title:"7836",tags:[],content:"I sent my ✉️:\n“Any decisions that weaken the ability of tech companies to build strong and secure applications, including circumventing encryption, are decisions that open more doors for cybersecurity attacks.”\nReject the Graham-Blumenthal bill.\n",url:"https://victoria.dev/neofeed/7836/"},"https:\/\/victoria.dev\/neofeed\/7708\/":{title:"7708",tags:[],content:"2020 high profit margin goods:\n🥑 Cooking utensils shaped like avocados\n📷 Video conference web cams that do bokeh\n🏖 VR company meeting beach scenes\n♟ Board games you can play with your cat\n🧴 Hand cream\n",url:"https://victoria.dev/neofeed/7708/"},"https:\/\/victoria.dev\/blog\/outsourcing-security-with-1password-authy-and-privacy.com\/":{title:"Outsourcing security with 1Password, Authy, and Privacy.com",tags:["cybersecurity","privacy","data","life"],content:"We\u0026rsquo;ve already got enough to deal with without worrying about our cybersecurity. When humans are busy and under stress, we tend to get lax in less-obviously-pressing areas, like the integrity of our online accounts. These areas only become an obvious problem when it\u0026rsquo;s too late for prevention.\nCybersecurity can be fiddly and time-consuming. You might need to reset forgotten passwords, transfer multifactor authentication (MFA) codes to different devices, or deal with the fallout of compromised payment details in the event one of your accounts is still breached.\nThankfully, most of the work necessary to keep up our cybersecurity measures can be outsourced.\nHere are three changes you can make to significantly reduce the chances of needing to fiddle with any of these things again.\n1Password I\u0026rsquo;ve historically avoided password managers because of an irrational knee-jerk reaction to putting all my eggs in one basket. You know what\u0026rsquo;s great for irrational reactions? Education.\nTo figure out if putting all my passwords into a password manager is more secure than not using one, I set out to see what some smart people wrote about it.\nFirst, we need to know a thing or two about passwords. Troy Hunt figured out almost a decade ago that trying to remember strong passwords doesn\u0026rsquo;t work. In more recent times, Alex Weinert expanded on this in Your Pa$$word doesn\u0026rsquo;t matter. TL;DR: our brains aren\u0026rsquo;t better at passwords than computers, and please use MFA.\nSo passwords don\u0026rsquo;t matter, but complicated passwords are still better than memorable and guessable ones. Since I\u0026rsquo;ve next to no hope of remembering a dozen variations of p/q2-q4! (I\u0026rsquo;m not a chess player), this is a task I can outsource to 1Password. I\u0026rsquo;ll still need to remember one, long, complicated master password - 1Password uses this to encrypt my data, so I really can\u0026rsquo;t lose it - but I can handle just one.\nUsing 1Password specifically has another, decidedly obvious, advantage. I chose 1Password because of their Watchtower feature. Thanks to Troy Hunt\u0026rsquo;s Have I Been Pwned, Watchtower will alert you if any of your passwords show up in a breach so you can change them. Passwords still don\u0026rsquo;t completely work, but this is probably the best band-aid there is.\nOne last bonus is that using a password manager is a heck of a lot more convenient. I don\u0026rsquo;t need to take a few tries to type in a complicated password. I don\u0026rsquo;t end up spending time resetting passwords I\u0026rsquo;ve forgotten on sites I only rarely use.\nWhen tasked with remembering all their own passwords, people typically create simpler passwords that are easier to remember \u0026ndash; and easier to hack. This occurs most frequently on sites that are considered unimportant. Using 1Password and generated passwords, those sites are now also first-class citizens in the land of strong passwords, instead of being half-abandoned and half-open attack vectors.\nSo, yes, all my eggs are in one basket. A well-protected, complex, and monitored basket.\nAuthy Okay - so it\u0026rsquo;s more like one-and-a-half baskets. 🤷🏻\nAuthy, from the folks over at Twilio, provides a 2FA solution that\u0026rsquo;s more secure than SMS. Unlike Google Authenticator, you can choose to back up your 2FA codes in case you lose or change your phone. (1Password offers 2FA functionality as well - but, you know, redundancies.)\nWith Authy, your back up is encrypted with your password, similarly to how 1Password works. This makes it the second password you can\u0026rsquo;t forget, if you don\u0026rsquo;t want to lose access to your codes. If you reset your account, they all go away. I can deal with remembering two passwords; I\u0026rsquo;ll take that trade.\nI\u0026rsquo;ve tried other methods of MFA, including hardware keys, which can make accessing accounts on your phone more complicated than I care to put up with. I find the combination of 1Password and Authy to be the most practical combination of convenience and security that yet exists to my knowledge.\nPrivacy.com Finally, there\u0026rsquo;s one last line of defense you can put in place in the unfortunate event that one of your accounts is still compromised. All the strong passwords and MFA in the world won\u0026rsquo;t help if you open the doors yourself, and scams and phishing are a thing.\nSince it\u0026rsquo;s rather impractical to use a different real credit card every place you shop, virtual cards are just a great idea. There\u0026rsquo;s no good reason to spend an afternoon (or more) resetting your payment information on every account just to thwart a misbehaving merchant or patch up a data breach from that online shop for cute salt shakers you made a purchase at last year (just me?).\nAs a bonus, a partnership between 1Password and Privacy.com lets you easily create virtual credit cards using the 1Password extension.\nBy setting up a separate virtual card for each merchant, in the event that one of those merchants is compromised, you can simply pause or delete that card. None of your other accounts or actual bank details are caught up in the process. Cards can have time-based limits or be one-off burner numbers, making them ideal for setting up subscriptions.\nThis is the sort of basic functionality that I hope, one day, becomes more prevalent from banks and credit cards. In the meantime, I\u0026rsquo;ll keep using Privacy.com. That\u0026rsquo;s my referral link; if you\u0026rsquo;d like to thank me by using it, we\u0026rsquo;ll both get five bucks as a bonus.\nOutsource better security All together, implementing these changes will probably take up an afternoon, depending on how many accounts you have. It\u0026rsquo;s worth it for the time you\u0026rsquo;d otherwise spend resetting passwords, setting up new devices, or (knock on wood) recovering from compromised banking details. Best of all, you\u0026rsquo;ll have continual protection just running in the background.\n 1Password Authy Privacy.com  We have the technology. Free up some brain cycles to focus on other things - or simply remove some unnecessary stress from your life by outsourcing the fiddly bits.\nWant to give the gift of cybersecurity to someone you know? Get them started with a cybersecurity starter pack.\n",url:"https://victoria.dev/blog/outsourcing-security-with-1password-authy-and-privacy.com/"},"https:\/\/victoria.dev\/neofeed\/3915\/":{title:"3915",tags:[],content:"Of course, the elephant in the room is that we’ve all had the “Happy Birthday” song stuck in our heads for a month.\n",url:"https://victoria.dev/neofeed/3915/"},"https:\/\/victoria.dev\/neofeed\/3852\/":{title:"3852",tags:[],content:"Weekend to-dos:\n✅ Remove plastic phone case ✅ Buy brass wallet (card holder) ◻️ Change door handles and drawer pulls to copper ◻️ Fill freezer with meat ◻️ Never go out again\n",url:"https://victoria.dev/neofeed/3852/"},"https:\/\/victoria.dev\/neofeed\/7409\/":{title:"7409",tags:[],content:"I turned on the “plain text only” option for my emails and everything is much better now.\n",url:"https://victoria.dev/neofeed/7409/"},"https:\/\/victoria.dev\/neofeed\/6830\/":{title:"6830",tags:[],content:"Yay! 🥰 My first rating!\n⭐️⭐️⭐️⭐️⭐️\n",url:"https://victoria.dev/neofeed/6830/"},"https:\/\/victoria.dev\/neofeed\/3503\/":{title:"3503",tags:[],content:"2017 vs 2020 doodles 😆🖌\n",url:"https://victoria.dev/neofeed/3503/"},"https:\/\/victoria.dev\/neofeed\/3565\/":{title:"3565",tags:[],content:"Be so awesome it wouldn’t have all fit anyway.\n",url:"https://victoria.dev/neofeed/3565/"},"https:\/\/victoria.dev\/neofeed\/6509\/":{title:"6509",tags:[],content:"If you tried to visit my site earlier and it was not there, that is because I accidentally deleted it.\nIt’s back now.\n",url:"https://victoria.dev/neofeed/6509/"},"https:\/\/victoria.dev\/blog\/sqlite-in-production-with-wal\/":{title:"SQLite in production with WAL 🔥",tags:["data","computing"],content:"Update: read the HackerNews discussion.\nSQLite (\u0026ldquo;see-quell-lite\u0026rdquo;) is a lightweight Sequel, or Structured Query Language (SQL), database engine. Instead of using the client-server database management system model, SQLite is self-contained in a single file. It is library, database, and data, all in one package.\nFor certain applications, SQLite is a solid choice for a production database. It\u0026rsquo;s lightweight, ultra-portable, and has no external dependencies. Remember when MacBook Air first came out? It\u0026rsquo;s nothing like that.\nSQLite is best suited for production use in applications that:\n Desire fast and simple set up. Require high reliability in a small package. Have, and want to retain, a small footprint. Are read-heavy but not write-heavy. Don\u0026rsquo;t need multiple user accounts or features like multiversion concurrency snapshots.  If your application can benefit from SQLite\u0026rsquo;s serverless convenience, you may like to know about the different modes available for managing database changes.\nWith and without WAL POSIX system call fsync() commits buffered data (data saved in the operating system cache) referred to by a specified file descriptor to permanent storage or disk. This is relevant to understanding the difference between SQLite\u0026rsquo;s two modes, as fsync() will block until the device reports the transfer is complete.\nFor efficiency, SQLite uses atomic commits to batch database changes into a single transaction. This enables the apparent writing of many transactions to a database file simultaneously. Atomic commits are performed using one of two modes: a rollback journal, or a write-ahead log (WAL).\nRollback journal A rollback journal is essentially a back-up file created by SQLite before write changes occur on a database file. It has the advantage of providing high reliability by helping SQLite restore the database to its original state in case a write operation is compromised during the disk-writing process.\nAssuming a cold cache, SQLite first needs to read the relevant pages from a database file before it can write to it. Information is read out into the operating system cache, then transferred into user space. SQLite obtains a reserved lock on the database file, preventing other processes from writing to the database. At this point, other processes may still read from the database.\nSQLite creates a separate file, the rollback journal, with the original content of the pages that will be changed. Initially existing in the cache, the rollback journal is written to persistent disk storage with fsync() to enable SQLite to restore the database should its next operations be compromised.\nSQLite then obtains an exclusive lock preventing other processes from reading or writing, and writes the page changes to the database file in cache. Since writing to disk is slower than interaction with the cache, writing to disk doesn\u0026rsquo;t occur immediately. The rollback journal continues to exist until changes are safely written to disk, with a second fsync(). From a user-space process point of view, the change to the disk (the COMMIT, or end of the transaction) happens instantaneously once the rollback journal is deleted - hence, atomic commits. However, the two fsync() operations required to complete the COMMIT make this option, from a transactional standpoint, slower than SQLite\u0026rsquo;s lesser known WAL mode.\nWrite-ahead logging (WAL) While the rollback journal method uses a separate file to preserve the original database state, the WAL method uses a separate WAL file to instead record the changes. Instead of a COMMIT depending on writing changes to disk, a COMMIT in WAL mode occurs when a record of one or more commits is appended to the WAL. This has the advantage of not requiring blocking read or write operations to the database file in order to make a COMMIT, so more transactions can happen concurrently.\nWAL mode introduces the concept of the checkpoint, which is when the WAL file is synced to persistent storage before all its transactions are transferred to the database file. You can optionally specify when this occurs, but SQLite provides reasonable defaults. The checkpoint is the WAL version of the atomic commit.\nIn WAL mode, write transactions are performed faster than in the traditional rollback journal mode. Each transaction involves writing the changes only once to the WAL file instead of twice - to the rollback journal, and then to disk - before the COMMIT signals that the transaction is over.\nThe simplicity of SQLite For medium-sized read-heavy applications, SQLite may be a great choice. Using SQLite in WAL mode may make it an even better one. Benchmarks on the smallest EC2 instance, with no provisioned IOPS, put this little trooper at 400 write transactions per second, and thousands of reads. That\u0026rsquo;s some perfectly adequate capability, in a perfectly compact package.\n",url:"https://victoria.dev/blog/sqlite-in-production-with-wal/"},"https:\/\/victoria.dev\/blog\/multithreaded-python-slithering-through-an-i\/o-bottleneck\/":{title:"Multithreaded Python: slithering through an I/O bottleneck",tags:["python","computing","ci/cd","data","open-source"],content:"I recently developed a project that I called Hydra: a multithreaded link checker written in Python. Unlike many Python site crawlers I found while researching, Hydra uses only standard libraries, with no external dependencies like BeautifulSoup. It\u0026rsquo;s intended to be run as part of a CI/CD process, so part of its success depended on being fast.\nMultiple threads in Python is a bit of a bitey subject (not sorry) in that the Python interpreter doesn\u0026rsquo;t actually let multiple threads execute at the same time. Python\u0026rsquo;s Global Interpreter Lock, or GIL, prevents multiple threads from executing Python bytecodes at once. Each thread that wants to execute must first wait for the GIL to be released by the currently executing thread. The GIL is pretty much the microphone in a low-budget conference panel, except where no one gets to shout.\nThis has the advantage of preventing race conditions. It does, however, lack the performance advantages afforded by running multiple tasks in parallel. (If you\u0026rsquo;d like a refresher on concurrency, parallelism, and multithreading, see Concurrency, parallelism, and the many threads of Santa Claus.) While I prefer Go for its convenient first-class primitives that support concurrency (see Goroutines), this project\u0026rsquo;s recipients were more comfortable with Python. I took it as an opportunity to test and explore!\nSimultaneously performing multiple tasks in Python isn\u0026rsquo;t impossible; it just takes a little extra work. For Hydra, the main advantage is in overcoming the input/output (I/O) bottleneck.\nIn order to get web pages to check, Hydra needs to go out to the Internet and fetch them. When compared to tasks that are performed by the CPU alone, going out over the network is comparatively slower. How slow?\nHere are approximate timings for tasks performed on a typical PC:\n    Task Time     CPU execute typical instruction 1/1,000,000,000 sec = 1 nanosec   CPU fetch from L1 cache memory 0.5 nanosec   CPU branch misprediction 5 nanosec   CPU fetch from L2 cache memory 7 nanosec   RAM Mutex lock/unlock 25 nanosec   RAM fetch from main memory 100 nanosec   Network send 2K bytes over 1Gbps network 20,000 nanosec   RAM read 1MB sequentially from memory 250,000 nanosec   Disk fetch from new disk location (seek) 8,000,000 nanosec (8ms)   Disk read 1MB sequentially from disk 20,000,000 nanosec (20ms)   Network send packet US to Europe and back 150,000,000 nanosec (150ms)    Peter Norvig first published these numbers some years ago in Teach Yourself Programming in Ten Years. Since computers and their components change year over year, the exact numbers shown above aren\u0026rsquo;t the point. What these numbers help to illustrate is the difference, in orders of magnitude, between operations.\nCompare the difference between fetching from main memory and sending a simple packet over the Internet. While both these operations occur in less than the blink of an eye (literally) from a human perspective, you can see that sending a simple packet over the Internet is over a million times slower than fetching from RAM. It\u0026rsquo;s a difference that, in a single-thread program, can quickly accumulate to form troublesome bottlenecks.\nIn Hydra, the task of parsing response data and assembling results into a report is relatively fast, since it all happens on the CPU. The slowest portion of the program\u0026rsquo;s execution, by over six orders of magnitude, is network latency. Not only does Hydra need to fetch packets, but whole web pages! One way of improving Hydra\u0026rsquo;s performance is to find a way for the page fetching tasks to execute without blocking the main thread.\nPython has a couple options for doing tasks in parallel: multiple processes, or multiple threads. These methods allow you to circumvent the GIL and speed up execution in a couple different ways.\nMultiple processes To execute parallel tasks using multiple processes, you can use Python\u0026rsquo;s ProcessPoolExecutor. A concrete subclass of Executor from the concurrent.futures module, ProcessPoolExecutor uses a pool of processes spawned with the multiprocessing module to avoid the GIL.\nThis option uses worker subprocesses that maximally default to the number of processors on the machine. The multiprocessing module allows you to maximally parallelize function execution across processes, which can really speed up compute-bound (or CPU-bound) tasks.\nSince the main bottleneck for Hydra is I/O and not the processing to be done by the CPU, I\u0026rsquo;m better served by using multiple threads.\nMultiple threads Fittingly named, Python\u0026rsquo;s ThreadPoolExecutor uses a pool of threads to execute asynchronous tasks. Also a subclass of Executor, it uses a defined number of maximum worker threads (at least five by default, according to the formula min(32, os.cpu_count() + 4)) and reuses idle threads before starting new ones, making it pretty efficient.\nHere is a snippet of Hydra with comments showing how Hydra uses ThreadPoolExecutor to achieve parallel multithreaded bliss:\n# Create the Checker class class Checker: # Queue of links to be checked TO_PROCESS = Queue() # Maximum workers to run THREADS = 100 # Maximum seconds to wait for HTTP response TIMEOUT = 60 def __init__(self, url): ... # Create the thread pool self.pool = futures.ThreadPoolExecutor(max_workers=self.THREADS) def run(self): # Run until the TO_PROCESS queue is empty while True: try: target_url = self.TO_PROCESS.get(block=True, timeout=2) # If we haven\u0026#39;t already checked this link if target_url[\u0026#34;url\u0026#34;] not in self.visited: # Mark it as visited self.visited.add(target_url[\u0026#34;url\u0026#34;]) # Submit the link to the pool job = self.pool.submit(self.load_url, target_url, self.TIMEOUT) job.add_done_callback(self.handle_future) except Empty: return except Exception as e: print(e) You can view the full code in Hydra\u0026rsquo;s GitHub repository.\nSingle thread to multithread If you\u0026rsquo;d like to see the full effect, I compared the run times for checking my website between a prototype single-thread program, and the multiheadedmultithreaded Hydra.\ntime python3 slow-link-check.py https://victoria.dev real 17m34.084s user 11m40.761s sys 0m5.436s time python3 hydra.py https://victoria.dev real 0m15.729s user 0m11.071s sys 0m2.526s The single-thread program, which blocks on I/O, ran in about seventeen minutes. When I first ran the multithreaded version, it finished in 1m13.358s - after some profiling and tuning, it took a little under sixteen seconds. Again, the exact times don\u0026rsquo;t mean all that much; they\u0026rsquo;ll vary depending on factors such as the size of the site being crawled, your network speed, and your program\u0026rsquo;s balance between the overhead of thread management and the benefits of parallelism.\nThe more important thing, and the result I\u0026rsquo;ll take any day, is a program that runs some orders of magnitude faster.\n",url:"https://victoria.dev/blog/multithreaded-python-slithering-through-an-i/o-bottleneck/"},"https:\/\/victoria.dev\/blog\/breaking-bottlenecks\/":{title:"Breaking bottlenecks 🍾",tags:["computing","ci/cd","coding","cybersecurity","go","python"],content:"I recently gave a lecture on the benefits of building non-blocking processes. This is a write-up of the full talk, minus any \u0026ldquo;ums\u0026rdquo; that may have occurred. You can view the slides here.\nI\u0026rsquo;ve been helping out a group called the Open Web Application Security Project (OWASP). They\u0026rsquo;re a non-profit foundation that produces some of the foremost application testing guides and cybersecurity resources. OWASP\u0026rsquo;s publications, checklists, and reference materials are a help to security professionals, penetration testers, and developers all over the world. Most of the individual teams that create these materials are run almost entirely by volunteers.\nOWASP is a great group doing important work. I\u0026rsquo;ve seen this firsthand as part of the core team that produces the Web Security Testing Guide. However, while OWASP inspires in its large volunteer base, it lacks in the area of central organization.\nThis lack of organization was most recently apparent in the group\u0026rsquo;s website, OWASP.org. A big organization with an even bigger website to match, OWASP.org enjoys hundreds of thousands of visitors. Unfortunately, many of its pages - individually managed by disparate projects - are infrequently updated. Some are abandoned. The website as a whole lacks a centralized quality assurance process, and as a result, OWASP.org is peppered with broken links.\nThe trouble with broken links Customers don\u0026rsquo;t like broken links; attackers really do. That\u0026rsquo;s because broken links are a security vulnerability. Broken links can signal opportunities for attacks like broken link hijacking and subdomain takeovers. At their least effective, these attacks can be embarrassing; at their worst, severely damaging to businesses and organizations. One OWASP group, the Application Security Verification Standard (ASVS) project, writes about integrity controls that can help to mitigate the likelihood of these attacks. This knowledge, unfortunately, has not yet propagated throughout the rest of OWASP yet.\nThis is the story of how I created a fast and efficient tool to help OWASP solve this problem.\nThe job I took on the task of creating a program that could run as part of a CI/CD process to detect and report broken links. The program needed to:\n Find and enumerate all the broken links on OWASP.org in a report. Keep track of the parent pages the broken links were on so they could be fixed. Run efficiently as part of a CI/CD pipeline.  Essentially; I need to build a web crawler.\nMy original journey through this process was also in Python, as that was a comfortable language choice for everyone in the OWASP group. Personally, I prefer to use Go for higher performance as it offers more convenient concurrency primitives. Between the task and this talk, I wrote three programs: a prototype single-thread Python program, a multithreaded Python program, and a Go program using goroutines. We\u0026rsquo;ll see a comparison of how each worked out near the end of the talk - first, let\u0026rsquo;s explore how to build a web crawler.\nPrototyping a web crawler Here\u0026rsquo;s what our web crawler will need to do:\n Get the HTML data of the first page of the website (for example, https://victoria.dev) Check all of the links on the page  Keep track of the links we\u0026rsquo;ve already visited so we don\u0026rsquo;t end up checking them twice Record any broken links we find   Fetch more HTML data from any valid links on the page, as long as they\u0026rsquo;re in the same domain (https://victoria.dev and not https://github.com, for instance) Repeat step #2 until all of the links on the site have been checked  Here\u0026rsquo;s what the execution flow will look like:\n As you can see, the nodes \u0026ldquo;GET page\u0026rdquo; -\u0026gt; \u0026ldquo;HTML\u0026rdquo; -\u0026gt; \u0026ldquo;Parse links\u0026rdquo; -\u0026gt; \u0026ldquo;Valid link\u0026rdquo; -\u0026gt; \u0026ldquo;Check visited\u0026rdquo; all form a loop. These are what enable our web crawler to continue crawling until all the links on the site have been accounted for in the \u0026ldquo;Check visited\u0026rdquo; node. When the crawler encounters links it\u0026rsquo;s already checked, it will \u0026ldquo;Stop.\u0026rdquo; This loop will become more important in a moment.\nFor now, the question on everyone\u0026rsquo;s mind (I hope): how do we make it fast?\nHow fast can you do the thing Here are some approximate timings for tasks performed on a typical PC:\n   Type Task Time     CPU execute typical instruction 1/1,000,000,000 sec = 1 nanosec   CPU fetch from L1 cache memory 0.5 nanosec   CPU branch misprediction 5 nanosec   CPU fetch from L2 cache memory 7 nanosec   RAM Mutex lock/unlock 25 nanosec   RAM fetch from main memory 100 nanosec   RAM read 1MB sequentially from memory 250,000 nanosec   Disk fetch from new disk location (seek) 8,000,000 nanosec (8ms)   Disk read 1MB sequentially from disk 20,000,000 nanosec (20ms)   Network send packet US to Europe and back 150,000,000 nanosec (150ms)    Peter Norvig first published these numbers some years ago in Teach Yourself Programming in Ten Years. They typically crop up now and then in articles titled along the lines of, \u0026ldquo;Latency numbers every developer should know.\u0026rdquo;\nSince computers and their components change year over year, the exact numbers shown above aren\u0026rsquo;t the point. What these numbers help to illustrate is the difference, in orders of magnitude, between operations.\nCompare the difference between fetching from main memory and sending a simple packet over the Internet. While both these operations occur in less than the blink of an eye (literally) from a human perspective, you can see that sending a simple packet over the Internet is over a million times slower than fetching from RAM. It\u0026rsquo;s a difference that, in a single-thread program, can quickly accumulate to form troublesome bottlenecks.\nBottleneck: network latency The numbers above mean that the difference in time it takes to send something over the Internet compared to fetching data from main memory is over six orders of magnitude. Remember the loop in our execution chart? The \u0026ldquo;GET page\u0026rdquo; node, in which our crawler fetches page data over the network, is going to be a million times slower than the next slowest thing in the loop!\nWe don\u0026rsquo;t need to run our prototype to see what that means in practical terms; we can estimate it. Let\u0026rsquo;s take OWASP.org, which has upwards of 12,000 links, as an example:\n150 milliseconds x 12,000 links --------- 1,800,000 milliseconds (30 minutes) A whole half hour, just for the network tasks. It may even be much slower than that, since web pages are frequently much larger than a packet. This means that in our single-thread prototype web crawler, our biggest bottleneck is network latency. Why is this problematic?\nFeedback loops I previously wrote about feedback loops. In essence, in order to improve at doing anything, you first need to be able to get feedback from your last attempt. That way, you have the necessary information to make adjustments and get closer to your goal on your next iteration.\nAs a software developer, bottlenecks can contribute to long and inefficient feedback loops. If I\u0026rsquo;m waiting on a process that\u0026rsquo;s part of a CI/CD pipeline, in our bottlenecked web crawler example, I\u0026rsquo;d be sitting around for a minimum of a half hour before learning whether or not changes in my last push were successful, or whether they broke master (hopefully staging).\nMultiply a slow and inefficient feedback loop by many runs per day, over many days, and you\u0026rsquo;ve got a slow and inefficient developer. Multiply that by many developers in an organization bottlenecked on the same process, and you\u0026rsquo;ve got a slow and inefficient company.\nThe cost of bottlenecks To add insult to injury, not only are you waiting on a bottlenecked process to run; you\u0026rsquo;re also paying to wait. Take the serverless example - AWS Lambda, for instance. Here\u0026rsquo;s a chart showing the cost of functions by compute time and CPU usage.\n Source: Understanding and Controlling AWS Lambda Costs\n  Again, the numbers change over the years, but the main concepts remain the same: the bigger the function and the longer its compute time, the bigger the cost. For applications taking advantage of serverless, these costs can add up dramatically.\nBottlenecks are a recipe for failure, for both productivity and the bottom line.\nThe good news is that bottlenecks are mostly unnecessary. If we know how to identify them, we can strategize our way out of them. To understand how, let\u0026rsquo;s get some tacos.\nTacos and threading Everyone, meet Bob. He\u0026rsquo;s a gopher who works at the taco stand down the street as the cashier. Say \u0026ldquo;Hi,\u0026rdquo; Bob.\n🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 🌮 🌳 🌮 🌮 ╔══════════════╗ 🌮 Hi I\u0026#39;m Bob 🌳 🌮 ╚══════════════╝ \\ 🌮 🐹 🌮 🌮 🌮 🌮 🌳 🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 Bob works very hard at being a cashier, but he\u0026rsquo;s still just one gopher. The customers who frequent Bob\u0026rsquo;s taco stand can eat tacos really quickly; but in order to get the tacos to eat them, they\u0026rsquo;ve got to order them through Bob. Here\u0026rsquo;s what our bottlenecked, single-thread taco stand currently looks like:\n🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 🌮 🌳 🌮 🌮 🌮 🌳 🌮 🐹 🧑💵🧑💵🧑💵🧑💵🧑💵🧑💵🧑💵🧑💵🧑💵 🌮 🌮 🌮 🌮 🌳 🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 As you can see, all the customers are queued up, right out the door. Poor Bob handles one customer\u0026rsquo;s transaction at a time, starting and finishing with that customer completely before moving on to the next. Bob can only do so much, so our taco stand is rather inefficient at the moment. How can we make Bob faster?\nWe can try splitting the queue:\n🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 🌮 🌳 🌮 🌮 🧑💵🧑💵🧑💵🧑💵 🌮 🌳 🌮 🐹 🌮 🌮 🧑💵🧑💵🧑💵🧑💵🧑💵 🌮 🌮 🌳 🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 Now Bob can do some multitasking. For example, he can start a transaction with a customer in one queue; then, while that customer counts their bills, Bob can pop over to the second queue and get started there. This arrangement, known as a concurrency model, helps Bob go a little bit faster by jumping back and forth between lines. However, it\u0026rsquo;s still just one Bob, which limits our improvement possibilities. If we were to make four queues, they\u0026rsquo;d all be shorter; but Bob would be very thinly stretched between them. Can we do better?\nWe could get two Bobs:\n🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 🌮 🌳 🌮 🌮 🌳 🌮 🐹 🧑💵🧑💵🧑💵🧑💵 🌮 🌳 🌮 🐹 🧑💵🧑💵🧑💵🧑💵🧑💵 🌮 🌳 🌮 🌮 🌳 🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 With twice the Bobs, each can handle a queue of his own. This is our most efficient solution for our taco stand so far, since two Bobs can handle much more than one Bob can, even if each customer is still attended to one at a time.\nWe can do even better than that:\n🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 🌮 🌳 🌮 🐹 🧑💵🧑💵 🌮 🌳 🌮 🐹 🧑💵🧑💵 🌮 🌳 🌮 🐹 🧑💵🧑💵 🌮 🌳 🌮 🐹 🧑💵🧑💵🧑💵 🌮 🌳 🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮🌮 With quadruple the Bobs, we have some very short queues, and a much more efficient taco stand. In computing, the concept of having multiple workers do tasks in parallel is called multithreading.\nIn Go, we can apply this concept using goroutines. Here are some illustrative snippets from my Go solution.\nSetting up a Go web crawler In order to share data between our goroutines, we\u0026rsquo;ll need to create some data structures. Our Checker structure will be shared, so it will have a Mutex (mutual exclusion) to allow our goroutines to lock and unlock it. The Checker structure will also hold a list of brokenLinks results, and visitedLinks. The latter will be a map of strings to booleans, which we\u0026rsquo;ll use to directly and efficiently check for visited links. By using a map instead of iterating over a list, our visitedLinks lookup will have a constant complexity of O(1) as opposed to a linear complexity of O(n), thus avoiding the creation of another bottleneck. For more on time complexity, see my coffee-break introduction to time complexity of algorithms article.\ntype Checker struct { startDomain string brokenLinks []Result visitedLinks map[string]bool workerCount, maxWorkers int sync.Mutex } ... // Page allows us to retain parent and sublinks type Page struct { parent, loc, data string } // Result adds error information for the report type Result struct { Page reason string code int } To extract links from HTML data, here\u0026rsquo;s a parser I wrote on top of package html:\n// Extract links from HTML func parse(parent, data string) ([]string, []string) { doc, err := html.Parse(strings.NewReader(data)) if err != nil { fmt.Println(\u0026#34;Could not parse: \u0026#34;, err) } goodLinks := make([]string, 0) badLinks := make([]string, 0) var f func(*html.Node) f = func(n *html.Node) { if n.Type == html.ElementNode \u0026amp;\u0026amp; checkKey(string(n.Data)) { for _, a := range n.Attr { if checkAttr(string(a.Key)) { j, err := formatURL(parent, a.Val) if err != nil { badLinks = append(badLinks, j) } else { goodLinks = append(goodLinks, j) } break } } } for c := n.FirstChild; c != nil; c = c.NextSibling { f(c) } } f(doc) return goodLinks, badLinks } If you\u0026rsquo;re wondering why I didn\u0026rsquo;t use a more full-featured package for this project, I highly recommend the story of left-pad. The short of it: more dependencies, more problems.\nHere are snippets of the main function, where we pass in our starting URL and create a queue (or channels, in Go) to be filled with links for our goroutines to process.\nfunc main() { ... startURL := flag.String(\u0026#34;url\u0026#34;, \u0026#34;http://example.com\u0026#34;, \u0026#34;full URL of site\u0026#34;) ... firstPage := Page{ parent: *startURL, loc: *startURL, } toProcess := make(chan Page, 1) toProcess \u0026lt;- firstPage var wg sync.WaitGroup The last significant piece of the puzzle is to create our workers, which we\u0026rsquo;ll do here:\nfor i := range toProcess { wg.Add(1) checker.addWorker() 🐹 go worker(i, \u0026amp;checker, \u0026amp;wg, toProcess) if checker.workerCount \u0026gt; checker.maxWorkers { time.Sleep(1 * time.Second) // throttle down  } } wg.Wait() A WaitGroup does just what it says on the tin: it waits for our group of goroutines to finish. When they have, we\u0026rsquo;ll know our Go web crawler has finished checking all the links on the site.\nDid we do the thing fast Here\u0026rsquo;s a comparison of the three programs I wrote on this journey. First, the prototype single-thread Python version:\ntime python3 slow-link-check.py https://victoria.dev real 17m34.084s user 11m40.761s sys 0m5.436s This finished crawling my website in about seventeen-and-a-half minutes, which is rather long for a site at least an order of magnitude smaller than OWASP.org.\nThe multithreaded Python version did a bit better:\ntime python3 hydra.py https://victoria.dev real 1m13.358s user 0m13.161s sys 0m2.826s My multithreaded Python program (which I dubbed Hydra) finished in one minute and thirteen seconds.\nHow did Go do?\ntime ./go-link-check --url=https://victoria.dev real 0m7.926s user 0m9.044s sys 0m0.932s At just under eight seconds, I found the Go version to be extremely palatable.\nBreaking bottlenecks As fun as it is to simply enjoy the speedups, we can directly relate these results to everything we\u0026rsquo;ve learned so far. Consider taking a process that used to soak up seventeen minutes and turning it into an eight-second-affair instead. Not only will that give developers a much shorter and more efficient feedback loop, it will give companies the ability to develop faster, and thus grow more quickly - while costing less. To drive the point home: a process that runs in seventeen-and-a-half minutes when it could take eight seconds will also cost over a hundred and thirty times as much to run!\nA better work day for developers, and a better bottom line for companies. There\u0026rsquo;s a lot of benefit to be had in making functions, code, and processes as efficient as possible - by breaking bottlenecks.\n",url:"https://victoria.dev/blog/breaking-bottlenecks/"},"https:\/\/victoria.dev\/neofeed\/2722\/":{title:"2722",tags:[],content:"“\u0026hellip;all technical solutions for better cybersecurity hinge on the people that use them. The cyber readiness of the department needs to be on the same level as physical fitness and weaponry.”\n👆\nNavy outlines a path to completely redesign its networks: https://www.fedscoop.com/navy-path-to-network-modernization/\n",url:"https://victoria.dev/neofeed/2722/"},"https:\/\/victoria.dev\/neofeed\/4946\/":{title:"4946",tags:[],content:"“The victim cited gaps in cybersecurity knowledge and the wide range of possible scenarios as reasons for failing to adequately incorporate cybersecurity into emergency response planning.”\nOr, “It seemed hard, so we just didn’t do anything at all.” 🙄\nhttps://www.us-cert.gov/ncas/alerts/aa20-049a\n",url:"https://victoria.dev/neofeed/4946/"},"https:\/\/victoria.dev\/blog\/command-line-tricks-for-managing-your-messy-open-source-repository\/":{title:"Command line tricks for managing your messy open source repository",tags:["terminal","linux","open-source"],content:"Effective collaboration, especially in open source software development, starts with effective organization. To make sure that nothing gets missed, the general rule, \u0026ldquo;one issue, one pull request\u0026rdquo; is a nice rule of thumb.\nInstead of opening an issue with a large scope like, \u0026ldquo;Fix all the broken links in the documentation,\u0026rdquo; open source projects will have more luck attracting contributors with several smaller and more manageable issues. In the preceding example, you might scope broken links by section or by page. This allows more contributors to jump in and dedicate small windows of their time, rather than waiting for one person to take on a larger and more tedious contribution effort.\nSmaller scoped issues also help project maintainers see where work has been completed and where it hasn\u0026rsquo;t. This reduces the chances that some part of the issue is missed, assumed to be completed, and later leads to bugs or security vulnerabilities.\nThat\u0026rsquo;s all well and good; but what if you\u0026rsquo;ve already opened several massively-scoped issues, some PRs have already been submitted or merged, and you currently have no idea where the work started or stopped?\nIt\u0026rsquo;s going to take a little sorting out to get the state of your project back under control. Thankfully, there are a number of command line tools to help you scan, sort, and make sense of a messy repository. Here\u0026rsquo;s a small selection of ones I use.\nJump to:\n Interactive search-and-replace with vim Find dead links in Markdown files with a node module List subdirectories with or without a git repository with find Pull multiple git repositories from a list with xargs List issues by number with jot CLI-powered open source organization  Interactive search-and-replace with vim You can open a file in Vim, then interactively search and replace with:\n:%s/\\\u0026lt;word\\\u0026gt;/newword/gcThe % indicates to look in all lines of the current file; s is for substitute; \\\u0026lt;word\\\u0026gt; matches the whole word; and the g for \u0026ldquo;global\u0026rdquo; is for every occurrence. The c at the end will let you view and confirm each change before it\u0026rsquo;s made. You can run it automatically, and much faster, without c; however, you put yourself at risk of complicating things if you\u0026rsquo;ve made a pattern-matching error.\nFind dead links in Markdown files with a node module The markdown-link-check node module has a great CLI buddy.\nI use this so often I turned it into a Bash alias function. To do the same, add this to your .bashrc:\n# Markdown link check in a folder, recursive function mlc () { find $1 -name \\*.md -exec markdown-link-check -p {} \\; } Then run with mlc \u0026lt;filename\u0026gt;.\nList subdirectories with or without a git repository with find Print all subdirectories that are git repositories, or in other words, have a .git in them:\nfind . -maxdepth 1 -type d -exec test -e \u0026#39;{}/.git\u0026#39; \u0026#39;;\u0026#39; -printf \u0026#34;is git repo: %p\\n\u0026#34; To print all subdirectories that are not git repositories, negate the test with !:\nfind . -maxdepth 1 -type d -exec test \u0026#39;!\u0026#39; -e \u0026#39;{}/.git\u0026#39; \u0026#39;;\u0026#39; -printf \u0026#34;not git repo: %p\\n\u0026#34; Pull multiple git repositories from a list with xargs I initially used this as part of automatically re-creating my laptop with Bash scripts, but it\u0026rsquo;s pretty handy when you\u0026rsquo;re working with cloud instances or Dockerfiles.\nGiven a file, repos.txt with a repository’s SSH link on each line (and your SSH keys set up), run:\nxargs -n1 git clone \u0026lt; repos.txt If you want to pull and push many repositories, I previously wrote about how to use a Bash one-liner to manage your repositories.\nList issues by number with jot I\u0026rsquo;m a co-author and maintainer for the OWASP Web Security Testing Guide repository where I recently took one large issue (yup, it was \u0026ldquo;Fix all the broken links in the documentation\u0026rdquo; - how\u0026rsquo;d you guess?) and broke it up into several smaller, more manageable issues. A whole thirty-seven smaller, more manageable issues.\nI wanted to enumerate all the issues that the original one became, but the idea of typing out thirty-seven issue numbers (#275 through #312) seemed awfully tedious and time-consuming. So, in natural programmer fashion, I spent the same amount of time I would have used to type out all those numbers and crafted a way to automate it instead.\nThe jot utility (apt install athena-jot) is a tiny tool that\u0026rsquo;s a big help when you want to print out some numbers. Just tell it how many you want, and where to start and stop.\n# jot [ reps [ begin [ end ] ] ] jot 37 275 312 This prints each number, inclusively, from 275 to 312 on a new line. To make these into issue number notations that GitHub and many other platforms automatically recognize and turn into links, you can pipe the output to awk.\njot 37 275 312 | awk \u0026#39;{printf \u0026#34;#\u0026#34;$0\u0026#34;, \u0026#34;}\u0026#39; #275, #276, #277, #278, #279, #280, #281, #282, #283, #284, #285, #286, #287, #288, #289, #290, #291, #292, #293, #295, #296, #297, #298, #299, #300, #301, #302, #303, #304, #305, #306, #307, #308, #309, #310, #311, #312 You can also use jot to generate random or redundant data, mainly for development or testing purposes.\nCLI-powered open source organization A well-organized open source repository is a well-maintained open source project. Save this post for handy reference, and use your newfound CLI superpowers for good! 🚀\n",url:"https://victoria.dev/blog/command-line-tricks-for-managing-your-messy-open-source-repository/"},"https:\/\/victoria.dev\/blog\/why-pixelfed-wont-save-us-from-instagram\/":{title:"Why PixelFed won\u0027t save us from Instagram",tags:["life"],content:"PixelFed is a decentralized photo sharing network based on the ActivityPub protocol, the same one that Mastodon uses. For a lot of people divorced (or wanting to be) from Instagram over mental health concerns and issues like forced consent to post-GDPR terms, a decentralized social network like PixelFed sounds like an exciting and promising alternative.\nPersonally, I stopped using Instagram once I accepted the fact that its core premise and integral structure of social interaction was encouraging me to form habits that were harmful to my life goals. I\u0026rsquo;m not alone - studies have shown that people are happier after deleting apps like Facebook. The reasons for this don\u0026rsquo;t differ greatly from why any social network can be bad for you - they\u0026rsquo;re just found in much greater intensity on photo sharing sites, and specifically Instagram.\nIt is still early days for PixelFed. As I write this I have no way to know what kind of network it will become, or even if it will survive at all. I do know, however, that there are many glaring and fundamental problems that a decentralized photo sharing network like PixelFed won\u0026rsquo;t solve. To elaborate, I\u0026rsquo;m going to discuss what makes Instagram so poisonous to health, why centralized social networks aren\u0026rsquo;t likely to ever be healthy, and why decentralized social networks have a very slim chance of being better.\nWhy Instagram is bad for your health Let\u0026rsquo;s start with the basics. Your brain responds very differently to reading text than it does to looking at images.\nIt doesn\u0026rsquo;t take more than a quick search to find hundreds of articles and studies about how reading can make you smarter, more empathetic, and stave off cognitive decline by improving brain connectivity. In essence, reading involves a multitude of brain regions including the temporal and frontal lobes. There\u0026rsquo;s still a lot to be discovered about the human brain, but here\u0026rsquo;s what we think we know. The frontal lobes control important cognitive skills like emotional expression, problem solving, memory, language, judgement, and sexual behaviors. The temporal lobes handle important functions such as the encoding of memory, and processing emotions and visual perception.\nIn other words, reading text - even on social media - stimulates your brain and makes you think about the information you\u0026rsquo;re taking in. To react to words on a page, you first have to read them and form thoughts about them.\nUnlike reading, looking at an image has a very different effect on your brain. Here\u0026rsquo;s an infographic about infographics that covers some of these effects. Basically, millennia of evolution have produced human brains hardwired to respond quickly to visual stimuli - in less than 1/10 of a second. As the infographic will literally show you, almost 50% of the brain is involved in visual processing, and 70% of all our sensory receptors are in our eyes. That\u0026rsquo;s a lot of resources devoted to quickly processing visuals. Why could this be bad?\nUnlike times past, we\u0026rsquo;re no longer (day-to-day) concerned with spotting a tiger in the bushes about to pounce on us. The near-instant processing time needed to discern if that shivering tree branch is the just wind or impending mortal danger is outdated in our current living arrangements. Our brain, however, doesn\u0026rsquo;t know that. It hasn\u0026rsquo;t evolved faster than our technologies or society. The downside to this is that anyone with a little knowledge of this fundamental flaw in the human mind is able to exploit it.\nAdvertisers call this exploitation, \u0026ldquo;visual marketing.\u0026rdquo;\nThese linked articles are stuffed with the same factoids over and over again. \u0026ldquo;The brain processes images 60,000 times faster than text.\u0026rdquo; \u0026ldquo;90% of the information sent to the brain is visual.\u0026rdquo; Whether or not these numbers are accurate, it\u0026rsquo;s clearly provable that visual marketing is on the whole more effective than advertisements without images. There\u0026rsquo;s a reason for it, and it should scare you.\nUnlike reading, which involves regions of your brain responsible for comprehension, decision making, and emotional control, images are processed by different areas of the brain. Visual input travels from our eyes through our optic nerves to the thalamus (or LGN, Lateral Geniculate Nucleus) and the superior colliculus. From the thalamus, it proceeds to the visual cortex at the rear of our brains, where the image is processed. Effectively, viewing images does not make us think in the same way that reading does. In other words, it\u0026rsquo;s easy to do.\nLet me be clear. This difference in the way words and images are processed is not, in itself, bad. A photo-centric social network is not, in itself, bad. Images and words alike have the power to evoke strong emotions, send powerful messages, spark revolutions, and spur progress. This is good\u0026hellip; if it\u0026rsquo;s used for good.\nInstagram, a photo-centric network chock full of product placements, paid sponsorships, and outright advertisements, is a social network primarily designed to bypass your cognitive thinking and sell you stuff.\nI don\u0026rsquo;t think Instagram started out with the same motivations it has now. Along with all the photo sharing networks that blossomed when Instagram first got popular, I still believe its initial vision was to make sharing photos with your friends fun and easy.\nIt just got too popular.\nWhy centralized networks are bad for your health In the wake of privacy concerns over the last few years, new uproar over algorithm-driven timelines, and the #DeleteFacebook, #DeleteTwitter, and #DeleteInstagram movements, more people today are aware of how networks that make their money on your data are bad for your health. This is in part due to their centralized nature - one hierarchy of authority makes decisions for the whole system, and at the same time, has to support it. It\u0026rsquo;s expensive to support millions of users, so it\u0026rsquo;s no wonder that the network\u0026rsquo;s main concern (and let\u0026rsquo;s just consider the most innocent case) is to remain profitable.\nWhat\u0026rsquo;s a good way to remain profitable?\n Take a human desire, preferably one that has been around for a really long time\u0026hellip; Identify that desire and use modern technology to take out steps. \u0026ndash; Evan Williams, co-founder of Twitter and Blogger\nQuoted in Wired article, 2013, \u0026ldquo;Twitter Founder Reveals Secret Formula for Getting Rich Online\u0026quot;\n There\u0026rsquo;s a book called Hooked: How to Build Habit-Forming Products which, if you\u0026rsquo;re ever in the mood for a good horror flick, you should curl up in bed with some popcorn and read.\nThe book details a simple model for a habit-forming product. The model is cyclical, and has the following key points: a trigger, an action, variable reward, and investment. In summary, if a product can get you to think of it, leading to some action that is easier to do than to think about, give you a reward for that action some of the time, and then compel you to commit or invest in it - you\u0026rsquo;re hooked.\nIf you\u0026rsquo;re paying attention, you might notice I\u0026rsquo;ve described Instagram. And Twitter. And Facebook. And every other social network.\nThere\u0026rsquo;s a reason it\u0026rsquo;s easy to use Instagram, easy to post a tweet, easy to browse Facebook. These products have been designed to make it easy for you to use them. They\u0026rsquo;ve been designed to alter your behavior to better suit the product\u0026rsquo;s goals.\n This industry employs some of the smartest people, thousands of Ph.D. designers, statisticians, engineers. They go to work every day to get us to do this one thing, to undermine our willpower. \u0026ndash; James Williams, co-founder of Time Well Spent\nQuoted in Nautilus article, 2017, \u0026ldquo;Modern Media Is a DoS Attack on Your Free Will\u0026quot;\n At the heart of the idea of getting you hooked is the concept of a dopamine feedback loop. Dopamine, an organic chemical neurotransmitter in your brain, is thought to be responsible for allowing us to anticipate the reward to an action. It inspires us to get a glass of water when we\u0026rsquo;re thirsty, for example, and may help us to feel good when we take actions towards doing so. Where dopamine is so effectively misused is in the practice of providing variable rewards to drive social media addiction.\nUnlike getting a glass of water when you\u0026rsquo;re thirsty, variable rewards are random. It\u0026rsquo;s as if drinking water sometimes, but not always, cured your thirst. This effectively programs your mind to pursue the action that results in the unpredictable reward. Since getting the reward isn\u0026rsquo;t guaranteed, you need to make more attempts to achieve success. Social media is designed to make these variable dopamine hits easy to obtain. It\u0026rsquo;s designed to hijack your intellectual independence in order to keep you on the network.\nEspecially when the main goal of a centralized social network is to make a profit, that network is exploiting evolutionary flaws in your brain to make that profit from you. You are literally being hacked.\nNow combine this information with the knowledge of how a product comprised primarily of images bypasses your cognitive thinking. Not only are you being hacked, but your main defense system is being easily, laughably, circumvented.\nExploiting users is a particularly compelling temptation for any social networks under pressure to make a profit, and this pressure is amplified in organizations with a centralized structure. Not all centralized networks do this, but undoubtedly, the very successful ones do.\nDecentralization is by no means a fix for exploitation and greed, but a decentralized social network might have a few things going for it.\nWhy decentralized networks might be slightly better for your health The main issues present in Twitter, Facebook, and Instagram as pertains to social media addiction do not go away on decentralized networks. I\u0026rsquo;m personally, currently, using both Twitter and Mastodon. The former is centralized and the latter is decentralized, but the the same motivations that could get me in trouble on one platform apply to both. Decentralization does not fix the problem.\nIt might help.\nUnlike a centralized, single-hierarchy, definitely-for-profit social network, decentralization has one thing going for it: more people. Specifically, more instance owners who are in control of their instances.\nRunning a Mastodon instance is a responsibility, should you choose to accept it. Besides the server itself, instances require their own sets of rules and code of conduct, and like the often adopted mastodon.social code of conduct, it can be collaboratively drafted by the community. Mastodon provides instance owners with moderation tools and provides users with reporting tools, and there\u0026rsquo;s an expectation that they\u0026rsquo;ll both be used. As with other decentralized social networks, it is the responsibility of the instance owner to moderate and foster a social environment that serves the best interests of the instance users.\nInstances typically run on donations, and in the grand scheme of things, are inexpensive to support. Decentralization means that instance owners individually have to bear smaller costs. There\u0026rsquo;s no central body being pressured to make a profit in order to run servers that support millions of users. The effect of this many-owners structure is that decisions that concern any particular instance and rules that it might want to adopt are made by that instance\u0026rsquo;s community, or the instance owner. If a user disagrees with the direction taken, they can communicate directly with the instance owner, or simply move to another instance. There\u0026rsquo;s no \u0026ldquo;take it or leave it,\u0026rdquo; and no forced acceptance of terms. Users always have somewhere else to go.\nThis, in general, means that over many instances, and via many moderators, more people from diverse backgrounds with a collection of both overlapping and contrasting interests are able to have a voice in how the social network evolves.\nIf instance owners have their users' best interests, not addiction, in mind; if moderators act responsibly, and according to their instance rules, moderate for good; and if a wide and varying selection of instances with differing interests, political viewpoints, and topics continue to be available; then decentralized social networks might be better for your health.\nWhere this leaves PixelFed (and all social networks) All social networks have the potential to do more good than harm, but it is up to those who control them to put in the constant, proactive effort required to make that happen. Twitter has recently been making some steps towards becoming a healthier network, like banning political ads and highlighting manipulated media. I think they\u0026rsquo;re ahead of the curve. With decentralized social networks, there\u0026rsquo;s at least more chances for the possibility that instance owners truly want to do more good than harm with their own little piece of the whole.\nWhile photo sharing networks will, by their essential nature, bypass cognitive thinking and have an advantage over their users that way, there are many design considerations that PixelFed can implement in order to make the network healthier. Features such as comments, likes, timelines, and push notifications can be designed to provide utility more than drive addiction, and there are designers more qualified than I who can tell you how.\nThese networks will have to constantly resist the temptation to take the easy route. They will have to work to avoid success based on the exploitation of their users' desires to chase the easy dopamine hit. They will have to prioritize the ability of the social network to add real value to the lives of its users - at the expense of its own potential to garner mindless, meaningless popularity.\nThis is in no way a condemnation of PixelFed or any other decentralized photo sharing network. Personally, I sincerely hope they succeed in giving users a healthy, safe, and free-as-in-freedom network for sharing photos with friends, and with the rest of the federated community. It will require considered design with mental health at the forefront; the active, caring effort of moderators and instance owners; and ongoing collaboration from the federated community at large to work together to build for the greater good.\nA photo-sharing social media network that does more good than harm? It\u0026rsquo;s possible. But it won\u0026rsquo;t be easy.\n",url:"https://victoria.dev/blog/why-pixelfed-wont-save-us-from-instagram/"},"https:\/\/victoria.dev\/neofeed\/2154\/":{title:"2154",tags:[],content:"And it’s ready! Here you go, @jekyllrb users. Make your CI/CD less evil.\nhttps://github.com/victoriadrake/jekyll-cd\n",url:"https://victoria.dev/neofeed/2154/"},"https:\/\/victoria.dev\/neofeed\/4108\/":{title:"4108",tags:[],content:"How to get a shell in local @Docker container:\ndocker run -it \u0026ndash;entrypoint /bin/sh repository:tag\nIf the image has Bash, you can also substitute /bin/bash.\n",url:"https://victoria.dev/neofeed/4108/"},"https:\/\/victoria.dev\/neofeed\/2134\/":{title:"2134",tags:[],content:"I have a suspicion that too many developers concern themselves with making what they can make, and too few developers concern themselves with making what people use.\n",url:"https://victoria.dev/neofeed/2134/"},"https:\/\/victoria.dev\/neofeed\/4129\/":{title:"4129",tags:[],content:"I’ll blog about this one but I couldn’t wait to share it with you!\n🐍 Multithreaded site-crawling link checker in Python standard library https://github.com/victoriadrake/hydra-link-checker\nWhat questions would you like to see answered in my post?\n",url:"https://victoria.dev/neofeed/4129/"},"https:\/\/victoria.dev\/neofeed\/2156\/":{title:"2156",tags:[],content:"Using @GoHugoIO with @GitHub Pages? I made this for you.\nGitHub Action using latest extended Hugo to build and deploy 🚀 a Hugo site to GitHub Pages https://github.com/victoriadrake/hugo-latest-cd\n",url:"https://victoria.dev/neofeed/2156/"},"https:\/\/victoria.dev\/neofeed\/3807\/":{title:"3807",tags:[],content:"Three little letters made me very happy recently:\nE. A. D.\nI can work again. 😁\n",url:"https://victoria.dev/neofeed/3807/"},"https:\/\/victoria.dev\/neofeed\/2017\/":{title:"2017",tags:[],content:"So I’m going to try the carnivore diet, but first I’m going to finish eating all the bread I keep whiskey-buying on Amazon Fresh. 🥃🥖🧈\n",url:"https://victoria.dev/neofeed/2017/"},"https:\/\/victoria.dev\/neofeed\/1809\/":{title:"1809",tags:[],content:"I received some exciting news yesterday that I’m hoping I’ll soon be able to share with you all! In the meantime, I made some updates to Kabukichō that make it better for long coding sessions like the one I’m hacking away at today. 👩🏻‍💻🌆\nGet v0.0.2: https://marketplace.visualstudio.com/items?itemName=VictoriaDrake.kabukicho\n",url:"https://victoria.dev/neofeed/1809/"},"https:\/\/victoria.dev\/neofeed\/1734\/":{title:"1734",tags:[],content:"Here’s a cool security project worth hacking on: an open source security key!\nhttps://github.com/google/OpenSK\n",url:"https://victoria.dev/neofeed/1734/"},"https:\/\/victoria.dev\/neofeed\/1743\/":{title:"1743",tags:[],content:"Published my first Visual Studio Code theme and it is a true reflection of my innermost soul. 🌆\nhttps://github.com/victoriadrake/kabukicho-vscode\n",url:"https://victoria.dev/neofeed/1743/"},"https:\/\/victoria.dev\/neofeed\/1256\/":{title:"1256",tags:[],content:"Monday thoughts:\nThe initial goal isn’t the forever goal. If you started over from scratch today, what would need to be different?\nRegularly reevaluate your assumptions. Adapt faster. Come out ahead.\n",url:"https://victoria.dev/neofeed/1256/"},"https:\/\/victoria.dev\/neofeed\/2338\/":{title:"2338",tags:[],content:"\u0026ldquo;\u0026hellip; And this is another example of why you shouldn\u0026rsquo;t mutate your underlying data structures!\u0026rdquo;\n",url:"https://victoria.dev/neofeed/2338/"},"https:\/\/victoria.dev\/neofeed\/1218\/":{title:"1218",tags:[],content:"Having a solution doesn\u0026rsquo;t mean you\u0026rsquo;ve solved a problem.\n",url:"https://victoria.dev/neofeed/1218/"},"https:\/\/victoria.dev\/neofeed\/1246\/":{title:"1246",tags:[],content:"If we all stopped making fun of people who ask silly questions and instead tried to teach what we know, we\u0026rsquo;d all know just a little bit more and we\u0026rsquo;d all have far fewer people to make fun of.\n",url:"https://victoria.dev/neofeed/1246/"},"https:\/\/victoria.dev\/neofeed\/1241\/":{title:"1241",tags:[],content:"It’s time for Conference Buzzword Drinking Game! 🍻\n Stood up Alignment Vertical POC Right-sizing Actionable  ",url:"https://victoria.dev/neofeed/1241/"},"https:\/\/victoria.dev\/neofeed\/1157\/":{title:"1157",tags:[],content:"Crypto means cryptography, and fries with gravy is not poutine. Oh, yes, it’s nice to meet you too.\n",url:"https://victoria.dev/neofeed/1157/"},"https:\/\/victoria.dev\/neofeed\/907\/":{title:"907",tags:[],content:"I could live the rest of my life on good sourdough bread and salted butter if it had a better nutritional profile. 🥖🧈\nHave we invented the pill that makes our bodies turn carbs into proteins yet?\n",url:"https://victoria.dev/neofeed/907/"},"https:\/\/victoria.dev\/neofeed\/930\/":{title:"930",tags:[],content:"For goodness sakes, you’ve done this a thousand times already. It’s still top, right, bottom, left.\n",url:"https://victoria.dev/neofeed/930/"},"https:\/\/victoria.dev\/neofeed\/1658\/":{title:"1658",tags:[],content:"I\u0026rsquo;m always amazed when I attend in-depth technical talks about complex topics with intelligent speakers and audiences, and infallibly, there will be a moment where no one in the room knows why the projector stopped working.\n🤷🏻‍♀️\n",url:"https://victoria.dev/neofeed/1658/"},"https:\/\/victoria.dev\/neofeed\/813\/":{title:"813",tags:[],content:"One challenge of software development is that I’m not productive unless I’m sitting at my desk with my hands on the keyboard. This means I don’t end up moving all day unless I get up to make tea.\nSo now when I wait for water to boil to make tea, I do air squats. 🏋🏻‍♀️🍵\n",url:"https://victoria.dev/neofeed/813/"},"https:\/\/victoria.dev\/neofeed\/847\/":{title:"847",tags:[],content:"You can rewrite history - Git history! Here’s how to remove a file:\ngit filter-branch --force --index-filter \\ \u0026#34;git rm --cached --ignore-unmatch FILE-PATH\u0026#34; \\ --prune-empty --tag-name-filter cat -- --all git push origin --force --all Use responsibly! 🖋\nhttps://help.github.com/en/github/authenticating-to-github/removing-sensitive-data-from-a-repository\n",url:"https://victoria.dev/neofeed/847/"},"https:\/\/victoria.dev\/neofeed\/1448\/":{title:"1448",tags:[],content:"Respawned my 2016 Pixel XL (which was prematurely cut from support and security updates) to use Gmail - and couldn’t download any apps from the Play Store.\nAfter investigation, disabling the Google App in Settings let me download apps again. What a sneaky spying 💩.\n",url:"https://victoria.dev/neofeed/1448/"},"https:\/\/victoria.dev\/neofeed\/745\/":{title:"745",tags:[],content:"It\u0026rsquo;s true that the Ergodox is very ergonomic! I no longer have to lean sideways when I\u0026rsquo;m eating in front of my computer. 🍴🍲🧂\n",url:"https://victoria.dev/neofeed/745/"},"https:\/\/victoria.dev\/neofeed\/728\/":{title:"728",tags:[],content:"I mostly work out in order to be not floppy at age 60 🏋🏻‍♀️\n",url:"https://victoria.dev/neofeed/728/"},"https:\/\/victoria.dev\/neofeed\/731\/":{title:"731",tags:[],content:"The if-all-your-friends-jumped-off-a-cliff question is really easy to answer if you don\u0026rsquo;t have any cliffs. ⛰️\n",url:"https://victoria.dev/neofeed/731/"},"https:\/\/victoria.dev\/neofeed\/720\/":{title:"720",tags:[],content:"I\u0026rsquo;m always amazed by the extent to which people can be convinced that something is normal, as long as no one bothers to ask, \u0026ldquo;Hey, is this normal?\u0026rdquo; 🤔\n",url:"https://victoria.dev/neofeed/720/"},"https:\/\/victoria.dev\/neofeed\/551\/":{title:"551",tags:[],content:"One does not simply push Action workflow files to create an Action\u0026hellip; apparently.\nIt helps if you put files in the expected folders.\n",url:"https://victoria.dev/neofeed/551/"},"https:\/\/victoria.dev\/neofeed\/205\/":{title:"205",tags:[],content:"This is happening.\n",url:"https://victoria.dev/neofeed/205/"},"https:\/\/victoria.dev\/neofeed\/36537\/":{title:"36537",tags:[],content:"NYE Checklist:\n Reflect Appreciate Tidy up Be kind  A very happy new year and new decade to you and yours! 💖🎉\n",url:"https://victoria.dev/neofeed/36537/"},"https:\/\/victoria.dev\/blog\/the-past-ten-years-or-how-to-get-better-at-anything\/":{title:"The past ten years, or, how to get better at anything",tags:["life","docs","coding"],content:"If you want to get better at anything:\n Solve your own problems, Write about it, Teach others.  1. Searching, a decade ago I was a young graduate with newly-minted freedoms, and I was about to fall in love. I had plenty of imagination, a couple handfuls of tenacity, and no sense of direction at all.\nFor much of my youth, when I encountered a problem, I just sort of bumped up against it. I tried using whatever was in my head from past experiences or my own imagination to find a solution. For some problems, like managing staff duties at work, my experience was sufficient guidance. For other, more complicated problems, it wasn\u0026rsquo;t.\nWhen you don\u0026rsquo;t have a wealth of experience to draw upon, relying on it is a poor strategy. Like many people at my age then, I thought I knew enough. Like many people at my age now, I recognize how insufficient \u0026ldquo;enough\u0026rdquo; can be. A lack of self-directed momentum meant being dragged in any direction life\u0026rsquo;s currents took me. When falling in love turned out to mean falling from a far greater height than I had anticipated, I tumbled on, complacent. When higher-ups at work handed me further responsibilities, I accepted them without considering if I wanted them at all. When, inevitably, life became more and more complicated, I encountered even more problems I didn\u0026rsquo;t know how to solve. I felt stuck.\nThough I was morbidly embarrassed about it at the time, I\u0026rsquo;m not shy to say it now. At one point, it had to be pointed out to me that I could search the Internet for the solution to any of my problems. Anything I wanted to solve - interactions with people at work, a floundering relationship, or the practicalities of filing taxes - I was lucky enough to have the greatest collection of human knowledge ever assembled at my disposal.\nInstead of bumbling along in the floatsam of my own trial and error, I started to take advantage of the collective experiences of all those who have been here before me. They weren\u0026rsquo;t always right, and I often found information only somewhat similar to my own experience. Still, it always got me moving in the right direction. Eventually, I started to steer.\nThere\u0026rsquo;s a learning curve, even when just searching for a problem. Distilling the jumble of confusion in your head to the right search terms is a learned skill. It helped me to understand how search engines like Google work:\n We use software known as web crawlers to discover publicly available webpages. Crawlers look at webpages and follow links on those pages, much like you would if you were browsing content on the web. They go from link to link and bring data about those webpages back to Google’s servers\u0026hellip;\nWhen crawlers find a webpage, our systems render the content of the page, just as a browser does. We take note of key signals — from keywords to website freshness — and we keep track of it all in the Search index.\n Sometimes, I find what I need by using the right keyword. Other times, I discover the keyword by searching for text that might surround it on the content of the page. For software development, I search for the weirdest word or combination of words attached to what I\u0026rsquo;m trying to learn. I rarely find whole solutions in my search results, but I always find direction for solving the problem myself.\nSolving my own problems, even just a few little ones at a time, gave me confidence and built momentum. I began to pursue the experiences I wanted, instead of waiting for experiences to happen to me.\n2. Updating the Internet, some years ago I\u0026rsquo;d solved myself out of a doomed relationship and stagnant job. I found myself, rather gleefully, country-hopping with just one backpack of possessions. I met, though I didn\u0026rsquo;t know it at the time, my future husband. I found a new sense of freedom, of having options, that I knew I never wanted to give up. I had to find a means to sustain myself by working remotely.\nWhen I first tried to make a living on the Internet, I felt like a right amateur. Sitting on the bed, hunched over my laptop, I started a crappy Wordpress blog with a modified theme that didn\u0026rsquo;t entirely work. I posted about how I tried and failed to start a dropshipping business. My site was terrible, and I knew it. My first forays into being a \u0026ldquo;real\u0026rdquo; developer were to solve my own problems: how to get my blog working, how to set up a custom domain, how to get and use a security certificate. I found some guidance in blogs and answers that others had written, but much of it was outdated, or not entirely correct. Still, it helped me.\nI can\u0026rsquo;t imagine a world in which people did nothing to pass on their knowledge to future generations. Our stories are all we have beyond instinct and determination.\nI stopped posting about dropshipping and started writing about the technical problems I was solving. I wrote about what I tried, and ultimately what worked. I started hearing from people who thanked me for explaining the solution they were looking for. Even in posts where all I\u0026rsquo;d done was link to the correct set of instructions on some other website, people thanked me for leading them to it. I still thought my website was terrible, but I realized I was doing something useful. The more problems I solved, the better I got at solving them, and the more I wrote about it in turn.\nOne day, someone offered me money for one of my solutions. To my great delight, they weren\u0026rsquo;t the last to do so.\nAs I built up my skills, I started taking on more challenging offers to solve problems. I discovered, as others have before me, that especially in software development, not every solution is out there waiting for you. The most frustrating part of working on an unsolved problem is that, at least to your knowledge, there\u0026rsquo;s no one about to tell you how to solve it. If you\u0026rsquo;re lucky, you\u0026rsquo;ve at least got a heading from someone\u0026rsquo;s cold trail in an old blog post. If you\u0026rsquo;re lucky and tenacious, you\u0026rsquo;ll find a working solution.\nDon\u0026rsquo;t leave it scribbled in the corner of a soon-forgotten notepad, never to ease the path of someone who comes along later. Update that old blog post by commenting on it, or sending a note to the author. Put your solution on the Internet, somewhere. Ideally, blog about it yourself in as much detail as you can recall. Some of the people who find your post might have the same problem, and might even be willing to pay you to solve it. And, if my own experience and some scattered stories hold true, one of the people to who\u0026rsquo;ll come along later, looking for that same solution, will be you.\n3. Paying it forwards, backwards, and investing; two years ago Already being familiar with how easy it is to stop steering and start drifting, I sought new ways to challenge myself and my skills. I wanted to do more than just sustain my lifestyle. I wanted to offer something to others; something that mattered.\nA strange thing started happening when I decided, deliberately, to write an in-depth technical blog about topics I was only beginning to become familiar with. I started to deeply understand some fundamental computer science topics - and trust me, that was strange enough - but odder than that was that others started to see me as a resource. People asked me questions because they thought I had the answers. I didn\u0026rsquo;t, at least, not always - but I knew enough now to not let that stop me. I went to find the answers, to test and understand them, and then I wrote about them to teach those who had asked. I hardly noticed, along the way, that I was learning too.\nWhen someone\u0026rsquo;s outdated blog post leads you to an eventual solution, you can pay them back by posting an update, or blogging about it yourself. When you solve an unsolved problem, you pay it forward by recording that solution for the next person who comes along (sometimes you). In either case, by writing about it - honestly, and with your best effort to be thorough and correct - you end up investing in yourself.\nExplaining topics you\u0026rsquo;re interested in to other people helps you find the missing pieces in your own knowledge. It helps you fill those gaps with learning, and integrate the things you learn into a new, greater understanding. Teaching something to others helps you become better at it yourself. Getting better at something - anything - means you have more to offer.\nThe past decade, and the next decade It\u0026rsquo;s the end of a decade. I went from an aimless drift through life to being captain of my ship. I bettered my environment, learned new skills, made myself a resource, and became a wife to my best friend. I\u0026rsquo;m pretty happy with all of it.\nIt\u0026rsquo;s the end of 2019. Despite a whole lot of life happening just this year, I\u0026rsquo;ve written one article on this blog for each week since I started in July. That\u0026rsquo;s 23 articles for 23 weeks, plus one Christmas bonus. I hear from people almost every day who tell me that an article I wrote was helpful to them, and it makes me happy and proud to think that I\u0026rsquo;ve been doing something that matters. The first week of January will make this blog two years old.\nThe past several months have seen me change tack, slightly. I\u0026rsquo;ve become very interested in cybersecurity, and have been lending my skills to the Open Web Application Security Project. I\u0026rsquo;m now an author and maintainer of the Web Security Testing Guide, version 5. I\u0026rsquo;m pretty happy with that, too.\nNext year, I\u0026rsquo;ll be posting a little less, though writing even more, as I pursue an old dream of publishing a book, as well as develop my new cybersecurity interests. I aim to get better at quite a few things. Thankfully, I know just how to do it - and now, so do you:\n Solve your own problems, Write about it, Teach others.  Have a very happy new decade, dear reader.\n",url:"https://victoria.dev/blog/the-past-ten-years-or-how-to-get-better-at-anything/"},"https:\/\/victoria.dev\/neofeed\/36025\/":{title:"36025",tags:[],content:"If Santa brought you a new phone for Christmas, don\u0026rsquo;t forget to change over your authentication app codes before wiping the old one! 🔐 Here\u0026rsquo;s instructions for Google Authenticator: https://support.google.com/accounts/troubleshooter/4430955?hl=en#ts=4430956\nNot using MFA yet? There\u0026rsquo;s a new year resolution for you! 🌟\n",url:"https://victoria.dev/neofeed/36025/"},"https:\/\/victoria.dev\/blog\/three-healthy-cybersecurity-habits\/":{title:"Three healthy cybersecurity habits",tags:["cybersecurity","privacy","life"],content:"In a similar fashion to everyone getting the flu now and again, the risk of catching a cyberattack is a common one. Both a sophisticated social engineering attack or grammatically-lacking email phishing scam can cause real damage. No one who communicates over the Internet is immune.\nLike proper hand washing and getting a flu shot, good habits can lower your risk of inadvertently allowing cybergerms to spread. Since the new year is an inspiring time for beginning new habits, I offer a few suggestions for ways to help protect yourself and those around you.\n1. Get a follow-up Recognizing a delivery method for cyberattack is getting more difficult. Messages with malicious links do not always come from strangers. They may appear to be routine communications, or seem to originate from someone you know or work with. Attacks use subtle but deeply-engrained cognitive biases to override your common sense. Your natural response ensures you click.\nThankfully, there\u0026rsquo;s a simple low-tech habit you can use to deter these attacks: before you act, follow-up.\nYou may get an email from a friend that needs help, or from your boss who\u0026rsquo;s about to get on a plane. It could be as enticing and mysterious as a direct message from an acquaintance who sends a link asking, \u0026ldquo;Lol. Is this you?\u0026rdquo; It takes presence of mind to override the panic these attacks prey on, but the deterrent itself is quick and straightforward. Send a text message, pick up the phone and call, or walk down the hall, and ask, \u0026ldquo;Did you send me this?\u0026rdquo;\nIf the message is genuine, there\u0026rsquo;s no harm in a few extra minutes to double check. If it\u0026rsquo;s not, you\u0026rsquo;ll immediately alert the originating party that they may be compromised, and you may have deterred a cyberattack!\n2. Use, and encourage others to use, end-to-end encrypted messaging When individuals in a neighborhood get the flu shot, others in that neighborhood are safer for it. Encryption is similarly beneficial. Encourage your friends, coworkers, and Aunt Matilda to switch to an app like Signal. By doing so, you\u0026rsquo;ll reduce everyone\u0026rsquo;s exposure to more exploitable messaging systems.\nThis doesn\u0026rsquo;t mean that you must stop using other methods of communication entirely. Instead, think of it as a hierarchy. Use Signal for important messages that should be trusted, like requests for money or making travel arrangements. Use all other methods of messaging, like SMS or social sites, only for \u0026ldquo;unimportant\u0026rdquo; communications. Now, if requests or links that seem important come to you through your unimportant methods, you\u0026rsquo;ll be all the more likely to second-guess them.\n3. Don\u0026rsquo;t put that dirty USB plug into your *** You wouldn\u0026rsquo;t brush your teeth with a toothbrush you found on the sidewalk. Why would you plug in a USB device if you don\u0026rsquo;t know where it\u0026rsquo;s been?! While we might ascribe putting a random found USB drive in your computer to a clever exploitation of natural human curiosity, we\u0026rsquo;re no sooner likely to suspect using a public phone-charging station or a USB cable we bought ourselves. Even seemingly-innocuous USB peripherals or rechargeable devices can be a risk.\nUnlike email and some file-sharing services that scan and filter files before they reach your computer, plugging in via USB is as direct and unprotected as connection gets. Once this connection is made, the user doesn\u0026rsquo;t need to do anything else for a whole host of bad things to happen. Through USB connections, problems like malware and ransomware can easily infect your computer or phone.\nThere\u0026rsquo;s no need to swear off the convenience of USB connectivity, or to avoid these devices altogether. Instead of engaging in questionable USB behavior, don\u0026rsquo;t cheap out on USB devices and cables. If it\u0026rsquo;s going to get plugged into your computer, ensure you\u0026rsquo;re being extra cautious. Buy it from the manufacturer (like the Apple Store) or from a reputable company or reseller with supply chain control. When juicing up USB-rechargeables, don\u0026rsquo;t plug them into your computer. Use a wall charger with a USB port instead.\nPractice healthy cybersecurity habits Keeping your devices healthy and happy is a matter of practicing good habits. Like battling the flu, good habits can help protect yourself and those around you. Incorporate some conscientious cybersecurity practices in your new year resolutions - or start them right away.\nHave a safe and happy holiday!\n",url:"https://victoria.dev/blog/three-healthy-cybersecurity-habits/"},"https:\/\/victoria.dev\/blog\/concurrency-parallelism-and-the-many-threads-of-santa-claus\/":{title:"Concurrency, parallelism, and the many threads of Santa Claus 🎅",tags:["computing"],content:"Consider the following: Santa brings toys to all the good girls and boys.\nThere are 7,713,468,100 people in the world in 2019, around 26.3% of which are under 15 years old. This works out to 2,028,642,110 children (persons under 15 years of age) in the world this year.\nSanta doesn\u0026rsquo;t seem to visit children of every religion, so we\u0026rsquo;ll generalize and only include Christians and non-religious folks. Collectively that makes up approximately 44.72% of the population. If we assume that all kids take after their parents, then 907,208,751.6 children would appear to be Santa-eligible.\nWhat percentage of those children are good? It\u0026rsquo;s impossible to know; however, we can work on a few assumptions. One is that Santa Claus functions more on optimism than economics and would likely have prepared for the possibility that every child is a good child in any given year. Thus, he would be prepared to give a toy to every child. Let\u0026rsquo;s assume it\u0026rsquo;s been a great year and that all 907,208,751.6 children are getting toys.\nThat\u0026rsquo;s a lot of presents, and, as we know, they\u0026rsquo;re all made by Santa\u0026rsquo;s elves at his North China Pole workshop. Given that there are 365 days in a year and one of them is Christmas, let\u0026rsquo;s assume that Santa\u0026rsquo;s elves collectively have 364 days to create and gift wrap 907,208,752 (rounded up) presents. That works out to 2,492,331.74 presents per day.\nAlmost two-and-a-half million presents per day is a heavy workload for any workshop. Let\u0026rsquo;s look at two paradigms that Santa might employ to hit this goal: concurrency, and parallelism.\nA sequential process Suppose that Santa\u0026rsquo;s workshop is staffed by exactly one, very hard working, very tired elf. The production of one present involves four steps:\n Cutting wood Assembly and glueing Painting Gift-wrapping  With a single elf, only one step for one present can be happening at any instance in time. If the elf were to produce one present at a time from beginning to end, that process would be executed sequentially. It\u0026rsquo;s not the most efficient method for producing two-and-a-half million presents per day; for instance, the elf would have to wait around doing nothing while the glue on the present was drying before moving on to the next step.\nConcurrency In order to be more efficient, the elf works on all presents concurrently.\nInstead of completing one present at a time, the elf first cuts all the wood for all the toys, one by one. When everything is cut, the elf assembles and glues the toys together, one after the other. This concurrent processing means that the glue from the first toy has time to dry (without needing more attention from the elf) while the remaining toys are glued together. The same goes for painting, one toy at a time, and finally wrapping.\nSince one elf can only do one task at a time, a single elf is using the day as efficiently as possible by concurrently producing presents.\nParallelism Hopefully, Santa\u0026rsquo;s workshop has more than just one elf. With more elves, more toys can be built simultaneously over the course of a day. This simultaneous work means that the presents are being produced in parallel. Parallel processing carried out by multiple elves means more work happens at the same time.\nElves working in parallel can also employ concurrency. One elf can still tackle only one task at a time, so it\u0026rsquo;s most efficient to have multiple elves concurrently producing presents.\nOf course, if Santa\u0026rsquo;s workshop has, say, two-and-a-half million elves, each elf would only need to finish a maximum of one present per day. In this case, working sequentially doesn\u0026rsquo;t detract from the workshop\u0026rsquo;s efficiency. There would still be 7,668.26 elves left over to fetch coffee and lunch.\nSanta Claus, and threading After all the elves' hard work is done, it\u0026rsquo;s up to Santa Claus to deliver the presents \u0026ndash; all 907,208,752 of them.\nSanta doesn\u0026rsquo;t need to make a visit to every kid; just to the one household tree. So how many trees does Santa need to visit? Again with broad generalization, we\u0026rsquo;ll say that the average number of children per household worldwide is 2.45, based on the year\u0026rsquo;s predicted fertility rates. That makes 370,289,286.4 houses to visit. Let\u0026rsquo;s round that up to 370,289,287.\nHow long does Santa have? The lore says one night, which means one earthly rotation, and thus 24 hours. NORAD confirms.\nThis means Santa must visit 370,289,287 households in 24 hours (86,400 seconds), at a rate of 4,285.75 households per second, nevermind the time it takes to put presents under the tree and grab a cookie.\nClearly, Santa doesn\u0026rsquo;t exist in our dimension. This is especially likely given that despite being chubby and plump, he fits down a chimney (with a lit fire, while remaining unhurt) carrying a sack of toys containing presents for all the household\u0026rsquo;s children. We haven\u0026rsquo;t even considered the fact that his sleigh carries enough toys for every believing boy and girl around the world, and flies.\nDoes Santa exist outside our rules of physics? How could one entity manage to travel around the world, delivering packages, in under 24 hours at a rate of 4,285.75 households per second, and still have time for milk and cookies and kissing mommy?\nOne thing is certain: Santa uses the Internet. No other technology has yet enabled packages to travel quite so far and quite so quickly. Even so, attempting to reach upwards of four thousand households per second is no small task, even with even the best gigabit Internet hookup the North Pole has to offer. How might Santa increase his efficiency?\nThere\u0026rsquo;s clearly only one logical conclusion to this mystery: Santa Claus is a multithreaded process.\nA single thread Let\u0026rsquo;s work outward. Think of a thread as one particular task, or the most granular sequence of instructions that Santa might execute. One thread might execute the task, put present under tree. A thread is a component of a process, in this case, Santa\u0026rsquo;s process of delivering presents.\nIf Santa Claus is single-threaded, he, as a process, would only be able to accomplish one task at a time. Since he\u0026rsquo;s old and a bit forgetful, he probably has a set of instructions for delivering presents, as well as a schedule to abide by. These two things guide Santa\u0026rsquo;s thread until his process is complete.\nSingle-threaded Santa Claus might work something like this:\n Land sleigh at Timmy\u0026rsquo;s house Get Timmy\u0026rsquo;s present from sleigh Enter house via chimney Locate Christmas tree Place Timmy\u0026rsquo;s present under Christmas tree Exit house via chimney Take off in sleigh  Rinse and repeat\u0026hellip; another 370,289,286 times.\nMultithreading Multithreaded Santa Claus, by contrast, is the Doctor Manhattan of the North Pole. There\u0026rsquo;s still only one Santa Claus in the world; however, he has the amazing ability to multiply his consciousness and accomplish multiple instruction sets of tasks simultaneously. These additional task workers, or worker threads, are created and controlled by the main process of Santa delivering presents.\nEach worker thread acts independently to complete its instructions. Since they all belong to Santa\u0026rsquo;s consciousness, they share Santa\u0026rsquo;s memory and know everything that Santa knows, including what planet they\u0026rsquo;re running around on, and where to get the presents from.\nWith this shared knowledge, each thread is able to execute its set of instructions in parallel with the other threads. This multithreaded parallelism makes the one and only Santa Claus as efficient as possible.\nIf an average present delivery run takes an hour, Santa need only spawn 4,286 worker threads. With each making one delivery trip per hour, Santa will have completed all 370,289,287 trips by the end of the night.\nOf course, in theory, Santa could even spawn 370,289,287 worker threads, each flying to one household to deliver presents for all the children in it! That would make Santa\u0026rsquo;s process extremely efficient, and also explain how he manages to consume all those milk-dunked cookies without getting full. 🥛🍪🍪🍪\nAn efficient and merry multithreaded Christmas Thanks to modern computing, we now finally understand how Santa Claus manages the seemingly-impossible task of delivering toys to good girls and boys the world-over. From my family to yours, I hope you have a wonderful Christmas. Don\u0026rsquo;t forget to hang up your stockings on the router shelf.\nOf course, none of this explains how reindeer manage to fly.\n",url:"https://victoria.dev/blog/concurrency-parallelism-and-the-many-threads-of-santa-claus/"},"https:\/\/victoria.dev\/neofeed\/35405\/":{title:"35405",tags:[],content:"How would you visit 264,492,348 Internet addresses within 24 hours?\nAsking for a friend.\n",url:"https://victoria.dev/neofeed/35405/"},"https:\/\/victoria.dev\/blog\/word-bugs-in-software-documentation-and-how-to-fix-them\/":{title:"Word bugs in software documentation and how to fix them",tags:["docs","open-source"],content:"I\u0026rsquo;ve been an editor longer than I\u0026rsquo;ve been a developer, so this topic for me is a real root issue. 🥁 When I see a great project with poorly-written docs, it hits close to /home. Okay, okay, I\u0026rsquo;m done.\nI help the Open Web Application Security Project (OWASP) with their Web Security Testing Guide (WSTG). I was recently tasked with writing a style guide and article template that show how to write technical instruction for testing software applications.\nI thought parts of the guide would benefit more people than just OWASP\u0026rsquo;s contributors, so I\u0026rsquo;m sharing some here.\nMany of the projects I participate in are open source. This is a wonderful way for people to share solutions and to build on each others' ideas. Unfortunately, it\u0026rsquo;s also a great way for misused and non-existent words to catch on. Here\u0026rsquo;s an excerpt of the guide with some mistakes I\u0026rsquo;ve noticed and how you can fix them in your technical documents.\n Use Correct Words The following are frequently misused words and how to correct them.\nand/or While sometimes used in legal documents, and/or leads to ambiguity and confusion in technical writing. Instead, use or, which in the English language includes and. For example:\n Bad: \u0026ldquo;The code will output an error number and/or description.\u0026rdquo; Good: \u0026ldquo;The code will output an error number or description.\u0026rdquo;\n The latter sentence does not exclude the possibility of having both an error number and description.\nIf you need to specify all possible outcomes, use a list:\n \u0026ldquo;The code will output an error number, or a description, or both.\u0026rdquo;\n frontend, backend While it\u0026rsquo;s true that the English language evolves over time, these are not yet words.\nWhen referring to nouns, use front end and back end. For example:\n Security is equally important on the front end as it is on the back end.\n As a descriptive adverb, use the hyphenated front-end and back-end.\n Both front-end developers and back-end developers are responsible for application security.\n whitebox, blackbox, greybox These are not words.\nAs nouns, use white box, black box, and grey box. These nouns rarely appear in connection with cybersecurity.\n My cat enjoys jumping into that grey box.\n As adverbs, use the hyphenated white-box, black-box, and grey-box. Do not use capitalization unless the words are in a title.\n While white-box testing involves knowledge of source code, black-box testing does not. A grey-box test is somewhere in-between.\n ie, eg These are letters.\nThe abbreviation i.e. refers to the Latin id est, which means \u0026ldquo;in other words.\u0026rdquo; The abbreviation e.g. is for exempli gratia, translating to \u0026ldquo;for example.\u0026rdquo; To use these in a sentence:\n Write using proper English, i.e. correct spelling and grammar. Use common words over uncommon ones, e.g. \u0026ldquo;learn\u0026rdquo; instead of \u0026ldquo;glean.\u0026rdquo;\n etc These are also letters.\nThe Latin phrase et cetera translates to \u0026ldquo;and the rest.\u0026rdquo; It is abbreviated etc. and typically placed at the end of a list that seems redundant to complete:\n WSTG authors like rainbow colors, such as red, yellow, green, etc.\n In technical writing, the use of etc. is problematic. It assumes the reader knows what you\u0026rsquo;re talking about, and they may not. Violet is one of the colors of the rainbow, but the example above does not explicitly tell you if violet is a color that WSTG authors like.\nIt is better to be explicit and thorough than to make assumptions of the reader. Only use etc. to avoid completing a list that was given in full earlier in the document.\n\u0026hellip; (ellipsis) The ellipsis punctuation mark can indicate that words have been left out of a quote:\n Linus Torvalds once said, \u0026ldquo;Once you realize that documentation should be laughed at\u0026hellip; THEN, and only then, have you reached the level where you can safely read it and try to use it to actually implement a driver.\u0026rdquo;\n As long as the omission does not change the meaning of the quote, this is acceptable usage of ellipsis in the WSTG.\nAll other uses of ellipsis, such as to indicate an unfinished thought, are not.\nex While this is a word, it is likely not the word you are looking for. The word ex has particular meaning in the fields of finance and commerce, and may refer to a person if you are discussing your past relationships. None of these topics should appear in the WSTG.\nThe abbreviation ex. may be used to mean \u0026ldquo;example\u0026rdquo; by lazy writers. Please don\u0026rsquo;t be lazy, and write example instead.\n Go forth and write docs If these reminders are helpful, please share them freely and use them when writing your own READMEs and documentation! If there\u0026rsquo;s some I\u0026rsquo;ve missed, I\u0026rsquo;d love to know.\nAnd if you\u0026rsquo;re here for the comments\u0026hellip;\nThere are none on my blog. You can still @ me.\nIf you\u0026rsquo;d like to help contribute to the OWASP WSTG, please read the contribution guide. See the full style guide here.\n",url:"https://victoria.dev/blog/word-bugs-in-software-documentation-and-how-to-fix-them/"},"https:\/\/victoria.dev\/neofeed\/35143\/":{title:"35143",tags:[],content:"When you accidentally press Ctrl+Z instead of Ctrl+C in a terminal, type fg to resume the suspended program.\n",url:"https://victoria.dev/neofeed/35143/"},"https:\/\/victoria.dev\/neofeed\/35127\/":{title:"35127",tags:[],content:"I wonder how eggplants feel about being dipped in babaganoush\n",url:"https://victoria.dev/neofeed/35127/"},"https:\/\/victoria.dev\/neofeed\/34542\/":{title:"34542",tags:[],content:"Figure out what you want, then figure out how to offer it up in service to other people.\n",url:"https://victoria.dev/neofeed/34542/"},"https:\/\/victoria.dev\/blog\/secure-web-forms-for-the-front-end-developer\/":{title:"Secure web forms for the front-end developer",tags:["cybersecurity","coding","tech-team"],content:"While cybersecurity is often thought of in terms of databases and architecture, much of a strong security posture relies on elements in the domain of the front-end developer. For certain potentially devastating vulnerabilities like SQL injection and Cross-Site Scripting (XSS), a well-considered user interface is the first line of defense.\nHere are a few areas of focus for front-end developers who want to help fight the good fight.\nControl user input A whole whack of crazy things can happen when developers build a form that fails to control user input. To combat vulnerabilities like injection, it\u0026rsquo;s important to validate or sanitize user input.\nInput can be validated by constraining it to known values, such as by using semantic input types or validation-related attributes in forms. Frameworks like Django also help by providing field types for this purpose. Sanitizing data can be done by removing or replacing contextually-dangerous characters, such as by using a whitelist or escaping the input data.\nWhile it may not be intuitive, even data that a user submits to their own area on a site should be validated. One of the fastest viruses to proliferate was the Samy worm on MySpace (yes, I\u0026rsquo;m old), thanks to code that Samy Kamkar was able to inject into his own profile page. Don\u0026rsquo;t directly return any input to your site without thorough validation or santization.\nFor some further guidance on battling injection attacks, see the OWASP Injection Prevention Cheat Sheet.\nBeware hidden fields Adding type=\u0026quot;hidden\u0026quot; is an enticingly convenient way to hide sensitive data in pages and forms, but unfortunately not an effective one. With tools like ZapProxy and even inspection tools in plain ol' web browsers, users can easily click to reveal tasty bits of invisible information. Hiding checkboxes can be a neat hack for creating CSS-only switches, but hidden fields do little to contribute to security.\nCarefully consider autofill fields When a user chooses to give you their Personally Identifiable Information (PII), it should be a conscious choice. Autofill form fields can be convenient - for both users and attackers. Exploits using hidden fields can harvest PII previously captured by an autocomplete field.\nMany users aren\u0026rsquo;t even aware what information their browser\u0026rsquo;s autofill has stored up. Use these fields sparingly, and disable autofilled forms for particularly sensitive data.\nIt\u0026rsquo;s important to also weigh your risk profile against its trade-offs. If your project must be WCAG compliant, disabling autocomplete can break your input for different modalities. For more, see 1.3.5: Identify Input Purpose in WCAG 2.1.\nKeep errors generic While it may seem helpful to let users know whether a piece of data exists, it\u0026rsquo;s also very helpful to attackers. When dealing with accounts, emails, and PII, it\u0026rsquo;s most secure to err (🥁) on the side of less. Instead of returning \u0026ldquo;Your password for this account is incorrect,\u0026rdquo; try the more ambiguous feedback \u0026ldquo;Incorrect login information,\u0026rdquo; and avoid revealing whether the username or email is in the system.\nIn order to be more helpful, provide a prominent way to contact a human in case an error should arise. Avoid revealing information that isn\u0026rsquo;t necessary. If nothing else, for heaven\u0026rsquo;s sake, don\u0026rsquo;t suggest data that\u0026rsquo;s a close match to the user input.\nBe a bad guy When considering security, it\u0026rsquo;s helpful to take a step back, observe the information on display, and ask yourself how a malicious attacker would be able to utilize it. Play devil\u0026rsquo;s advocate. If a bad guy saw this page, what new information would they gain? Does the view show any PII?\nAsk yourself if everything on the page is actually necessary for a genuine user. If not, redact or remove it. Less is safer.\nSecurity starts at the front door These days, there\u0026rsquo;s a lot more overlap between coding on the front end and the back end. To create a well-rounded and secure application, it helps to have a general understanding of ways attackers can get their foot in the front door.\n",url:"https://victoria.dev/blog/secure-web-forms-for-the-front-end-developer/"},"https:\/\/victoria.dev\/neofeed\/34041\/":{title:"34041",tags:[],content:"I explain how I talk to computers so some people can also talk to computers and so some other people don\u0026rsquo;t have to.\n",url:"https://victoria.dev/neofeed/34041/"},"https:\/\/victoria.dev\/blog\/the-surprisingly-difficult-task-of-printing-newlines-in-a-terminal\/":{title:"The surprisingly difficult task of printing newlines in a terminal",tags:["terminal","cybersecurity"],content:"Surprisingly, getting computers to give humans readable output is no easy feat. With the introduction of standard streams and specifically standard output, programs gained a way to talk to each other using plain text streams; humanizing and displaying stdout is another matter. Technology throughout the computing age has tried to solve this problem, from the use of ASCII characters in video computer displays to modern shell commands like echo and printf.\nThese advancements have not been seamless. The job of printing output to a terminal is fraught with quirks for programmers to navigate, as exemplified by the deceptively nontrivial task of expanding an escape sequence to print newlines. The expansion of the placeholder \\n can be accomplished in a multitude of ways, each with its own unique history and complications.\nUsing echo From its appearance in Multics to its modern-day Unix-like system ubiquity, echo remains a familiar tool for getting your terminal to say \u0026ldquo;Hello world!\u0026rdquo; Unfortunately, inconsistent implementations across operating systems make its usage tricky. Where echo on some systems will automatically expand escape sequences, others require the -e option to do the same:\necho \u0026#34;the study of European nerves is \\neurology\u0026#34; # the study of European nerves is \\neurology echo -e \u0026#34;the study of European nerves is \\neurology\u0026#34; # the study of European nerves is # eurology Because of these inconsistencies in implementations, echo is considered non-portable. Additionally, its usage in conjunction with user input is relatively easy to corrupt through shell injection attack using command substitutions.\nIn modern systems, it is retained only to provide compatibility with the many programs that still use it. The POSIX specification recommends the use of printf in new programs.\nUsing printf Since 4th Edition Unix, the portable printf command has essentially been the new and better echo. It allows you to use format specifiers to humanize input. To interpret backslash escape sequences, use %b. The character sequence \\n ensures the output ends with a newline:\nprintf \u0026#34;%b\\n\u0026#34; \u0026#34;Many females in Oble are \\noblewomen\u0026#34; # Many females in Oble are # oblewomen Though printf has further options that make it a far more powerful replacement of echo, this utility is not foolproof and can be vulnerable to an uncontrolled format string attack. It\u0026rsquo;s important for programmers to ensure they carefully handle user input.\nPutting newlines in variables In an effort to improve portability amongst compilers, the ANSI C Standard was established in 1983. With ANSI-C quoting using $'...', escape sequences are replaced in output according to the standard.\nThis allows us to store strings with newlines in variables that are printed with the newlines interpreted. You can do this by setting the variable, then calling it with printf using $:\npuns=$\u0026#39;\\number\\narrow\\nether\\nice\u0026#39; printf \u0026#34;%b\\n\u0026#34; \u0026#34;These words started with n but don\u0026#39;t make $puns\u0026#34; # These words started with n but don\u0026#39;t make # umber # arrow # ether # ice The expanded variable is single-quoted, which is passed literally to printf. As always, it is important to properly handle the input.\nBonus round: shell parameter expansion In my article explaining Bash and braces, I covered the magic of shell parameter expansion. We can use one expansion, ${parameter@operator}, to interpret escape sequences, too. We use printf\u0026rsquo;s %s specifier to print as a string, and the E operator will properly expand the escape sequences in our variable:\nprintf \u0026#34;%s\\n\u0026#34; ${puns@E} # umber # arrow # ether # ice The ongoing challenge of humanizing output String interpolation continues to be a chewy problem for programmers. Besides getting languages and shells to agree on what certain placeholders mean, properly using the correct escape sequences requires an eye for detail.\nPoor string interpolation can lead to silly-looking output, as well as introduce security vulnerabilities, such as from injection attacks. Until the next evolution of the terminal has us talking in emojis, we\u0026rsquo;d best pay attention when printing output for humans.\n",url:"https://victoria.dev/blog/the-surprisingly-difficult-task-of-printing-newlines-in-a-terminal/"},"https:\/\/victoria.dev\/blog\/the-care-and-feeding-of-an-iot-device\/":{title:"The care and feeding of an IoT device",tags:["cybersecurity","privacy","life"],content:"Giving someone a puppy for Christmas might work really well in a movie, but in real life often comes hitched to a multitude of responsibilities that the giftee may not be fully prepared to take on. The same is true for Internet of Things (IoT) devices, including Amazon\u0026rsquo;s Alexa-enabled devices, Google Home, and other Internet-connected appliances like cameras, lightbulbs, and toasters. Yes, they have those now.\nLike puppies, IoT devices are still young. Many contain known vulnerabilities that remote attackers can use to gain access to device owners' networks. These attacks are sometimes as laughably simple as using a default username and password that the device owner cannot change.\nDoes all this mean you shouldn\u0026rsquo;t give Grandma Mabel a new app-enabled coffee maker or Ring doorbell for Christmas? Probably, although not necessarily. Like puppies, properly-maintained IoT devices are capable of warming your heart without causing too much havoc; but they take a lot of work to care for. Here are a few responsibilities to keep in mind for the care and feeding of an IoT device.\nImmature security Many manufacturers of IoT devices have not made security a priority. There aren\u0026rsquo;t yet any enforced security requirements for this industry, which leaves the protection of your device and the network it\u0026rsquo;s connected to in the hands of the manufacturer.\nIt\u0026rsquo;s not just obscure no-name toasters, either; malicious third-party apps have snuck onto Amazon\u0026rsquo;s and Google\u0026rsquo;s more reputable devices and enabled attackers to eavesdrop on unsuspecting owners.\nUntil security regulations are put in place and enforced, it\u0026rsquo;s buyer beware for both devices and third-party applications. To the extent possible, potential owners must do ample research to weed out vulnerable devices and untrustworthy apps.\nProtecting your network If you think hackers aren\u0026rsquo;t likely to find your device in the vast expanse of the Internet, you might be wrong. These days, obscurity doesn\u0026rsquo;t provide security. It\u0026rsquo;s no longer left up to a potential attacker\u0026rsquo;s fallible human eyes to find your insecure front door camera in a cacophony of wireless traffic; IoT search engines like Shodan will do that for them. Thankfully, these search engines are also used for good, enabling white hat hackers and penetration testers to find and fix insecure devices.\nJust like locking your own front door, IoT owners are responsible for locking down access to their devices. This may mean searching through device settings to make sure default credentials are changed, or checking to make sure that a device used on your private home network doesn\u0026rsquo;t by default have public Internet access.\nWhere the options are available, HTTPS and multifactor authentication should be enabled. The use of a VPN can also keep your devices from being found.\nKeeping them patched Unlike puppies, many IoT devices are \u0026ldquo;headless\u0026rdquo; and have no inherent way of interfacing with a human. An app-controlled lightbulb, for example, may be all but useless without the software that makes it shine. As convenient as it may be to have your 1500K mood lighting come on automatically at dusk, it also means automatically ceding control of the device to its software developers.\nWhen vulnerabilities in your phone\u0026rsquo;s operating system are discovered and patched, it\u0026rsquo;s likely that automatic updates are pushed and installed overnight, possibly without you even knowing. Your IoT device, on the other hand, may have no such support. In those cases, it\u0026rsquo;s completely up to the user to discover that an update is needed, find and download the patch, then correctly update their device. Even for owners with some technical expertise, this process takes significant effort. Many device owners aren\u0026rsquo;t even aware that their software is dangerously outdated.\nIn practical terms, this means that users without the time, knowledge, or willingness to keep their devices updated should reconsider owning them. Alternatively, some research can help prospective owners choose devices that receive automatic push updates from their (hopefully responsible) manufacturers over WiFi.\nBeing responsible Raising a healthy and happy IoT device is no small task, especially for potential owners with little time or willingness to put in the required effort. With the proper attention and maintenance, your Internet-connected appliance can bring joy and convenience to your life; but without, it introduces a potential security risk and a whole lot of trouble.\nBefore getting or giving IoT, be sure the potential owner is up to the task of caring for it.\nYou can learn more about basic cybersecurity for IoT (as a user or maker) by reading NIST\u0026rsquo;s draft guidelines publication.\n",url:"https://victoria.dev/blog/the-care-and-feeding-of-an-iot-device/"},"https:\/\/victoria.dev\/neofeed\/33040\/":{title:"33040",tags:[],content:"Patience is usually rewarded. Inaction never is.\n",url:"https://victoria.dev/neofeed/33040/"},"https:\/\/victoria.dev\/neofeed\/32439\/":{title:"32439",tags:[],content:"Break things, then move fast.\n",url:"https://victoria.dev/neofeed/32439/"},"https:\/\/victoria.dev\/blog\/bash-and-shell-expansions-lazy-list-making\/":{title:"Bash and shell expansions: lazy list-making",tags:["terminal","linux"],content:"It\u0026rsquo;s that time of year again! When stores start putting up colourful sparkly lit-up plastic bits, we all begin to feel a little festive, and by festive I mean let\u0026rsquo;s go shopping. Specifically, holiday gift shopping! (Gifts for yourself are still gifts, technically.)\nJust so this doesn\u0026rsquo;t all go completely madcap, you ought to make some gift lists. Bash can help.\nBrace expansion These are not braces: ()\nNeither are these: []\nThese are braces: {}\nBraces tell Bash to do something with the arbitrary string or strings it finds between them. Multiple strings are comma-separated: {a,b,c}. You can also add an optional preamble and postscript to be attached to each expanded result. Mostly, this can save some typing, such as with common file paths and extensions.\nLet\u0026rsquo;s make some lists for each person we want to give stuff to. The following commands are equivalent:\ntouch /home/me/gift-lists/Amy.txt /home/me/gift-lists/Bryan.txt /home/me/gift-lists/Charlie.txt touch /home/me/gift-lists/{Amy,Bryan,Charlie}.txt tree gift-lists /home/me/gift-lists ├── Amy.txt ├── Bryan.txt └── Charlie.txt Oh darn, \u0026ldquo;Bryan\u0026rdquo; spells his name with an \u0026ldquo;i.\u0026rdquo; I can fix that.\nmv /home/me/gift-lists/{Bryan,Brian}.txt renamed \u0026#39;/home/me/gift-lists/Bryan.txt\u0026#39; -\u0026gt; \u0026#39;/home/me/gift-lists/Brian.txt\u0026#39; Shell parameter expansions Shell parameter expansion allows us to make all sorts of changes to parameters enclosed in braces, like manipulate and substitute text.\nThere are a few stocking stuffers that all our giftees deserve. Let\u0026rsquo;s make that a variable:\nSTUFF=$\u0026#39;socks\\nlump of coal\\nwhite chocolate\u0026#39; echo \u0026#34;$STUFF\u0026#34; socks lump of coal white chocolate Now to add these items to each of our lists with some help from the tee command to get echo and expansions to play nice.\necho \u0026#34;$STUFF\u0026#34; | tee {Amy,Brian,Charlie}.txt cat {Amy,Brian,Charlie}.txt socks lump of coal white chocolate socks lump of coal white chocolate socks lump of coal white chocolate Pattern match substitution On second thought, maybe the lump of coal isn\u0026rsquo;t such a nice gift. You can replace it with something better using a pattern match substitution in the form of ${parameter/pattern/string}:\necho \u0026#34;${STUFF/lump of coal/candy cane}\u0026#34; | tee {Amy,Brian,Charlie}.txt cat {Amy,Brian,Charlie}.txt socks candy cane white chocolate socks candy cane white chocolate socks candy cane white chocolate This replaces the first instance of \u0026ldquo;lump of coal\u0026rdquo; with \u0026ldquo;candy cane.\u0026rdquo; To replace all instances (if there were multiple), use ${parameter//pattern/string}. This doesn\u0026rsquo;t change our $STUFF variable, so we can still reuse the original list for someone naughty later.\nSubstrings While we\u0026rsquo;re improving things, our giftees may not all like white chocolate. We\u0026rsquo;d better add some regular chocolate to our lists just in case. Since I\u0026rsquo;m super lazy, I\u0026rsquo;m just going to hit the up arrow and modify a previous Bash command. Luckily, the last word in the $STUFF variable is \u0026ldquo;chocolate,\u0026rdquo; which is nine characters long, so I\u0026rsquo;ll tell Bash to keep just that part using ${parameter:offset}. I\u0026rsquo;ll use tee\u0026rsquo;s -a flag to append to my existing lists:\necho \u0026#34;${STUFF: -9}\u0026#34; | tee -a {Amy,Brian,Charlie}.txt cat {Amy,Brian,Charlie}.txt socks candy cane white chocolate chocolate socks candy cane white chocolate chocolate socks candy cane white chocolate chocolate You can also:\n   Do this With this     Get substring from n characters onwards ${parameter:n}   Get substring for x characters starting at n ${parameter:n:x}    There! Now our base lists are finished. Let\u0026rsquo;s have some eggnog.\nTesting variables You know, it may be the eggnog, but I think I started a list for Amy yesterday and stored it in a variable that I might have called amy. Let\u0026rsquo;s see if I did. I\u0026rsquo;ll use the ${parameter:?word} expansion. It\u0026rsquo;ll write word to standard error and exit if there\u0026rsquo;s no amy parameter.\necho \u0026#34;${amy:?no such}\u0026#34; bash: amy: no such I guess not. Maybe it was Brian instead?\necho \u0026#34;${brian:?no such}\u0026#34; Lederhosen You can also:\n   Do this With this     Substitute word if parameter is unset or null ${parameter:-word}   Substitute word if parameter is not unset or null ${parameter:+word}   Assign word to parameter if parameter is unset or null ${parameter:=word}    Changing case That\u0026rsquo;s right! Brian said he wanted some lederhosen and so I made myself a note. This is pretty important, so I\u0026rsquo;ll add it to Brian\u0026rsquo;s list in capital letters with the ${parameter^^pattern} expansion. The pattern part is optional. We\u0026rsquo;re only writing to Brian\u0026rsquo;s list, so I\u0026rsquo;ll just use \u0026gt;\u0026gt; instead of tee -a.\necho \u0026#34;${brian^^}\u0026#34; \u0026gt;\u0026gt; Brian.txt cat Brian.txt socks candy cane white chocolate chocolate LEDERHOSEN You can also:\n   Do this With this     Capitalize the first letter ${parameter^pattern}   Lowercase the first letter ${parameter,pattern}   Lowercase all letters ${parameter,,pattern}    Expanding arrays You know what, all this gift-listing business is a lot of work. I\u0026rsquo;m just going to make an array of things I saw at the store:\ngifts=(sweater gameboy wagon pillows chestnuts hairbrush) I can use substring expansion in the form of ${parameter:offset:length} to make this simple. I\u0026rsquo;ll add the first two to Amy\u0026rsquo;s list, the middle two to Brian\u0026rsquo;s, and the last two to Charlie\u0026rsquo;s. I\u0026rsquo;ll use printf to help with newlines.\nprintf \u0026#39;%s\\n\u0026#39; \u0026#34;${gifts[@]:0:2}\u0026#34; \u0026gt;\u0026gt; Amy.txt printf \u0026#39;%s\\n\u0026#39; \u0026#34;${gifts[@]:2:2}\u0026#34; \u0026gt;\u0026gt; Brian.txt printf \u0026#39;%s\\n\u0026#39; \u0026#34;${gifts[@]: -2}\u0026#34; \u0026gt;\u0026gt; Charlie.txt cat Amy.txt socks candy cane white chocolate chocolate sweater gameboy cat Brian.txt socks candy cane white chocolate chocolate LEDERHOSEN wagon pillows cat Charlie.txt socks candy cane white chocolate chocolate chestnuts hairbrush There! Now we\u0026rsquo;ve got a comprehensive set of super personalized gift lists. Thanks Bash! Too bad it can\u0026rsquo;t do the shopping for us, too.\n",url:"https://victoria.dev/blog/bash-and-shell-expansions-lazy-list-making/"},"https:\/\/victoria.dev\/neofeed\/31738\/":{title:"31738",tags:[],content:"\u0026ldquo;Number of lines of code\u0026rdquo; tells us\u0026hellip;\nA) The level of experience of the coder B) The relative complexity of the program C) The number of newline characters the file contains FFS\n",url:"https://victoria.dev/neofeed/31738/"},"https:\/\/victoria.dev\/blog\/a-cron-job-that-could-save-you-from-a-ransomware-attack\/":{title:"A cron job that could save you from a ransomware attack",tags:["cybersecurity","data","terminal","aws"],content:"It\u0026rsquo;s 2019, and ransomware has become a thing.\nSystems that interact with the public, like companies, educational institutions, and public services, are most susceptible. While delivery methods for ransomware vary from the physical realm to communication via social sites and email, all methods only require one person to make one mistake in order for ransomware to proliferate.\nRansomware, as you may have heard, is a malicious program that encrypts your files, rendering them unreadable and useless to you. It can include instructions for paying a ransom, usually by sending cryptocurrency, in order to obtain the decryption key. Successful ransomware attacks typically exploit vital, time-sensitive systems. Victims like public services and medical facilities are more likely to have poor or zero recovery processes, leaving governments or insurance providers to reward attackers with ransom payments.\nIndividuals, especially less-than-tech-savvy ones, are no less at risk. Ransomware can occlude personal documents and family photos that may only exist on one machine.\nThankfully, a fairly low-tech solution exists for rendering ransomware inept: back up your data!\nYou could achieve this with a straightforward system like plugging in an external hard drive and dragging files over once a day, but this method has a few hurdles. Manually transferring files may be slow or incomplete, and besides, you\u0026rsquo;ll first have to remember to do it.\nIn my constant pursuit of automating all the things, there\u0026rsquo;s one tool I often return to for its simplicity and reliability: cron. Cron does one thing, and does it well: it runs commands on a schedule.\nI first used it a few months shy of three years ago (Have I really been blogging that long?!) to create custom desktop notifications on Linux. Using the crontab configuration file, which you can edit by running crontab -e, you can specify a schedule for running any commands you like. Here\u0026rsquo;s what the scheduling syntax looks like, from the Wikipedia cron page:\n# ┌───────────── minute (0 - 59) # │ ┌───────────── hour (0 - 23) # │ │ ┌───────────── day of the month (1 - 31) # │ │ │ ┌───────────── month (1 - 12) # │ │ │ │ ┌───────────── day of the week (0 - 6) # │ │ │ │ │ # │ │ │ │ │ # │ │ │ │ │ # * * * * * command to execute For example, a cron job that runs every day at 00:00 would look like:\n0 0 * * * To run a job every twelve hours, the syntax is:\n0 */12 * * * This great tool can help you wrap your head around the cron scheduling syntax.\nWhat\u0026rsquo;s a scheduler have to do with backing up? By itself, not much. The simple beauty of cron is that it runs commands - any shell commands, and any scripts that you\u0026rsquo;d normally run on the command line. As you may have gleaned from my other posts, I\u0026rsquo;m of the strong opinion that you can do just about anything on the command line, including backing up your files. Options for storage in this area are plentiful, from near-to-free local and cloud options, as well as paid managed services too numerous to list. For CLI tooling, we have utilitarian classics like rsync, and CLI tools for specific cloud providers like AWS.\nBacking up with rsync The rsync utility is a classic choice, and can back up your files to an external hard drive or remote server while making intelligent determinations about which files to update. It uses file size and modification times to recognize file changes, and then only transfers changed files, saving time and bandwidth.\nThe rsync syntax can be a little nuanced; for example, a trailing forward slash will copy just the contents of the directory, instead of the directory itself. I found examples to be helpful in understanding the usage and syntax.\nHere\u0026rsquo;s one for backing up a local directory to a local destination, such as an external hard drive:\nrsync -a /home/user/directory /media/user/destination The first argument is the source, and the second is the destination. Reversing these in the above example would copy files from the mounted drive to the local home directory.\nThe a flag for archive mode is one of rsync\u0026rsquo;s superpowers. Equivalent to flags -rlptgoD, it:\n Syncs files recursively through directories (r); Preserves symlinks (l), permissions (p), modification times (t), groups (g), and owner (o); and Copies device and special files (D).  Here\u0026rsquo;s another example, this time for backing up the contents of a local directory to a directory on a remote server using SSH:\nrsync -avze ssh /home/user/directory/ user@remote.host.net:home/user/directory The v flag turns on verbose output, which is helpful if you like realtime feedback on which files are being transferred. During large transfers, however, it can tend to slow things down. The z flag can help with that, as it indicates that files should be compressed during transfer.\nThe e flag, followed by ssh, tells rsync to use SSH according to the destination instructions provided in the final argument.\nBacking up with AWS CLI Amazon Web Services offers a command line interface tool for doing just about everything with your AWS set up, including a straightforward s3 sync command for recursively copying new and updated files to your S3 storage buckets. As a storage method for back up data, S3 is a stable and inexpensive choice. You can even turn on versioning in your bucket.\nThe syntax for interacting with directories is fairly straightforward, and you can directly indicate your S3 bucket as an S3Uri argument in the form of s3://mybucket/mykey. To back up a local directory to your S3 bucket, the command is:\naws s3 sync /home/user/directory s3://mybucket Similar to rsync, reversing the source and destination would download files from the S3 bucket.\nThe sync command is intuitive by default. It will guess the mime type of uploaded files, as well as include files discovered by following symlinks. A variety of options exist to control these and other defaults, even including flags to specify the server-side encryption to be used.\nSetting up your cronjob back up You can edit your machine\u0026rsquo;s cron file by running:\ncrontab -e Intuitive as it may be, it\u0026rsquo;s worth mentioning that your back up commands will only run when your computer is turned on and the cron daemon is running. With this in mind, choose a schedule for your cronjob that aligns with times when your machine is powered on, and maybe not overloaded with other work.\nTo back up to an S3 bucket every day at 8AM, for example, you\u0026rsquo;d put a line in your crontab that looks like:\n0 8 * * * aws s3 sync /home/user/directory s3://mybucket If you\u0026rsquo;re curious whether your cron job is currently running, find the PID of cron with:\npstree -ap | grep cron Then run pstree -ap \u0026lt;PID\u0026gt;.\nThis rabbit hole goes deeper; a quick search can reveal different ways of organizing and scheduling cronjobs, or help you find different utilities to run cronjobs when your computer is asleep. To protect against the possibility of ransomware-affected files being transferred to your back up, incrementally separated archives are a good idea. In essence, however, this basic set up is all you really need to create a reliable, automatic back up system.\nDon\u0026rsquo;t feed the trolls Humans are fallible; that\u0026rsquo;s why cyberattacks work. The success of a ransomware attack depends on the victim having no choice but to pay up in order to return to business as usual. A highly accessible recent back up undermines attackers who depend on us being unprepared. By blowing away a system and restoring from yesterday\u0026rsquo;s back up, we may lose a day of progress; ransomers, however, gain nothing at all.\nFor further resources on ransomware defense for users and organizations, check out CISA\u0026rsquo;s advice on ransomware.\n",url:"https://victoria.dev/blog/a-cron-job-that-could-save-you-from-a-ransomware-attack/"},"https:\/\/victoria.dev\/neofeed\/31037\/":{title:"31037",tags:[],content:"I think the most effective way to make a contribution is to keep doing the useful, productive thing that you enjoy doing most.\nWhatever you already do the most of - when left to your own devices, when you are producing instead of consuming - is meaningful to you.\n",url:"https://victoria.dev/neofeed/31037/"},"https:\/\/victoria.dev\/neofeed\/30828\/":{title:"30828",tags:[],content:"I\u0026rsquo;m just a girl, standing in front of the Internet, asking it to please sign my guestbook. ❤️\nhttps://victoria.dev/github-guestbook/\n",url:"https://victoria.dev/neofeed/30828/"},"https:\/\/victoria.dev\/blog\/publishing-github-event-data-with-github-actions-and-pages\/":{title:"Publishing GitHub event data with GitHub Actions and Pages",tags:["data","ci/cd","api","git","websites"],content:"Teams who work on GitHub rely on event data to collaborate. The data recorded as issues, pull requests, and comments, become vital to understanding the project.\nWith the general availability of GitHub Actions, we have a chance to programmatically access and preserve GitHub event data in our repository. Making the data part of the repository itself is a way of preserving it outside of GitHub, and also gives us the ability to feature the data on a front-facing website, such as with GitHub Pages, through an automated process that\u0026rsquo;s part of our CI/CD pipeline.\nAnd, if you\u0026rsquo;re like me, you can turn GitHub issue comments into an awesome 90s guestbook page.\nNo matter the usage, the principle concepts are the same. We can use Actions to access, preserve, and display GitHub event data - with just one workflow file. To illustrate the process, I\u0026rsquo;ll take you through the workflow code that makes my guestbook shine on.\nFor an introductory look at GitHub Actions including how workflows are triggered, see A lightweight, tool-agnostic CI/CD flow with GitHub Actions.\nAccessing GitHub event data An Action workflow runs in an environment with some default environment variables. A lot of convenient information is available here, including event data. The most complete way to access the event data is using the $GITHUB_EVENT_PATH variable, the path of the file with the complete JSON event payload.\nThe expanded path looks like /home/runner/work/_temp/_github_workflow/event.json and its data corresponds to its webhook event. You can find the documentation for webhook event data in GitHub REST API Event Types and Payloads. To make the JSON data available in the workflow environment, you can use a tool like jq to parse the event data and put it in an environment variable.\nBelow, I grab the comment ID from an issue comment event:\nID=\u0026#34;$(jq \u0026#39;.comment.id\u0026#39; $GITHUB_EVENT_PATH)\u0026#34; Most event data is also available via the github.event context variable without needing to parse JSON. The fields are accessed using dot notation, as in the example below where I grab the same comment ID:\nID=${{ github.event.comment.id }} For my guestbook, I want to display entries with the user\u0026rsquo;s handle, and the date and time. I can capture this event data like so:\nAUTHOR=${{ github.event.comment.user.login }} DATE=${{ github.event.comment.created_at }} Shell variables are handy for accessing data, however, they\u0026rsquo;re ephemeral. The workflow environment is created anew each run, and even shell variables set in one step do not persist to other steps. To persist the captured data, you have two options: use artifacts, or commit it to the repository.\nPreserving event data: using artifacts Using artifacts, you can persist data between workflow jobs without committing it to your repository. This is handy when, for example, you wish to transform or incorporate the data before putting it somewhere more permanent. It\u0026rsquo;s necessary to persist data between workflow jobs because:\n Each job in a workflow runs in a fresh instance of the virtual environment. When the job completes, the runner terminates and deletes the instance of the virtual environment. (Persisting workflow data using artifacts)\n Two actions assist with using artifacts: upload-artifact and download-artifact. You can use these actions to make files available to other jobs in the same workflow. For a full example, see passing data between jobs in a workflow.\nThe upload-artifact action\u0026rsquo;s action.yml contains an explanation of the keywords. The uploaded files are saved in .zip format. Another job in the same workflow run can use the download-artifact action to utilize the data in another step.\nYou can also manually download the archive on the workflow run page, under the repository\u0026rsquo;s Actions tab.\nPersisting workflow data between jobs does not make any changes to the repository files, as the artifacts generated live only in the workflow environment. Personally, being comfortable working in a shell environment, I see a narrow use case for artifacts, though I\u0026rsquo;d have been remiss not to mention them. Besides passing data between jobs, they could be useful for creating .zip format archives of, say, test output data. In the case of my guestbook example, I simply ran all the necessary steps in one job, negating any need for passing data between jobs.\nPreserving event data: pushing workflow files to the repository To preserve data captured in the workflow in the repository itself, it is necessary to add and push this data to the Git repository. You can do this in the workflow by creating new files with the data, or by appending data to existing files, using shell commands.\nCreating files in the workflow To work with the repository files in the workflow, use the checkout action to first get a copy to work with:\n- uses:actions/checkout@masterwith:fetch-depth:1To add comments to my guestbook, I turn the event data captured in shell variables into proper files, using substitutions in shell parameter expansion to sanitize user input and translate newlines to paragraphs. I wrote previously about why user input should be treated carefully.\n- name:Turn comment into filerun:|ID=${{ github.event.comment.id }} AUTHOR=${{ github.event.comment.user.login }} DATE=${{ github.event.comment.created_at }} COMMENT=$(echo \u0026#34;${{ github.event.comment.body }}\u0026#34;) NO_TAGS=${COMMENT//[\u0026lt;\u0026gt;]/\\`} FOLDER=comments printf \u0026#39;%b\\n\u0026#39; \u0026#34;\u0026lt;div class=\\\u0026#34;comment\\\u0026#34;\u0026gt;\u0026lt;p\u0026gt;${AUTHOR} says:\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;${NO_TAGS//$\u0026#39;\\n\u0026#39;/\\\u0026lt;\\/p\\\u0026gt;\\\u0026lt;p\\\u0026gt;}\u0026lt;/p\u0026gt;\u0026lt;p\u0026gt;${DATE}\u0026lt;/p\u0026gt;\u0026lt;/div\u0026gt;\\r\\n\u0026#34; \u0026gt; ${FOLDER}/${ID}.htmlBy using printf and directing its output with \u0026gt; to a new file, the event data is transformed into an HTML file, named with the comment ID number, that contains the captured event data. Formatted, it looks like:\n\u0026lt;div class=\u0026#34;comment\u0026#34;\u0026gt; \u0026lt;p\u0026gt;victoriadrake says:\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;This is a comment!\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;2019-11-04T00:28:36Z\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; When working with comments, one effect of naming files using the comment ID is that a new file with the same ID will overwrite the previous. This is handy for a guestbook, as it allows any edits to a comment to replace the original comment file.\nIf you\u0026rsquo;re using a static site generator like Hugo, you could build a Markdown format file, stick it in your content/ folder, and the regular site build will take care of the rest. In the case of my simplistic guestbook, I have an extra step to consolidate the individual comment files into a page. Each time it runs, it overwrites the existing index.html with the header.html portion (\u0026gt;), then finds and appends (\u0026gt;\u0026gt;) all the comment files' contents in descending order, and lastly appends the footer.html portion to end the page.\n- name:Assemble pagerun:|cat header.html \u0026gt; index.html find comments/ -name \u0026#34;*.html\u0026#34; | sort -r | xargs -I % cat % \u0026gt;\u0026gt; index.html cat footer.html \u0026gt;\u0026gt; index.htmlCommitting changes to the repository Since the checkout action is not quite the same as cloning the repository, at time of writing, there are some issues still to work around. A couple extra steps are necessary to pull, checkout, and successfully push changes back to the master branch, but this is pretty trivially done in the shell.\nBelow is the step that adds, commits, and pushes changes made by the workflow back to the repository\u0026rsquo;s master branch.\n- name:Push changes to reporun:|REMOTE=https://${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }} git config user.email \u0026#34;${{ github.actor }}@users.noreply.github.com\u0026#34; git config user.name \u0026#34;${{ github.actor }}\u0026#34; git pull ${REMOTE} git checkout master git add . git status git commit -am \u0026#34;Add new comment\u0026#34; git push ${REMOTE} masterThe remote, in fact, our repository, is specified using the github.repository context variable. For our workflow to be allowed to push to master, we give the remote URL using the default secrets.GITHUB_TOKEN variable.\nSince the workflow environment is shiny and newborn, we need to configure Git. In the above example, I\u0026rsquo;ve used the github.actor context variable to input the username of the account initiating the workflow. The email is similarly configured using the default noreply GitHub email address.\nDisplaying event data If you\u0026rsquo;re using GitHub Pages with the default secrets.GITHUB_TOKEN variable and without a site generator, pushing changes to the repository in the workflow will only update the repository files. The GitHub Pages build will fail with an error, \u0026ldquo;Your site is having problems building: Page build failed.\u0026rdquo;\nTo enable Actions to trigger a Pages site build, you\u0026rsquo;ll need to create a Personal Access Token. This token can be stored as a secret in the repository settings and passed into the workflow in place of the default secrets.GITHUB_TOKEN variable. I wrote more about Actions environment and variables in this post.\nWith the use of a Personal Access Token, a push initiated by the Actions workflow will also update the Pages site. You can see it for yourself by leaving a comment in my guestbook! The comment creation event triggers the workflow, which then takes around 30 seconds to run and update the guestbook page.\nWhere a site build is necessary for changes to be published, such as when using Hugo, an Action can do this too. However, in order to avoid creating unintended loops, one Action workflow will not trigger another. Instead, it\u0026rsquo;s extremely convenient to handle the process of building the site with a Makefile, which any workflow can then run. Simply add running the Makefile as the final step in your workflow job, with the repository token where necessary:\n- name:Run Makefileenv:TOKEN:${{ secrets.GITHUB_TOKEN }}run:make allThis ensures that the final step of your workflow builds and deploys the updated site.\nNo more event data horizon GitHub Actions provides a neat way to capture and utilize event data so that it\u0026rsquo;s not only available within GitHub. The possibilities are only as limited as your imagination! Here are a few ideas for things this lets us create:\n A public-facing issues board, where customers without GitHub accounts can view and give feedback on project issues. An automatically-updating RSS feed of new issues, comments, or PRs for any repository. A comments system for static sites, utilizing GitHub issue comments as an input method. An awesome 90s guestbook page.  Did I mention I made a 90s guestbook page? My inner-Geocities-nerd is a little excited.\n",url:"https://victoria.dev/blog/publishing-github-event-data-with-github-actions-and-pages/"},"https:\/\/victoria.dev\/neofeed\/30544\/":{title:"30544",tags:[],content:"Halloween walks into a room full of hackers. No one is afraid because all the costumes and passwords are the same ones they were last year.\n",url:"https://victoria.dev/neofeed/30544/"},"https:\/\/victoria.dev\/blog\/a-lightweight-tool-agnostic-ci\/cd-flow-with-github-actions\/":{title:"A lightweight, tool-agnostic CI/CD flow with GitHub Actions",tags:["ci/cd","terminal","websites"],content:"Agnostic tooling is the clever notion that you should be able to run your code in various environments. With many continuous integration and continuous development (CI/CD) apps available, agnostic tooling gives developers a big advantage: portability.\nOf course, having your CI/CD work everywhere is a tall order. Popular CI apps for GitHub repositories alone use a multitude of configuration languages spanning Groovy, YAML, TOML, JSON, and more\u0026hellip; all with differing syntax, of course. Porting workflows from one tool to another is more than a one-cup-of-coffee process.\nThe introduction of GitHub Actions has the potential to add yet another tool to the mix; or, for the right set up, greatly simplify a CI/CD workflow.\nPrior to this article, I accomplished my CD flow with several lashed-together apps. I used AWS Lambda to trigger site builds on a schedule. I had Netlify build on push triggers, as well as run image optimization, and then push my site to the public Pages repository. I used Travis CI in the public repository to test the HTML. All this worked in conjunction with GitHub Pages, which actually hosts the site.\nI\u0026rsquo;m now using the GitHub Actions beta to accomplish all the same tasks, with one portable Makefile of build instructions, and without any other CI/CD apps.\nAppreciating the shell What do most CI/CD tools have in common? They run your workflow instructions in a shell environment! This is wonderful, because that means that most CI/CD tools can do anything that you can do in a terminal\u0026hellip; and you can do pretty much anything in a terminal.\nEspecially for a contained use case like building my static site with a generator like Hugo, running it all in a shell is a no-brainer. To tell the magic box what to do, we just need to write instructions.\nWhile a shell script is certainly the most portable option, I use the still-very-portable Make to write my process instructions. This provides me with some advantages over simple shell scripting, like the use of variables and macros, and the modularity of rules.\nI got into the nitty-gritty of my Makefile in my last post. Let\u0026rsquo;s look at how to get GitHub Actions to run it.\nUsing a Makefile with GitHub Actions To our point on portability, my magic Makefile is stored right in the repository root. Since it\u0026rsquo;s included with the code, I can run the Makefile locally on any system where I can clone the repository, provided I set the environment variables. Using GitHub Actions as my CI/CD tool is as straightforward as making Make go worky-worky.\nI found the GitHub Actions workflow syntax guide to be pretty straightforward, though also lengthy on options. Here\u0026rsquo;s the necessary set up for getting the Makefile to run.\nThe workflow file at .github/workflows/make-master.yml contains the following:\nname:make-masteron:push:branches:- masterschedule:- cron:\u0026#39;20 13 * * *\u0026#39;jobs:build:runs-on:ubuntu-lateststeps:- uses:actions/checkout@masterwith:fetch-depth:1- name:Run Makefileenv:TOKEN:${{ secrets.TOKEN }}run:make allI\u0026rsquo;ll explain the components that make this work.\nTriggering the workflow Actions support multiple triggers for a workflow. Using the on syntax, I\u0026rsquo;ve defined two triggers for mine: a push event to the master branch only, and a scheduled cron job.\nOnce the make-master.yml file is in your repository, either of your triggers will cause Actions to run your Makefile. To see how the last run went, you can also add a fun badge to the README.\nOne hacky thing Because the Makefile runs on every push to master, I sometimes would get errors when the site build had no changes. When Git, via my Makefile, attempted to commit to the Pages repository, no changes were detected and the commit would fail annoyingly:\nnothing to commit, working tree clean On branch master Your branch is up to date with \u0026#39;origin/master\u0026#39;. nothing to commit, working tree clean Makefile:62: recipe for target \u0026#39;deploy\u0026#39; failed make: *** [deploy] Error 1 ##[error]Process completed with exit code 2. I came across some solutions that proposed using diff to check if a commit should be made, but this may not work for reasons. As a workaround, I simply added the current UTC time to my index page so that every build would contain a change to be committed.\nEnvironment and variables You can define the virtual environment for your workflow to run in using the runs-on syntax. The obvious best choice one I chose is Ubuntu. Using ubuntu-latest gets me the most updated version, whatever that happens to be when you\u0026rsquo;re reading this.\nGitHub sets some default environment variables for workflows. The actions/checkout action with fetch-depth: 1 creates a copy of just the most recent commit your repository in the GITHUB_WORKSPACE variable. This allows the workflow to access the Makefile at GITHUB_WORKSPACE/Makefile. Without using the checkout action, the Makefile won\u0026rsquo;t be found, and I get an error that looks like this:\nmake: *** No rule to make target \u0026#39;all\u0026#39;. Stop. Running Makefile ##[error]Process completed with exit code 2. While there is a default GITHUB_TOKEN secret, this is not the one I used. The default is only locally scoped to the current repository. To be able to push to my separate GitHub Pages repository, I created a personal access token scoped to public_repo and pass it in as the secrets.TOKEN encrypted variable. For a step-by-step, see Creating and using encrypted secrets.\nPortable tooling The nice thing about using a simple Makefile to define the bulk of my CI/CD process is that it\u0026rsquo;s completely portable. I can run a Makefile anywhere I have access to an environment, which is most CI/CD apps, virtual instances, and, of course, on my local machine.\nOne of the reasons I like GitHub Actions is that getting my Makefile to run was pretty straightforward. I think the syntax is well done - easy to read, and intuitive when it comes to finding an option you\u0026rsquo;re looking for. For someone already using GitHub Pages, Actions provides a pretty seamless CD experience; and if that should ever change, I can run my Makefile elsewhere. ¯\\_(ツ)_/¯\n",url:"https://victoria.dev/blog/a-lightweight-tool-agnostic-ci/cd-flow-with-github-actions/"},"https:\/\/victoria.dev\/neofeed\/29828\/":{title:"29828",tags:[],content:"Famous last words: I\u0026rsquo;ll just try this real quick.\n",url:"https://victoria.dev/neofeed/29828/"},"https:\/\/victoria.dev\/blog\/a-portable-makefile-for-continuous-delivery-with-hugo-and-github-pages\/":{title:"A portable Makefile for continuous delivery with Hugo and GitHub Pages",tags:["ci/cd","linux","terminal","websites"],content:"Fun fact: I first launched this GitHub Pages site 1,018 days ago.\nSince then, we\u0026rsquo;ve grown together. From early cringe-worthy commit messages, through eighty-six versions of Hugo, and up until last week, a less-than-streamlined multi-app continuous integration and deployment (CI/CD) workflow.\nIf you know me at all, you know I love to automate things. I\u0026rsquo;ve been using a combination of AWS Lambda, Netlify, and Travis CI to automatically build and publish this site. My workflow for the task includes:\n Build with Hugo on push to master, and on a schedule (Netlify and Lambda); Optimize and resize images (Netlify); Test with HTMLProofer (Travis CI); and Deploy to my separate, public, GitHub Pages repository (Netlify).  Thanks to the introduction of GitHub Actions, I\u0026rsquo;m able to do all the above with just one portable Makefile.\nNext week I\u0026rsquo;ll cover my Actions set up; today, I\u0026rsquo;ll take you through the nitty-gritty of my Makefile so you can write your own.\nMakefile portability POSIX-standard-flavour Make runs on every Unix-like system out there. Make derivatives, such as GNU Make and several flavours of BSD Make also run on Unix-like systems, though their particular use requires installing the respective program. To write a truly portable Makefile, mine follows the POSIX standard. (For a more thorough summation of POSIX-compatible Makefiles, I found this article helpful: A Tutorial on Portable Makefiles.) I run Ubuntu, so I\u0026rsquo;ve tested the portability aspect using the BSD Make programs bmake, pmake, and fmake. Compatibility with non-Unix-like systems is a little more complicated, since shell commands differ. With derivatives such as Nmake, it\u0026rsquo;s better to write a separate Makefile with appropriate Windows commands.\nWhile much of my particular use case could be achieved with shell scripting, I find Make offers some worthwhile advantages. I enjoy the ease of using variables and macros, and the modularity of rules when it comes to organizing my steps.\nThe writing of rules mostly comes down to shell commands, which is the main reason Makefiles are as portable as they are. The best part is that you can do pretty much anything in a terminal, and certainly handle all the workflow steps listed above.\nMy continuous deployment Makefile Here\u0026rsquo;s the portable Makefile that handles my workflow. Yes, I put emojis in there. I\u0026rsquo;m a monster.\n.POSIX: DESTDIR=public HUGO_VERSION=0.58.3 OPTIMIZE = find $(DESTDIR) -not -path \u0026#34;*/static/*\u0026#34; \\( -name \u0026#39;*.png\u0026#39; -o -name \u0026#39;*.jpg\u0026#39; -o -name \u0026#39;*.jpeg\u0026#39; \\) -print0 | \\ xargs -0 -P8 -n2 mogrify -strip -thumbnail \u0026#39;1000\u0026gt;\u0026#39; .PHONY: all all: get_repository clean get build test deploy .PHONY: get_repository get_repository: @echo \u0026#34;🛎 Getting Pages repository\u0026#34; git clone https://github.com/victoriadrake/victoriadrake.github.io.git $(DESTDIR) .PHONY: clean clean: @echo \u0026#34;🧹 Cleaning old build\u0026#34; cd $(DESTDIR) \u0026amp;\u0026amp; rm -rf * .PHONY: get get: @echo \u0026#34;❓ Checking for hugo\u0026#34; @if ! [ -x \u0026#34;$$(command -v hugo)\u0026#34; ]; then\\  echo \u0026#34;🤵 Getting Hugo\u0026#34;;\\  wget -q -P tmp/ https://github.com/gohugoio/hugo/releases/download/v$(HUGO_VERSION)/hugo_extended_$(HUGO_VERSION)_Linux-64bit.tar.gz;\\  tar xf tmp/hugo_extended_$(HUGO_VERSION)_Linux-64bit.tar.gz -C tmp/;\\  sudo mv -f tmp/hugo /usr/bin/;\\  rm -rf tmp/;\\  hugo version;\\  fi .PHONY: build build: @echo \u0026#34;🍳 Generating site\u0026#34; hugo --gc --minify -d $(DESTDIR) @echo \u0026#34;🧂 Optimizing images\u0026#34; $(OPTIMIZE) .PHONY: test test: @echo \u0026#34;🍜 Testing HTML\u0026#34; docker run -v $(GITHUB_WORKSPACE)/$(DESTDIR)/:/mnt 18fgsa/html-proofer mnt --disable-external .PHONY: deploy deploy: @echo \u0026#34;🎁 Preparing commit\u0026#34; @cd $(DESTDIR) \\  \u0026amp;\u0026amp; git config user.email \u0026#34;hello@victoria.dev\u0026#34; \\  \u0026amp;\u0026amp; git config user.name \u0026#34;Victoria via GitHub Actions\u0026#34; \\  \u0026amp;\u0026amp; git add . \\  \u0026amp;\u0026amp; git status \\  \u0026amp;\u0026amp; git commit -m \u0026#34;🤖 CD bot is helping\u0026#34; \\  \u0026amp;\u0026amp; git push -f -q https://$(TOKEN)@github.com/victoriadrake/victoriadrake.github.io.git master @echo \u0026#34;🚀 Site is deployed!\u0026#34; Sequentially, this workflow:\n Clones the public Pages repository; Cleans (deletes) the previous build files; Downloads and installs the specified version of Hugo, if Hugo is not already present; Builds the site; Optimizes images; Tests the built site with HTMLProofer, and Prepares a new commit and pushes to the public Pages repository.  If you\u0026rsquo;re familiar with command line, most of this may look familiar. Here are a couple bits that might warrant a little explanation.\nChecking if a program is already installed I think this bit is pretty tidy:\nif ! [ -x \u0026#34;$$(command -v hugo)\u0026#34; ]; then\\ ... fi I use a negated if conditional in conjunction with command -v to check if an executable (-x) called hugo exists. If one is not present, the script gets the specified version of Hugo and installs it. This Stack Overflow answer has a nice summation of why command -v is a more portable choice than which.\nImage optimization My Makefile uses mogrify to batch resize and compress images in particular folders. It finds them automatically using the file extension, and only modifies images that are larger than the target size of 1000px in any dimension. I wrote more about the batch-processing one-liner in this post.\nThere are a few different ways to achieve this same task, one of which, theoretically, is to take advantage of Make\u0026rsquo;s suffix rules to run commands only on image files. I find the shell script to be more readable.\nUsing Dockerized HTMLProofer HTMLProofer is installed with gem, and uses Ruby and Nokogiri, which adds up to a lot of installation time for a CI workflow. Thankfully, 18F has a Dockerized version that is much faster to implement. Its usage requires starting the container with the built site directory mounted as a data volume, which is easily achieved by appending to the docker run command.\ndocker run -v /absolute/path/to/site/:/mounted-site 18fgsa/html-proofer /mounted-site In my Makefile, I specify the absolute site path using the default environment variable GITHUB_WORKSPACE. I\u0026rsquo;ll dive into this and other GitHub Actions features in the next post.\nIn the meantime, happy Making!\n",url:"https://victoria.dev/blog/a-portable-makefile-for-continuous-delivery-with-hugo-and-github-pages/"},"https:\/\/victoria.dev\/neofeed\/29036\/":{title:"29036",tags:[],content:"No man has ever satisfied me quite as much as a well-written bash one-liner does (except my husband since he also writes bash one-liners and is very handsome).\n",url:"https://victoria.dev/neofeed/29036/"},"https:\/\/victoria.dev\/blog\/how-to-quickly-batch-resize-compress-and-convert-images-with-a-bash-one-liner\/":{title:"How to quickly batch resize, compress, and convert images with a Bash one-liner",tags:["terminal","ci/cd"],content:"Part of my Hugo site continuous deployment workflow is the processing of 210 images, at time of writing.\nHere\u0026rsquo;s my one-liner:\nfind public/ -not -path \u0026#34;*/static/*\u0026#34; \\( -name \u0026#39;*.png\u0026#39; -o -name \u0026#39;*.jpg\u0026#39; -o -name \u0026#39;*.jpeg\u0026#39; \\) -print0 | xargs -0 -P8 -n2 mogrify -strip -thumbnail \u0026#39;1000\u0026gt;\u0026#39; -format jpg I use find to target only certain image file formats in certain directories. With mogrify, part of ImageMagick, I resize only the images that are larger than a certain dimension, compress them, and strip the metadata. I tack on the format flag to create jpg copies of the images.\nHere\u0026rsquo;s the one-liner again (broken up for better reading):\n# Look in the public/ directory find public/ \\ # Ignore directories called \u0026#34;static\u0026#34; regardless of location -not -path \u0026#34;*/static/*\u0026#34; \\ # Print the file paths of all files ending with any of these extensions \\( -name \u0026#39;*.png\u0026#39; -o -name \u0026#39;*.jpg\u0026#39; -o -name \u0026#39;*.jpeg\u0026#39; \\) -print0 \\ # Pipe the file paths to xargs and use 8 parallel workers to process 2 arguments | xargs -0 -P8 -n2 \\ # Tell mogrify to strip metadata, and... mogrify -strip \\ # ...compress and resize any images larger than the target size (1000px in either dimension) -thumbnail \u0026#39;1000\u0026gt;\u0026#39; \\ # Convert the files to jpg format -format jpg That\u0026rsquo;s it. That\u0026rsquo;s the post.\n",url:"https://victoria.dev/blog/how-to-quickly-batch-resize-compress-and-convert-images-with-a-bash-one-liner/"},"https:\/\/victoria.dev\/neofeed\/28235\/":{title:"28235",tags:[],content:"How I strive to explain most things to most people: https://xkcd.com/1133/\n",url:"https://victoria.dev/neofeed/28235/"},"https:\/\/victoria.dev\/blog\/personal-cybersecurity-posture-with-too-many-pop-culture-references\/":{title:"Personal cybersecurity posture with too many pop culture references",tags:["cybersecurity","life"],content:" \u0026ldquo;Zaphod\u0026rsquo;s just this guy, you know?\u0026rdquo;\n\u0026ndash; Halfrunt, Hitchhiker\u0026rsquo;s Guide to the Galaxy by Douglas Adams. The book, not the movie. Definitely not the movie.\n Some people (🙋🏻‍) are really into cybersecurity, end-to-end encryption, and totally geeked out when they first learned how the Enigma worked. These people are likely to have an innate interest in building a less-than-laughable personal cybersecurity posture.\nMost people, unfortunately, consider cybersecurity optional. Most people say things like:\n\u0026ldquo;There\u0026rsquo;s no one targeting lil ol' me.\u0026quot;\n\u0026ldquo;I have nothing to hide, anyway.\u0026quot;\n\u0026ldquo;I\u0026rsquo;m too busy to learn all this stuff. Why can\u0026rsquo;t someone just give me a simple summary of best practices that I can skim in approximately seven minutes?\u0026quot;\nTo those people, I say, hello, hypothetical incorporeal reader! Here is a simple summary of best practices that you can skim in approximately seven minutes.\nWait why do I care You may have a hard time understanding why cybersecurity matters when you\u0026rsquo;re just an average person. Sure, you don\u0026rsquo;t want your devices hacked or your personal data stolen, but it\u0026rsquo;s not like anyone is coming after you, specifically, right?\nHey Alex, I\u0026rsquo;ll take \u0026ldquo;right,\u0026rdquo; for $400. It\u0026rsquo;s unlikely anyone is attempting to steal your particular stuff, although I must admit that Persian rug of yours would really tie the room together. Instead, it can help to understand cybersecurity if you think of it in terms of low-hanging fruit.\nYou\u0026rsquo;ve got some fruit, I\u0026rsquo;ve got some fruit. Joe from down the block has a 1.21 gigawatt flux-capacitor-powered fruit-snatching robot. Joe doesn\u0026rsquo;t know either of us exist, but his robot goes (very quickly) from door to door, all the way around the block, looking for fruit. If my front door is locked and yours is standing open, whose fruit is Joe\u0026rsquo;s robot going to snatch?\nIf that sounds like boring, old, regular security, you\u0026rsquo;re correct! Cybersecurity isn\u0026rsquo;t about finding some magic spell that makes your fruit maximally secure. It\u0026rsquo;s about making your fruit more secure than the fruit next to you. You do this by employing some thoughtful habits, in much the same way as you learned to lock your front door to guard against fruit-snatching robots.\nSecurity breaches and incidents happen every day. Most of them occur because an automated scanner cast a wide net and found a person or company with lax security that a hacker could then exploit. Don\u0026rsquo;t be that guy.\nWait what\u0026rsquo;s a security posture anyway Here is how the National Institute of Standards and Technology defines security posture:\n The security status of an enterprise’s networks, information, and systems based on information assurance resources (e.g., people, hardware, software, policies) and capabilities in place to manage the defense of the enterprise and to react as the situation changes. (NIST Special Publication 800-30, B-11)\n The important bit above is, \u0026ldquo;capabilities in place to manage the defense of the enterprise.\u0026quot; In the context of personal security, you are the enterprise. Congratulations. May you boldly go where no man has gone before.\nBefore you explore strange new worlds (it is the Internet, after all), there are steps you can take to manage your defenses. The word \u0026ldquo;capabilities\u0026rdquo; is apt, as having certain things in place will pretty much give you cybersecurity superpowers. Here are the three steps I consider most important and beneficial:\n Use multifactor authentication Use a VPN Develop healthy skepticism  With these three keys in hand, your cybersecurity posture goes from being robot lunch to War Games - where the winning move for an attacker is not to play.\n1. Use multifactor authentication Passwords are dead. Computationally, they are a solved problem, and cracking passwords is just a matter of time. Unfortunately, many people still help to speed up the process by using the same compromised passwords for multiple accounts, putting themselves at risk for inconceivable benefit. Pass phrases are longer and more complicated, and would take a lot more time to crack. I highly recommend them; even so, your password ultimately doesn\u0026rsquo;t matter.\nThe answer, at least for now, is multifactor authentication (MFA). MFA is made up of three kinds of authentication factors:\n Something you know, like a pass phrase; Something you have, like a chip pin card or phone; and Something that you are, like your face or fingerprint.  Two or more of these factors are infinitely better than a password alone, especially if your password is on this list.\nMultiple authentication factors are now widely supported by account providers and social media sites. If you have the choice, avoid using text messages as a way of receiving authentication codes. SMS authentication leaves you vulnerable to the SIM swap attack - please direct further questions to Jack Dorsey. Instead, use an authenticator app like Authy to generate codes on your device. This ensures that you alone, using that particular device, will have the correct authentication code. No power in the \u0026lsquo;verse can stop you.\nHardware authentication keys such as the YubiKey are also a great option, but may not yet be as well-supported as authentication apps.\n2. Use a VPN The difference between using a VPN and not using one is like how The Dark Knight Rises was really good and Batman v Superman was really, really bad. Same franchise, totally different standards.\nLet\u0026rsquo;s say you send a lot of mail, but never bother to put your letters in envelopes or even fold them in half. Anyone who bothers to look will know that you\u0026rsquo;re not really the Dread Pirate Roberts after all. When you use a Virtual Private Network, especially if you often connect to public WiFi, it\u0026rsquo;s like putting your letters into cryptographically-sealed envelopes and sending them via a special invisible courier service. No one but the intended recipient can read your letters, and no one but you and the courier know to whom the letters are sent.\nVPNs prevent others from reading your communications, like opportunistic attackers who scan open WiFi, and even your own Internet Service Provider (ISP) who may sell your usage data for advertising dollars.\nChoosing a trustworthy VPN provider requires some research, and is in itself material enough for a separate article. As a starting point, look for providers with firm policies against logging, and expect to pay between $5-$10 USD monthly for the service. Avoid free VPN apps and services with ambiguous privacy policies; they\u0026rsquo;ll typically cost you much more than you\u0026rsquo;ll know.\n3. Develop healthy skepticism Ultimately, the weakest link in your cybersecurity defense is you. All the MFA and VPNs on the Internet won\u0026rsquo;t protect you if a scam or malware bot can trick you into opening the front gates. Yes, I know it\u0026rsquo;s a very nice looking wooden horse. Also free. Did you order it? No? Then it can stay outside.\nDevelop the habit of second-guessing things delivered to your virtual doorstep. Email, phone, and messaging scams range in sophistication, from rickety robot-assembled shotgun blasts to elaborate social engineering attacks that use cognitive biases very effectively. Don\u0026rsquo;t assume you\u0026rsquo;re too clever for them; humans are very predictable creatures. After all, nobody expects the Spanish Inquisition.\nInstead, ask questions. Double check communications that ask you to click on links or visit a website, even if they come from someone you know or a company you use. If you\u0026rsquo;re not certain, based on a previous in-person interaction, that your friend or bank or mother sent this email, pick up the phone and call them. Even if you think you are certain, pick up the phone and check. You don\u0026rsquo;t call your mother enough, anyway.\nOh, and if the person on the phone is from your local tax office or the IRS or the CRA and they\u0026rsquo;re about to freeze your accounts because a case of mistaken identity has resulted in you being criminally charged for not repaying a loan on a 600-foot yacht in Malibu, just hang up. You know better than that. Tax agencies don\u0026rsquo;t have phones.\nYour personal cybersecurity starter pack You now have three keys to open three gates to a robust personal cybersecurity posture. If those keys have also unlocked your curiosity, there\u0026rsquo;s plenty more rabbit hole to go down. I highly recommend the Security in Five podcast for Binary Blogger\u0026rsquo;s great advice, which inspired much of this post. Surveillance Self Defense offers the Electronic Frontier Foundation\u0026rsquo;s tips on securing online communication. Troy Hunt also has a YouTube series entitled Internet Security Basics that goes into more depth on how to protect yourself online.\nFor now, I hope you use your newfound cybersecurity powers for good. Mind what you have learned. Save you it can.\n",url:"https://victoria.dev/blog/personal-cybersecurity-posture-with-too-many-pop-culture-references/"},"https:\/\/victoria.dev\/neofeed\/27834\/":{title:"27834",tags:[],content:"Hario V60 Silver.\n16g BRCC Gunship.\n90 seconds off the boil. 50 second bloom.\n3:20 total brew time.\nAroma: charcoal grill, melted milk chocolate.\nTaste: raw cocoa, slight acidity, charred finish.\nVerdict: I like it. Under extraction seems to help this roast go from \u0026ldquo;LETS PUNCH EVERYONE IN THE FACE\u0026rdquo; to \u0026ldquo;Let\u0026rsquo;s gently nudge everyone in a firm but understanding manner.\u0026rdquo;\n",url:"https://victoria.dev/neofeed/27834/"},"https:\/\/victoria.dev\/neofeed\/27734\/":{title:"27734",tags:[],content:"Hario V60 Silver.\n16g BRCC Gunship.\n40 seconds off the boil. 50 second bloom.\nAroma: warm chocolate, campfire.\nTaste: char, roasted nuts, bitter.\nVerdict: a good strong cup, but more bitter than I\u0026rsquo;d like. Next time try cooler water for less extraction.\n",url:"https://victoria.dev/neofeed/27734/"},"https:\/\/victoria.dev\/neofeed\/27634\/":{title:"27634",tags:[],content:"Day one. The irony of establishing familiar rituals: everything is strange and difficult, at first.\n",url:"https://victoria.dev/neofeed/27634/"},"https:\/\/victoria.dev\/neofeed\/27434\/":{title:"27434",tags:[],content:"Happy #Hacktoberfest! This year I’m jazzed to be contributing to @owasp Testing Guide, @OWASPTop10, and @OWASP_ASVS, and sharing what I learn in my blog and on @ThePracticalDev! ‬\n‪Projects like these need data, code review, writers, and editors! What are you working on? 👩🏻‍💻🖋🔐 ✨‬\n",url:"https://victoria.dev/neofeed/27434/"},"https:\/\/victoria.dev\/neofeed\/27434\/":{title:"27434",tags:[],content:"Oh, hey Bob. Yeah, I got your email. I\u0026rsquo;ll get a reply back to you in a couple weeks. Yeah. Just got the ErgoDox.\n",url:"https://victoria.dev/neofeed/27434/"},"https:\/\/victoria.dev\/neofeed\/27434\/":{title:"27434",tags:[],content:"Split life. @ErgoDoxEZ\n",url:"https://victoria.dev/neofeed/27434/"},"https:\/\/victoria.dev\/blog\/secure-application-architecture-basics-separation-configuration-and-access\/":{title:"Secure application architecture basics: separation, configuration, and access",tags:["cybersecurity","coding","data"],content:"Software developers today are encouraged to focus on building, and that\u0026rsquo;s a great thing. We\u0026rsquo;re benefitting from maker culture, an attitude of \u0026ldquo;always be shipping,\u0026rdquo; open source collaboration, and a bevy of apps that help us prioritize and execute with maximum efficiency. We\u0026rsquo;re in an environment of constant creation, where both teams and solo entrepreneurs can be maximally productive.\nSometimes, this breakneck-speed productivity shows its downsides.\nAs I learn more about security best practices, I can\u0026rsquo;t help but see more and more applications that just don\u0026rsquo;t have a clue. A lack of awareness of security seems to lead to a lack of prioritization of tasks that don\u0026rsquo;t directly support bringing the product to launch. The market seems to have made it more important to launch a usable product than a secure one, with the prevailing attitude being, \u0026ldquo;we can do the security stuff later.\u0026rdquo;\nCobbling together a foundation based more on expediency than longevity is a bad way to build applications and a great way to build security debt. Security debt, like technical debt, amasses when developers make (usually hasty) decisions that can make it more difficult to secure the application later on. If you\u0026rsquo;re familiar with the concept of \u0026ldquo;pushing left\u0026rdquo; (or if you read my article about sensitive data exposure), you\u0026rsquo;ll know that when it comes to security, sometimes there isn\u0026rsquo;t a version of \u0026ldquo;later\u0026rdquo; that isn\u0026rsquo;t too late. It\u0026rsquo;s a shame, especially since following some basic security practices with high benefit yield early on in the development process doesn\u0026rsquo;t take significantly more time than not following them. Often, it comes down to having some basic but important knowledge that enables making the more secure decision.\nWhile application architecture specifics vary, there are a few basic principles we can commonly apply. This article will provide a high-level overview of areas that I hope will help point developers in the right direction.\nThere must be a reason we call it application \u0026ldquo;architecture.\u0026rdquo; I like to think it\u0026rsquo;s because the architecture of software is similar in some basic ways to the architecture of a building. (Or at least, in my absolute zero building-creation expertise, how I imagine a pretty utilitarian building to be.) Here\u0026rsquo;s how I like to summarize three basic points of building secure application architecture:\n Separated storage Customized configuration Controlled access and user scope  This is only a jumping-off point meant to get us started on the right foot; a complete picture of a fully-realized application\u0026rsquo;s security posture includes areas outside the scope of this post, including authentication, logging and monitoring, integration, and sometimes compliance.\n1. Separated storage From a security standpoint, the concept of separation refers to storing files that serve different purposes in different places. When we\u0026rsquo;re constructing our building and deciding where all the rooms go, we similarly create the lobby on the ground floor and place administrative offices on higher floors, perhaps off the main path. While both are rooms, we understand that they serve different purposes, have different functional needs, and possibly very different security requirements.\nWhen it comes to our files, the benefit is perhaps easiest for us to understand if we consider a simple file structure:\napplication/ ├───html/ │ └───index.html ├───assets/ │ ├───images/ │ │ ├───rainbows.jpg │ │ └───unicorns.jpg │ └───style.css └───super-secret-configurations/ └───master-keys.txt In our simplified example, let\u0026rsquo;s say that all our application\u0026rsquo;s images are stored in the application/assets/images/ directory. When one of our users creates a profile and uploads their picture to it, this picture is also stored in this folder. Makes sense, right? It\u0026rsquo;s an image, and that\u0026rsquo;s where the images go. What\u0026rsquo;s the issue?\nIf you\u0026rsquo;re familiar with navigating a file structure in a terminal, you may have seen this syntax before: ../../. The two dots are a handy way of saying, \u0026ldquo;go up one directory.\u0026rdquo; If we execute the command cd ../../ in the images/ directory of our simple file structure above, we\u0026rsquo;d go up into assets/, then up again to the root directory, application/. This is a problem because of a wee little vulnerability dubbed path traversal.\nWhile the dot syntax saves us some typing, it also introduces the interesting advantage of not actually needing to know what the parent directory is called in order to go to it. Consider an attack payload script, delivered into the images/ folder of our insecure application, that went up one directory using cd ../ and then sent everything it found to the attacker, on repeat. Eventually, it would reach the root application directory and access the super-secret-configurations/ folder. Not good.\nWhile other measures should be in place to prevent path traversal and related user upload vulnerabilities, the simplest prevention by far is a separation of storage. Core application files and assets should not be combined with other data, and especially not with user input. It\u0026rsquo;s best to keep user-uploaded files and activity logs (which may contain juicy data and can be vulnerable to injection attacks) separate from the main application.\nSeparation can be achieved in a few ways, such as by using a different server, different instance, separate IP range, or separate domain.\n2. Customized configuration While wasting time on customization can hinder productivity, one area that we definitely want to customize is configuration settings. Security misconfiguration is listed in the OWASP Top 10. A significant number of security incidents occur because a server, firewall, or administrative account is running in production with default settings. Upon the opening of our new building, we\u0026rsquo;d hopefully be more careful to ensure we haven\u0026rsquo;t left any keys in the locks.\nUsually, the victims of attacks related to default settings aren\u0026rsquo;t specifically targeted. Rather, they are found by automated scanning tools that attackers run over many possible targets, effectively prodding at many different systems to see if any roll over and expose some useful exploit. The automated nature of this attack means that it\u0026rsquo;s important for us to review settings for every piece of our architecture. Even if an individual piece doesn\u0026rsquo;t seem significant, it may provide a vulnerability that allows an attacker to use it as a gateway to our larger application.\nIn particular, examine architecture components for unattended areas such as:\n Default accounts, especially with default passwords, left in service; Example web pages, tutorial applications, or sample data left in the application; Unnecessary ports left in service, or ports left open to the Internet; Unrestricted permitted HTTP methods; Sensitive information stored in automated logs; Default configured permissions in managed services; and, Directory listings, or sensitive file types, left accessible by default.  This list isn\u0026rsquo;t exhaustive. Specific architecture components, such as cloud storage or web servers, will have other configurable features that should be reviewed. In general, reduce the application\u0026rsquo;s attack surface by using minimal architecture components. If we use fewer components or don\u0026rsquo;t install modules we don\u0026rsquo;t need, we\u0026rsquo;ll have fewer possible attack entry points to configure and safeguard.\n3. Controlled access and user scope One of the more difficult security problems to test in an application is misconfigured access control. Automated testing tools have limited capability to find areas of an application that one user shouldn\u0026rsquo;t be able to access. Thus, this is often left to manual testing or source code review to discover. By considering this vulnerability early on in the software development lifecycle when architectural decisions are being made, we reduce the risk that it becomes a problem that\u0026rsquo;s harder to fix later. After all, we wouldn\u0026rsquo;t simply leave our master keys out of reach on a high ledge and hope no one comes along with a ladder.\nBroken access control is listed in the OWASP Top 10, which goes into more detail on its various forms. As a simple example, consider an application with two levels of access: administrators, and users. We want to build a new feature - the ability to moderate or ban users - with the intention that only administrators would be allowed to use it.\nIf we\u0026rsquo;re aware of the possibility of access control misconfigurations or exploits, we may decide to build the moderation feature in a completely separate area from the user-accessible space, such as on a different domain, or as part of a model that users don\u0026rsquo;t share. This greatly reduces the risk that an access control misconfiguration or elevation of privilege vulnerability might allow a user to improperly access the moderation feature later on.\nOf course, robust access control in our application needs more support to be effective. Consider factors such as sensitive tokens, or keys passed as URL parameters, or whether a control fails securely or insecurely. Nevertheless, by considering authorization at the architectural stage, we can set ourselves up to make further reinforcements easier to implement.\nSecurity basics for maximum benefit Similar to avoiding racking up technical debt by choosing a well-vetted framework, developers can avoid security debt by becoming aware of common vulnerabilities and the simple architectural decisions we can make to help mitigate them. For a much more detailed resource on how to bake security into our applications from the start, the OWASP Application Security Verification Standard is a robust guide.\n",url:"https://victoria.dev/blog/secure-application-architecture-basics-separation-configuration-and-access/"},"https:\/\/victoria.dev\/neofeed\/26934\/":{title:"26934",tags:[],content:"‪My new website is up, and one of my favourite parts isn\u0026rsquo;t even prominent. It\u0026rsquo;s at the veeery bottom of the index page, where Hugo\u0026rsquo;s WordCount function prints that I\u0026rsquo;ve written (approximately) 56,208 words. 🖋✨‬\n‪Hopefully the majority of them are useful.‬\n",url:"https://victoria.dev/neofeed/26934/"},"https:\/\/victoria.dev\/neofeed\/26834\/":{title:"26834",tags:[],content:"Given how much must be overcome for humans to communicate with machines, I\u0026rsquo;m always pleasantly surprised when the thing does what I meant it to do the first time around.\n",url:"https://victoria.dev/neofeed/26834/"},"https:\/\/victoria.dev\/blog\/migrating-to-the-cloud-but-without-screwing-it-up-or-how-to-move-house\/":{title:"Migrating to the cloud but without screwing it up, or how to move house",tags:["aws","coding","data","websites"],content:"For an application that\u0026rsquo;s ready to scale, not using managed cloud architecture these days is like insisting on digging your own well for water. It\u0026rsquo;s far more labour-intensive, requires buying all your own equipment, takes a lot more time, and there\u0026rsquo;s a higher chance you\u0026rsquo;re going to get it wrong because you don\u0026rsquo;t personally have a whole lot of experience digging wells, anyway.\nThat said - let\u0026rsquo;s just get this out of the way first - there is no cloud. It\u0026rsquo;s just someone else\u0026rsquo;s computer.\nOf course, these days, cloud services go far beyond the utility we\u0026rsquo;d expect from a single computer. Besides being able to quickly set up and utilize the kind of computing power that previously required a new office lease agreement to house, there are now a multitude of monitoring, management, and analysis tools at our giddy fingertips. While it\u0026rsquo;s important to understand that the cloud isn\u0026rsquo;t a better option in every case, for applications that can take advantage of it, we can do more, do it faster, and do it for less money than if we were to insist on building our own on-premises infrastructure.\nThat\u0026rsquo;s all great, and easily said; moving to the cloud, however, can look from the outset like a pretty daunting task. How, exactly, do we go about shifting what may be years of on-premises data and built-up systems to someone else\u0026rsquo;s computer? You know, without being able to see it, touch it, and without completely screwing up our stuff.\nWhile it probably takes less work and money than setting up or maintaining the same architecture on-premise, it does take some work to move to the cloud initially. It\u0026rsquo;s important that our application is prepared to migrate, and capable of using the benefits of cloud services once it gets there. To accomplish this, and a smooth transition, preparation is key. In fact, it\u0026rsquo;s a whole lot like moving to a new house.\nIn this article, we\u0026rsquo;ll take a high-level look at the general stages of taking an on-premise or self-hosted application and moving it to the cloud. This guide is meant to serve as a starting point for designing the appropriate process for your particular situation, and to enable you to better understand the cloud migration process. While cloud migration may not be the best choice for some applications - such as ones without scalable architecture or where very high computing resources are needed - a majority of modular and modern applications stand to benefit from a move to the cloud.\nIt\u0026rsquo;s certainly possible, as I discovered at a recent event put on by Amazon Web Services (AWS) Solutions Architects, to migrate smoothly and efficiently, with near-zero loss of availability to customers. I\u0026rsquo;ll specifically reference some services provided by AWS, however, similar functionality can be found with other cloud providers. I\u0026rsquo;ve found the offerings from AWS to be pleasantly modular in scope, which is why I use them myself and why they make good examples for discussing general concepts.\nTo have our move go as smoothly as possible, here are the things we\u0026rsquo;ll want to consider:\n The type of move we\u0026rsquo;re making; The things we\u0026rsquo;ll take, and the things we\u0026rsquo;ll clean up; How to choose the right type and size for the infrastructure we\u0026rsquo;re moving into; and How to do test runs to practice for the big day.  The type of move we\u0026rsquo;re making While it\u0026rsquo;s important to understand why we\u0026rsquo;re moving our application to cloud services, we should also have an idea of what we\u0026rsquo;d like it to look like when it gets there. There are three main ways to move to the cloud: re-host, re-platform, or re-factor.\nRe-host A re-host scenario is the the most straightforward type of move. It involves no change to the way our application is built or how it runs. For example, if we currently have Python code, use PostgreSQL, and serve our application with Apache, a re-host move would mean we use all the same components, combined in just the same way, only now they\u0026rsquo;re in the cloud. It\u0026rsquo;s a lot like moving into a new house that has the exact same floor plan as the current one. All the furniture goes into the same room it\u0026rsquo;s in now, and it\u0026rsquo;s going to feel pretty familiar when we get there.\nThe main draw of a re-host move is that it may offer the least amount of complication necessary in order to take advantage of going to the cloud. Scalable applications, for example, can gain the ability to automatically manage necessary application resources.\nWhile re-hosting makes scaling more automatic, it\u0026rsquo;s important to note that it won\u0026rsquo;t in itself make an application scalable. If the application infrastructure is not organized in such a way that gives it the ability to scale, a re-factor may be necessary instead.\nRe-platform If a component of our current application set up isn\u0026rsquo;t working out well for us, we\u0026rsquo;re probably going to want to re-platform. In this case, we\u0026rsquo;re making a change to at least one component of our architecture; for example, switching our database from Oracle to MySQL on Amazon Relational Database Service (RDS).\nLike moving from a small apartment in Tokyo to an equally small apartment in New York, a re-platform doesn\u0026rsquo;t change the basic nature of our application, but does change its appearance and environment. In the database change example, we\u0026rsquo;ll have all the same data, just organized or formatted a little differently. In most cases, we won\u0026rsquo;t have to make these changes manually. A tool such as Amazon Database Migration Service (DMS) can help to seamlessly shift our data over to the new database.\nWe might re-platform in order to enable us to better meet a business demand in the future, such as scaling up, integrating with other technological components, or choosing a more modern technology stack.\nRe-factor A move in which we re-factor our application is necessarily more complicated than our other options, however, it may provide the most overall benefit for companies or applications that have reason to make this type of move. As with code, refactoring is done when fundamental changes need to be made in order for our application to meet a business need. The specifics necessarily differ case-by-case, but typically involve changes to architectural components or how those components relate to one another. This type of move may also involve changing application code in order to optimize the application\u0026rsquo;s performance in a cloud environment. We can think of it like moving out from our parent\u0026rsquo;s basement in the suburbs and getting a nice townhouse in the city. There\u0026rsquo;s no way we\u0026rsquo;re taking that ancient hand-me-down sofa, so we\u0026rsquo;ll need some new furniture, and for our neighbour\u0026rsquo;s sake, probably window dressings.\nRefactoring may enable us to modernize a dated application, or make it more efficient in general. With greater efficiency, we can better take advantage of services that cloud providers typically offer, like bursting resources or attaining deep analytical insight.\nIf a re-factor is necessary but time is scarce, it may be better to re-host or re-platform first, then re-factor later. That way, we\u0026rsquo;ll have a job well done later instead of a hasty, botched migration (and more problems) sooner.\nWhat to take, and what to clean up Over the years of living in one place, stuff tends to pile up unnoticed in nooks and crannies. When moving house, it\u0026rsquo;s usually a great opportunity to sort everything out and decide what is useful enough to keep, and what should be discarded or given away. Moving to the cloud is a similarly great opportunity to do the same when it comes to our application.\nWhile cloud storage is inexpensive nowadays, there may be some things that don\u0026rsquo;t make sense to store any longer, or at least not keep stored with our primary application. If data cannot be discarded due to policy or regulations, we may choose a different storage class to house data that we don\u0026rsquo;t expect to need anytime soon outside of our main application.\nIn the case of Amazon\u0026rsquo;s Simple Storage Service (S3), we can choose to use different storage classes that accomplish this goal. While the data that our business relies on every day can take advantage of the Standard class 99.99% availability, data meant for long-term cold storage such as archival backups can be put into the Glacier class, which has longer retrieval time and lower cost.\nThe right type and size Choosing the type and size of cloud infrastructure appropriate for our business is usually the part that can be the most confusing. How should we predict, in a new environment or for a growing company, the computing power we\u0026rsquo;ll need?\nPart of the beauty of not procuring hardware on our own is that won\u0026rsquo;t have to make predictions like these. Using cloud storage and instances, expanding or scaling back resources can be done in a matter of minutes, sometimes seconds. With managed services, it can even be done automatically for us. With the proper support for scalability in our application, it\u0026rsquo;s like having a magical house that instantly generates any type of room and amenity we need at that moment. The ability to continually ensure that we\u0026rsquo;re using appropriate, cost-effective resources is at our fingertips, and often clearly visualized in charts and dashboards.\nFor applications new to the cloud, some leeway for experimentation may be necessary. While cloud services enables us to quickly spin up and try out different architectures, there\u0026rsquo;s no guarantee that all of those set ups will work well for our application. For example, running a single instance may be less expensive than going serverless, but we\u0026rsquo;d be hard pressed to know this until we tried it out.\nAs a starting point, we simply need enough storage and computing power to support the application as it is currently running, today. For example, in the case of storage, consider the size of the current database - the actual database data, not the total storage capacity of hardware on-premises. For a detailed cost exploration, AWS even offers a Simple Monthly Calculator with use case samples to help guide expectations.\nDo test runs before the big day Running a trial cloud migration may be an odd concept, but it is an essential component to ensuring that the move goes as planned with minimal service interruption. Imagine the time and energy that would be saved in the moving house example if we could automate test runs! Invariably, some box or still-hung picture is forgotten and left out of the main truck, necessitating additional trips in other vehicles. With multiple chances to ensure we\u0026rsquo;ve got it down pat, we minimize the possibility that our move causes any break in normal day-to-day business.\nGenerally, to do a test run, we create a duplicate version of our application. The more we can duplicate, the more thorough the test run will be, especially if our data is especially large. Though duplication may seem tedious, working with the actual components we intend to migrate is essential to ensuring the migration goes as planned. After all, if we only did a moving-house test run with one box, it wouldn\u0026rsquo;t be very representative.\nTest runs can help to validate our migration plan against any challenges we may encounter. These challenges might include:\n Downtime restrictions; Encrypting data in transit and immediately when at rest on the target; Schema conversion to a new target schema (the AWS Schema Conversion Tool can also help); Access to databases, such as through firewalls or VPNs; Developing a process to ensure that all the data successfully migrated, such as by using a hash function.  Test runs also help to give us a more accurate picture of the overall time that a migration will take, as well as affording us the opportunity to fine-tune it. Factors that may affect the overall speed of a migration include:\n The sizes of the source and target instances; Available bandwidth for moving data; Schema configurations; and Transaction pressure on the source, such as changes to the data and the volume of incoming transactions.  Once the duplicate application has been migrated via one or more options, we test the heck out of the application that\u0026rsquo;s now running in the cloud to ensure it performs as expected. Ideally, on the big day, we\u0026rsquo;d follow this same general process to move up-to-date duplicate data, and then seamlessly point the \u0026ldquo;real\u0026rdquo; application or web address to the new location in the cloud. This means that our customers experience near-zero downtime; essentially, only the amount of time that the change in location-pointing would need to propagate to their device.\nIn the case of very large or complex applications with many components or many teams working together at the same time, a more gradual approach may be more appropriate than the \u0026ldquo;Big Bang\u0026rdquo; approach, and may help to mitigate risk of any interruptions. This means migrating in stages, component by component, and running tests between stages to ensure that all parts of the application are communicating with each other as expected.\nPreparation is essential to a smooth migration I hope this article has enabled a more practical understanding of how cloud migration can be achieved. With thorough preparation, it\u0026rsquo;s possible to take advantage of all the cloud has to offer, with minimal hassle to get there.\nMy thanks to the AWS Solutions Architects who presented at Pop-Up Loft and shared their knowledge on these topics, in particular: Chandra Kapireddy, Stephen Moon, John Franklin, Michael Alpaugh, and Priyanka Mahankali.\nOne last nugget of wisdom, courtesy of John: \u0026ldquo;Friends don\u0026rsquo;t let friends use DMS to create schema objects.\u0026rdquo;\n",url:"https://victoria.dev/blog/migrating-to-the-cloud-but-without-screwing-it-up-or-how-to-move-house/"},"https:\/\/victoria.dev\/neofeed\/26133\/":{title:"26133",tags:[],content:"You can learn a lot more about yourself than you really want to know by looking at your \u0026ldquo;Inspired by your browsing history\u0026rdquo; Amazon product suggestions.\n",url:"https://victoria.dev/neofeed/26133/"},"https:\/\/victoria.dev\/blog\/how-users-and-applications-stay-safe-on-the-internet-its-proxy-servers-all-the-way-down\/":{title:"How users and applications stay safe on the Internet: it\u0027s proxy servers all the way down",tags:["cybersecurity","websites"],content:"Both Internet users and Internet-connected applications can benefit from investing in cybersecurity. One core aspect of online privacy is the use of a proxy server, though this basic building block may not be initially visible underneath its more recognizable forms. Proxy servers are a useful thing to know about nowadays, for developers, software product owners, as well as the average dog on the Internet. Let\u0026rsquo;s explore what makes proxy servers an important piece of cybersecurity support.\n \u0026ldquo;On the Internet, nobody knows you\u0026rsquo;re a dog.\u0026rdquo;\n When Peter Steiner\u0026rsquo;s caption was first published in The New Yorker in 1993, it reportedly went largely unnoticed. Only later did the ominous and slightly frightening allusion to online anonymity touch the public consciousness with the icy fingers of the unknown. As Internet usage became more popular, users became concerned that other people could represent themselves online in any manner they chose, without anyone else knowing who they truly were.\nThis, to make a gross understatement, is no longer the case. Thanks to tracking cookies, browser fingerprinting, Internet Service Providers (ISPs) selling our browsing logs to advertisers, and our own inexplicable inclination to put our names and faces on social networks, online anonymity is out like last year\u0026rsquo;s LaCroix flavours. While your next-door neighbor may not know how to find you online (well, except for through that location-based secondhand marketplace app you\u0026rsquo;re using), you can be certain that at least one large advertising company has a series of zeroes and ones somewhere that represent you, the specific details of your market demographic, and all your online habits, including your preferred flavour of LaCroix.\nThere are ways to add some layers of obscurity, like using a corporate firewall that hides your IP, or using Tor. The underlying mechanism of both these methods is the same. Like being enshrouded in the layers of an onion, we\u0026rsquo;re using one or more proxy servers to shield our slightly sulfuric selves from third-party tracking.\nWhat\u0026rsquo;s a proxy server, anyway A proxy, in the traditional English definition, is the \u0026ldquo;authority or power to act for another.\u0026rdquo; (Merriam-Webster) A proxy server, in the computing context, is a server that acts on behalf of another server, or a user\u0026rsquo;s machine.\nBy using a proxy to browse the Internet, for example, a user can defer being personally identifiable. All of the user\u0026rsquo;s Internet traffic appears to come from the proxy server instead of their machine.\nProxy servers are for users There are a few ways that we, as the client, can use a proxy server to conceal our identity when we go online. It\u0026rsquo;s important to know that these methods offer differing levels of anonymity, and that no single method will really provide true anonymity; if others are actively seeking to find you on the Internet, for whatever reason, further steps should be taken to make your activity truly difficult to identify. (Those steps are beyond the scope of this article, but you can get started with the Electronic Frontier Foundation\u0026rsquo;s (EFF) Surveillance Self-Defense resource.) For the average user, however, here is a small menu of options ranging from least to most anonymous.\nUse a proxy in your web browser Certain web browsers, including Firefox and Safari on Mac, allow us to configure them to send our Internet traffic through a proxy server. The proxy server attempts to anonymize our requests by replacing our originating IP address with the proxy server\u0026rsquo;s own IP. This provides us with some anonymity, as the website we\u0026rsquo;re trying to reach will not see our originating IP address; however, the proxy server that we choose to use will know exactly who originated the request. This method also doesn\u0026rsquo;t necessarily encrypt traffic, block cookies, or stop social media and cross-site trackers from following us around; on the upside, it\u0026rsquo;s the method least likely to prevent websites that use cookies from functioning properly.\nPublic proxy servers are out there, and deciding whether or not we should use any one of them is on par with deciding whether we should eat a piece of candy handed to us by a smiling stranger. If your academic institution or company provides a proxy server address, it is (hopefully) a private server with some security in place. My preferred method, if we have a little time and a few monthly dollars to invest in our security, is to set up our own virtual instance with a company such as Amazon Web Services or Digital Ocean and use this as our proxy server.\nTo use a proxy through our browser, we can edit our Connection Settings in Firefox, or set up a proxy server using Safari on Mac.\nIn regards to choosing a browser, I would happily recommend Firefox to any Internet user who wants to beef up the security of their browsing experience right out of the box. Mozilla has been a champion of privacy-first since I\u0026rsquo;ve heard of them, and recently made some well-received changes to Enhanced Tracking Protection in Firefox Browser that blocks social media trackers, cross-site tracking cookies, fingerprinters, and cryptominers by default.\nUse a VPN on your device In order to take advantage of a proxy server for all our Internet usage instead of just through one browser, we can use a Virtual Private Network (VPN). A VPN is a service, usually paid, that sends our Internet traffic through their servers, thus acting as a proxy. A VPN can be used on our laptop as well as phone and tablet devices, and since it encompasses all our Internet traffic, it doesn\u0026rsquo;t require much extra effort to use other than ensuring our device is connected. Using a VPN is an effective way to keep nosy ISPs from snooping on our requests.\nTo use a paid, third-party VPN service, we\u0026rsquo;d usually sign up on their website and download their app. It\u0026rsquo;s important to keep in mind that whichever provider we choose, we\u0026rsquo;re entrusting them with our data. VPN providers anonymize our activity from the Internet, but can themselves see all our requests. Providers vary in terms of their privacy policies and the data they choose to log, so a little research may be necessary to determine which, if any, we are comfortable trusting.\nWe can also roll our own VPN service by using a virtual instance and OpenVPN. OpenVPN is an open source VPN protocol, and can be used with a few virtual instance providers, such as Amazon VPC, Microsoft Azure, Google Cloud, and Digital Ocean Droplets. I previously wrote a tutorial on setting up your own personal VPN service with AWS using an EC2 instance. I\u0026rsquo;ve been running this solution personally for about a month, and it\u0026rsquo;s cost me almost $4 USD in total, which is a price I\u0026rsquo;m quite comfortable paying for some peace of mind.\nUse Tor Tor takes the anonymity offered by a proxy server and compounds it by forwarding our requests through a relay network of other servers, each called a \u0026ldquo;node.\u0026rdquo; Our traffic passes through three nodes on its way to a destination: the guard, middle, and exit nodes. At each step, the request is encrypted and anonymized such that the current node only knows where to send it, and nothing more about what the request contains. This separation of knowledge means that, of the options discussed, Tor provides the most complete version of anonymity. (For a more complete explanation, see Robert Heaton\u0026rsquo;s article on how Tor works, which is so excellently done that I wish I\u0026rsquo;d written it myself.)\nThat said, this level of anonymity comes with its own cost. Not monetary, as Tor Browser is free to download and use. It is, however, slower than using a VPN or simple proxy server through a browser, due to the circuitous route our requests take.\nProxy servers are for servers too We\u0026rsquo;re now familiar with proxy servers in the context of protecting users as they surf the web, but proxies aren\u0026rsquo;t just for clients. Websites and Internet-connected applications can use reverse proxy servers for obfuscation too. The \u0026ldquo;reverse\u0026rdquo; part just means that the proxy is acting on behalf of the server, instead of the client.\nWhy would a web server care about anonymity? Generally, they don\u0026rsquo;t, at least not in the same way some users do. Web servers can benefit from using a proxy for a few different reasons; for example, they typically offer faster service to users by caching or compressing content to optimize delivery. From a cybersecurity perspective, however, a reverse proxy can improve an application\u0026rsquo;s security posture by obfuscating the underlying infrastructure.\nBasically, by placing another web server (the \u0026ldquo;proxy\u0026rdquo;) in front of the web server that directly accesses all the files and assets, we make it more difficult for an attacker to pinpoint our \u0026ldquo;real\u0026rdquo; web server and mess with our stuff. Like when you want to see the store manager and the clerk you\u0026rsquo;re talking to says, \u0026ldquo;I speak for the manager,\u0026rdquo; and you\u0026rsquo;re not really sure there even is a manager, anyway, but you successfully exchange the hot pink My Little Pony they sold you for a fuchsia one, thankyouverymuch, so now you\u0026rsquo;re no longer concerned with who the manager is and whether or not they really exist, and if you passed them on the street you would not be able to stop them and call them out for passing off hot pink as fuchsia, and the manager is just fine with that.\nSome common web servers can also act as reverse proxies, often with just a minimal and straightforward configuration change. While the best choice for your particular architecture is unknown to me, I will offer a couple common examples here.\nUsing NGINX as a reverse proxy NGINX uses the proxy_pass directive in its configuration file (nginx.conf by default) to turn itself into a reverse proxy server. The set up requires the following lines to be placed in the configuration file:\nlocation /requested/path/ { proxy_pass http://www.example.com/target/path/; } This specifies that all requests for the path /requested/path/ are forwarded to http://www.example.com/target/path/. The target can be a domain name or an IP address, the latter with or without a port.\nThe full guide to using NGINX as a reverse proxy is part of the NGINX documentation.\nUsing Apache httpd as a reverse proxy Apache httpd similarly requires some straightforward configuration to act as a reverse proxy server. In the configuration file, usually httpd.conf, set the following directives:\nProxyPass \u0026quot;/requested/path/\u0026quot; \u0026quot;http://www.example.com/target/path/\u0026quot; ProxyPassReverse \u0026quot;/requested/path/\u0026quot; \u0026quot;http://www.example.com/target/path/\u0026quot; The ProxyPass directive ensures that all requests for the path /requested/path/ are forwarded to http://www.example.com/target/path/. The ProxyPassReverse directive ensures that the headers sent by the web server are modified to point to the reverse proxy server instead.\nThe full reverse proxy guide for Apache HTTP server is available in their documentation.\nProxy servers most of the way down I concede that my title is a little facetious, as cybersecurity best practices aren\u0026rsquo;t really some eternal infinite-regression mystery (though they may sometimes seem to be). Regardless, I hope this post has helped in your understanding of what proxy servers are, how they contribute to online anonymity for both clients and servers, and that they are an integral building block of cybersecurity practices.\nIf you\u0026rsquo;d like to learn more about personal best practices for online security, I highly recommend exploring the articles and resources provided by EFF. For a guide to securing web sites and applications, the OWASP Cheat Sheet Series is a fantastic resource.\n",url:"https://victoria.dev/blog/how-users-and-applications-stay-safe-on-the-internet-its-proxy-servers-all-the-way-down/"},"https:\/\/victoria.dev\/neofeed\/25332\/":{title:"25332",tags:[],content:"This, in fact, was my first post written entirely in Vim! I typically use Visual Studio Code but recently decided to become more comfortable with keyboard shortcuts. In the process I got more accustomed to visual mode, and finally got around to figuring out buffers. 😅\nRemarkably, now when I go back to VSC it feels slow!\nhttps://victoria.dev/verbose/sql-injection-and-xss-what-white-hat-hackers-know-about-trusting-user-input/\n",url:"https://victoria.dev/neofeed/25332/"},"https:\/\/victoria.dev\/blog\/hackers-are-googling-your-plain-text-passwords-preventing-sensitive-data-exposure\/":{title:"Hackers are Googling your plain text passwords: preventing sensitive data exposure",tags:["cybersecurity","privacy","data","websites"],content:"Last week, I wrote about the importance of properly handling user input in our websites and applications. I alluded to an overarching security lesson that I hope to make explicit today: the security of our software, application, and customer data is built from the ground up, long before the product goes live.\nThe OWASP Top 10 is a comprehensive guide to web application security risks. It is relied upon by technology professionals, corporations, and those interested in cybersecurity or information security. The most recent publication lists Sensitive Data Exposure as the third most critical web application security risk. Here\u0026rsquo;s how the risk is described:\n Many web applications and APIs do not properly protect sensitive data, such as financial, healthcare, and PII. Attackers may steal or modify such weakly protected data to conduct credit card fraud, identity theft, or other crimes. Sensitive data may be compromised without extra protection, such as encryption at rest or in transit, and requires special precautions when exchanged with the browser.\n \u0026ldquo;Sensitive Data Exposure\u0026rdquo; is a sort of catch-all category for leaked data resulting from many sources, ranging from weak cryptographic algorithms to unenforced encryption. The simplest source of this security risk, however, takes far fewer syllables to describe: people.\nThe phrase \u0026ldquo;an ounce of prevention is worth a pound of cure,\u0026rdquo; applies to medicine as well as secure software development. In the world of the latter, this is referred to as \u0026ldquo;pushing left,\u0026rdquo; a rather unintuitive term for establishing security best practices earlier, rather than later, in the software development life cycle (SDLC). Establishing procedures \u0026ldquo;to the left\u0026rdquo; of the SDLC can help ensure that the people involved in creating a software product are properly taking care of sensitive data from day one.\nUnfortunately, a good amount of security testing often seems to occur much farther to the right side of the SDLC; too late for some security issues, such as sensitive data leakage, to be prevented.\nI\u0026rsquo;m one of the authors contributing to the upcoming OWASP Testing Guide and recently expanded a section on search engine discovery reconnaissance, or what the kids these days call \u0026ldquo;Google dorking.\u0026rdquo; This is one method, and arguably the most accessible method, by which a security tester (or black hat hacker) could find exposed sensitive data on the Internet. Here\u0026rsquo;s an excerpt from that section (currently a work in progress on GitHub, to be released in v5):\n Search Operators A search operator is a special keyword that extends the capabilities of regular search queries, and can help obtain more specific results. They generally take the form of operator:query. Here are some commonly supported search operators:\n site: will limit the search to the provided URL. inurl: will only return results that include the keyword in the URL. intitle: will only return results that have the keyword in the page title. intext: or inbody: will only search for the keyword in the body of pages. filetype: will match only a specific filetype, i.e. png, or php.  For example, to find the web content of owasp.org as indexed by a typical search engine, the syntax required is:\nsite:owasp.org\n\u0026hellip; Searching with operators can be a very effective discovery reconnaissance technique when combined with the creativity of the tester. Operators can be chained to effectively discover specific kinds of sensitive files and information. This technique, called Google hacking or Google dorking, is also possible using other search engines, as long as the search operators are supported.\nA database of dorks, such as Google Hacking Database, is a useful resource that can help uncover specific information.\n Regularly reviewing search engine results can be a fruitful task for security testers. However, when a search for site:myapp.com passwords turns up no results, it may still be a little too early to break for lunch. Here are a couple other places a security tester might like to look for sensitive data exposed in the wild.\nPastebin The self-declared \u0026ldquo;#1 paste tool since 2002,\u0026rdquo; Pastebin allows users to temporarily store any kind of text. It\u0026rsquo;s mostly used for sharing information with others, or retrieving your own \u0026ldquo;paste\u0026rdquo; on another machine, perhaps in another location. Pastebin makes it easy to share large amounts of complicated text, like error logs, source code, configuration files, tokens, api keys\u0026hellip; what\u0026rsquo;s that? Oh, yes, it\u0026rsquo;s public by default.\nHere are some screenshots of a little dorking I did for a public bug bounty program.\n API keys in plain view.\n   Log-in details out in the open.\n  Thanks in part to the convenience of using Pastebin and similar websites, it would appear that some people fail to think twice before making sensitive data publicly available.\nBut why Granted, non-technical employees with access to the application may not have an understanding of which items should or should not be freely shared. Someone unfamiliar with what encrypted data is or what it looks like may not realize the difference between an encrypted string and an unencrypted token made up of many random letters and numbers. Even technical staff can miss things, make mistakes, or act carelessly after a hard day at work. It may be easy to call this a training problem and move on; however, none of these rationalizations address the root cause of the issue.\nWhen people turn to outside solutions for an issue they face, it\u0026rsquo;s usually because they haven\u0026rsquo;t been provided with an equally-appealing internal solution, or are unaware that one exists. Employees using pastes to share or move sensitive data do so because they don\u0026rsquo;t have an easier, more convenient, and secure internal solution to use instead.\nMitigation Everyone involved in the creation and maintenance of a web application should be briefed on a few basic things in regards to sensitive data protection:\n what constitutes sensitive data, the difference between plain text and encrypted data, and how to properly transmit and store sensitive data.  When it comes to third-party services, ensure people are aware that some transmission may not be encrypted, or may be publicly searchable. If there is no system currently in place for safely sharing and storing sensitive data internally, this is a good place to start. The security of application data is in the hands of everyone on the team, from administrative staff to C-level executives. Ensure people have the tools they need to work securely.\nPublic repositories Developers are notorious for leaving sensitive information hanging out where it doesn\u0026rsquo;t belong (yes, I\u0026rsquo;ve done it too!). Without a strong push-left approach in place for handling tokens, secrets, and keys, these little gems can end up in full public view on sites like GitHub, GitLab, and Bitbucket (to name a few). A 2019 study found that thousands of new, unique secrets are leaked every day on GitHub alone.\n GitHub has implemented measures like token scanning, and GitLab 11.9 introduced secret detection. While these tools aim to reduce the chances that a secret might accidentally be committed, to put it bluntly, it\u0026rsquo;s really not their job. Secret scanning won\u0026rsquo;t stop developers from committing the data in the first place.\nBut why Without an obvious process in place for managing secrets, developers may tend too much towards their innate sense of just-get-it-done-ness. Sometimes this leads to the expedient but irresponsible practice of storing keys as unencrypted variables within the program, perhaps with the intention of it being temporary. Nonetheless, these variables inevitably fall from front of mind and end up in a commit.\nMitigation Having a strong push-left culture means ensuring that sensitive data is properly stored and can be securely retrieved long before anyone is ready to make a commit. Tools and strategies for doing so are readily available for those who seek them. Here are some examples of tools that can support a push-left approach:\n Use a management tool to store and control access to keys and secrets, such as Amazon Key Management Service or Microsoft\u0026rsquo;s Azure Key Vault. Make use of encrypted environment variables in CI tools, such as Netlify\u0026rsquo;s environment variables or virtual environments in GitHub Actions. Craft a robust .gitignore file that everyone on the team can contribute to and use.  We also need not rely entirely on the public repository to catch those mistakes that may still slip through. It\u0026rsquo;s possible to set up Git pre-commit hooks that scan for committed secrets using regular expressions. There are some open-source programs available for this, such as Talisman from ThoughtWorks and git-secrets from AWS Labs.\nPushing left to prevent sensitive data exposure A little perspective can go a long way in demonstrating why it\u0026rsquo;s important to begin managing sensitive data even before any sensitive data exists. By establishing security best practices on the left of the SDLC, we give our people the best chance to increase the odds that any future dorking on our software product looks more like this.\nAnother great resource for checking up on the security of our data is Troy Hunt\u0026rsquo;s Have I Been Pwned, a service that compares your data (such as your email) to data that has been leaked in previous data breaches.\nTo learn about more ways we can be proactive with our application security, the OWASP Proactive Controls publication is a great resource. There\u0026rsquo;s also more about creating a push-left approach to security in the upcoming OWASP Testing Guide. If these topics interest you, I encourage you to read, learn, and contribute so more people will make it harder for sensitive data to be found.\n",url:"https://victoria.dev/blog/hackers-are-googling-your-plain-text-passwords-preventing-sensitive-data-exposure/"},"https:\/\/victoria.dev\/blog\/sql-injection-and-xss-what-white-hat-hackers-know-about-trusting-user-input\/":{title:"SQL injection and XSS: what white hat hackers know about trusting user input",tags:["cybersecurity","data","websites"],content:"Software developers have a lot on their minds. There are are myriad of questions to ask when it comes to creating a website or application: What technologies will we use? How will the architecture be set up? What functions do we need? What will the UI look like? Especially in a software market where shipping new apps seems more like a race for reputation than a well-considered process, one of the most important questions often falls to the bottom of the \u0026ldquo;Urgent\u0026rdquo; column: how will our product be secured?\nIf you\u0026rsquo;re using a robust, open-source framework for building your product (and if one is applicable and available, why wouldn\u0026rsquo;t you?) then some basic security concerns, like CSRF tokens and password encryption, may already be handled for you. Still, fast-moving developers would be well served to brush up on their knowledge of common threats and pitfalls, if only to avoid some embarrass ing rookie mistakes. Usually, the weakest point in the security of your software is you.\nI\u0026rsquo;ve recently become more interested in information security in general, and practicing ethical hacking in particular. An ethical hacker, sometimes called \u0026ldquo;white hat\u0026rdquo; hacker, and sometimes just \u0026ldquo;hacker,\u0026rdquo; is someone who searches for possible security vulnerabilities and responsibly (privately) reports them to project owners. By contrast, a malicious or \u0026ldquo;black hat\u0026rdquo; hacker, also called a \u0026ldquo;cracker,\u0026rdquo; is someone who exploits these vulnerabilities for amusement or personal gain. Both white hat and black hat hackers might use the same tools and resources, and generally try to get into places they aren\u0026rsquo;t supposed to be; however, white hats do this with permission, and with the intention of fortifying defences instead of destroying them. Black hats are the bad guys.\nWhen it comes to learning how to find security vulnerabilities, it should come as no surprise that I\u0026rsquo;ve been devouring whatever information I can get my hands on; this post is a distillation of some key areas that are specifically helpful to developers when handling user input. These lessons have been collectively gleaned from these excellent resources:\n The Open Web Application Security Project guides The Hacker101 playlist from HackerOne\u0026rsquo;s YouTube channel Web Hacking 101 by Peter Yaworski Brute Logic\u0026rsquo;s blog The Computerphile YouTube channel Videos featuring Jason Haddix (@jhaddix) and Tom Hudson (@tomnomnom) (two accomplished ethical hackers with different, but both effective, methodologies)  You may be familiar with the catchphrase, \u0026ldquo;sanitize your inputs!\u0026rdquo; However, as I hope this post demonstrates, developing an application with robust security isn\u0026rsquo;t quite so straightforward. I suggest an alternate phrase: pay attention to your inputs. Let\u0026rsquo;s elaborate by examining the most common attacks that take advantage of vulnerabilities in this area: SQL injection and cross site scripting.\nSQL injection attacks If you\u0026rsquo;re not yet familiar with SQL (Structured Query Language) injection attacks, or SQLi, here is a great explain-like-I\u0026rsquo;m-five video on SQLi. You may already know of this attack from xkcd\u0026rsquo;s Little Bobby Tables. Essentially, malicious actors may be able to send SQL commands that affect your application through some input on your site, like a search box that pulls results from your database. Sites coded in PHP can be especially susceptible to these, and a successful SQL attack can be devastating for software that relies on a database (as in, your Users table is now a pot of petunias).\n You have no chance to survive make your time.\n  You can test your own site to see if you\u0026rsquo;re susceptible to this kind of attack. (Please only test sites that you own, since running SQL injections where you don\u0026rsquo;t have permission to be doing so is, possibly, illegal in your locality; and definitely, universally, not very funny.) The following payloads can be used to test inputs:\n ' OR 1='1 evaluates to a constant true, and when successful, returns all rows in the table. ' AND 0='1 evaluates to a constant false, and when successful, returns no rows.  This video demonstrates the above tests, and does a great job of showing how impactful an SQL injection attack can be.\nThankfully, there are ways to mitigate SQL injection attacks, and they all boil down to one basic concept: don\u0026rsquo;t trust user input.\nSQL injection mitigation In order to effectively mitigate SQL injections, developers must prevent users from being able to successfully submit raw SQL commands to any part of the site.\nSome frameworks will do most of the heavy lifting for you. For example, Django implements the concept of Object-Relational Mapping, or ORM, with its use of QuerySets. We can think of these as wrapper functions that help your application query the database using pre-defined methods that avoid the use of raw SQL.\nBeing able to use a framework, however, is never a guarantee. When dealing directly with a database, there are other methods we can use to safely abstract our SQL queries from user input, though they vary in efficacy. These are, by order of most to least preferred, and with links to relevant examples:\n Prepared statements with variable binding (or parameterized queries), Stored procedures; and Whitelisting or escaping user input.  If you want to implement the above techniques, the linked cheatsheets are a great starting point for digging deeper. Suffice to say, the use of these techniques to obtain data instead of using raw SQL queries helps to minimize the chances that SQL will be processed by any part of your application that takes input from users, thus mitigating SQL injection attacks.\nThe battle, however, is only half won\u0026hellip;\nCross Site Scripting (XSS) attacks If you\u0026rsquo;re a malicious coder, JavaScript is pretty much your best friend. The right commands will do anything a legitimate user could do (and even some things they aren\u0026rsquo;t supposed to be able to) on a web page, sometimes without any interaction on the part of an actual user. Cross Site Scripting attacks, or XSS, occur when JavaScript code is injected into a web page and changes that page\u0026rsquo;s behavior. Its effects can range from prank nuisance occurrences to more severe authentication bypasses or credential stealing. This incident report from Apache in 2010 is a good example of how XSS can be chained in a larger attack to take over accounts and machines.\n The annual DOM dance-off receives an unexpected guest);\n  XSS can occur on the server or on the client side, and generally comes in three flavors: DOM (Document Object Model) based, stored, and reflected XSS. The differences amount to where the attack payload is injected into the application.\nDOM-based XSS DOM-based XSS occurs when a JavaScript payload affects the structure, behavior, or content of the web page the user has loaded in their browser. These are most commonly executed through modified URLs, such as in phishing.\nTo see how easy it would be for injected JavaScript to manipulate a page, we can create a working example with an HTML web page. Try creating a file on your local system called xss-test.html (or whatever you like) with the following HTML and JavaScript code:\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;My XSS Example\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1 id=\u0026#34;greeting\u0026#34;\u0026gt;Hello there!\u0026lt;/h1\u0026gt; \u0026lt;script\u0026gt; var name = new URLSearchParams(document.location.search).get(\u0026#39;name\u0026#39;); if (name !== \u0026#39;null\u0026#39;) { document.getElementById(\u0026#39;greeting\u0026#39;).innerHTML = \u0026#39;Hello \u0026#39; + name + \u0026#39;!\u0026#39;; } \u0026lt;/script\u0026gt; \u0026lt;/h1\u0026gt; \u0026lt;/html\u0026gt; This web page will display the title \u0026ldquo;Hello there!\u0026rdquo; unless it receives a URL parameter from a query string with a value for name. To see the script work, open the page in a browser with an appended URL parameter, like so:\nfile:///path/to/file/xss-test.html?name=Victoria\nFun, right? Our insecure (in the safety sense, not the emotional one) page takes the URL parameter value for name and displays it in the DOM. The page is expecting the value to be a nice friendly string, but what if we change it to something else? Since the page is owned by us and only exists on our local system, we can test it all we like. What happens if we change the name parameter to, say, \u0026lt;img+src+onerror=alert(\u0026quot;pwned\u0026quot;)\u0026gt;?\nThis is just one example, largely based on one from Brute\u0026rsquo;s post, that demonstrates how an XSS attack could be executed. Funny pop-up alerts may be amusing, but JavaScript can do a lot of harm, including helping malicious attackers steal passwords and personal information.\nStored and reflected XSS Stored XSS occurs when the attack payload is stored on the server, such as in a database. The attack affects a victim whenever that stored data is retrieved and rendered in the browser. For example, instead of using a URL query string, an attacker might update their profile page on a social site to include a hidden script in, say, their \u0026ldquo;About Me\u0026rdquo; section. The script, improperly stored on the site\u0026rsquo;s server, would successfully execute at a later time when another user views the attacker\u0026rsquo;s profile.\nOne of the most famous examples of this is the Samy worm that all but took over MySpace in 2005. It propagated by sending HTTP requests that replicated it onto a victim\u0026rsquo;s profile page whenever an infected profile was viewed. Within just 20 hours, it had spread to over a million users.\nReflected XSS similarly occurs when the injected payload travels to the server, however, the malicious code does not end up stored in a database. It is instead immediately returned to the browser by the web application. An attack like this might be executed by luring the victim to click a malicious link that sends a request to the vulnerable website\u0026rsquo;s server. The server would then send a response to the attacker as well as the victim, which may result in the attacker being able to obtain passwords, or perpetrate actions that appear to originate from the victim.\nXSS attack mitigation In all of these cases, XSS attacks can be mitigated with two key strategies: validating form fields, and avoiding the direct injection of user input on the web page.\nValidating form fields Frameworks can again help us out when it comes to making sure that user-submitted forms are on the up-and-up. One example is Django\u0026rsquo;s built-in Field classes, which provide fields that validate to some commonly used types and also specify sane defaults. Django\u0026rsquo;s EmailField, for instance, uses a set of rules to determine if the input provided is a valid email. If the submitted string has characters in it that are not typically present in email addresses, or if it doesn\u0026rsquo;t imitate the common format of an email address, then Django won\u0026rsquo;t consider the field valid and the form will not be submitted.\nIf relying on a framework isn\u0026rsquo;t an option, we can implement our own input validation. This can be accomplished with a few different techniques, including type conversion, for example, ensuring that a number is of type int(); checking minimum and maximum range values for numbers and lengths for strings; using a pre-defined array of choices that avoids arbitrary input, for example, months of the year; and checking data against strict regular expressions.\nThankfully, we needn\u0026rsquo;t start from scratch. Open source resources are available to help, such as the OWASP Validation Regex Repository, which provides patterns to match against for some common forms of data. Many programming languages offer validation libraries specific to their syntax, and we can find plenty of these on GitHub. Additionally, the XSS Filter Evasion Cheat Sheet has a couple suggestions for test payloads we can use to test our existing applications.\nWhile it may seem tedious, properly implemented input validation can protect our application from being susceptible to XSS.\nAvoiding direct injection Elements of an application that directly return user input to the browser may not, on a casual inspection, be obvious. We can determine areas of our application that may be at risk by exploring a few questions:\n How does data flow through our application? What does a user expect to happen when they interact with this input? Where on our page does data appear? Does it become embedded in a string or an attribute?  Here are some sample payloads that we can play with in order to test inputs on our site (again, only our own site!) courtesy of Hacker101. The successful execution of any of these samples can indicate a possible XSS vulnerability due to direct injection.\n \u0026quot;\u0026gt;\u0026lt;h1\u0026gt;test\u0026lt;/h1\u0026gt; '+alert(1)+' \u0026quot;onmouserover=\u0026quot;alert(1) http://\u0026quot;onmouseover=\u0026quot;alert(1)  As a general rule, if you are able to design around directly injecting input, do so. Alternatively, be sure to completely understand the effect of the methods you choose; for example, using innerText instead of innerHTML in JavaScript will ensure that content will be set as plain text instead of (potentially vulnerable) HTML.\nPay attention to your inputs Software developers are at a marked disadvantage when it comes to competing with black hat, or malicious, hackers. For all the work we do to secure each and every input that could potentially compromise our application, an attacker need only find the one we missed. It\u0026rsquo;s like installing deadbolts on all the doors, but leaving a window open!\nBy learning to think along the same lines as an attacker, however, we can better prepare our software to stand up against bad actors. Exciting as it may be to ship features as quickly as possible, we\u0026rsquo;ll avoid racking up a lot of security debt if we take the time beforehand to think through our application\u0026rsquo;s flow, follow the data, and pay attention to our inputs.\n",url:"https://victoria.dev/blog/sql-injection-and-xss-what-white-hat-hackers-know-about-trusting-user-input/"},"https:\/\/victoria.dev\/neofeed\/24231\/":{title:"24231",tags:[],content:"While not the brightest of the bunch, Bob\u0026rsquo;s rendition of \u0026ldquo;I will always love you\u0026rdquo; at karaoke night had everyone blown away.\n",url:"https://victoria.dev/neofeed/24231/"},"https:\/\/victoria.dev\/neofeed\/24230\/":{title:"24230",tags:[],content:"Well, my afternoon is booked.\n",url:"https://victoria.dev/neofeed/24230/"},"https:\/\/victoria.dev\/neofeed\/30227\/":{title:"30227",tags:[],content:"I prefer, \u0026ldquo;move at a reasonably maintainable fast speed while breaking things and then fixing them\u0026rdquo; but I guess that\u0026rsquo;s too long to be a sticker.\n",url:"https://victoria.dev/neofeed/30227/"},"https:\/\/victoria.dev\/neofeed\/30027\/":{title:"30027",tags:[],content:"TIL: https://tools.ietf.org/html/rfc1178\nMy next computer will be named \u0026ldquo;whos-on-first\u0026rdquo;.\n",url:"https://victoria.dev/neofeed/30027/"},"https:\/\/victoria.dev\/blog\/how-to-set-up-openvpn-on-aws-ec2-and-fix-dns-leaks-on-ubuntu-18.04-lts\/":{title:"How to set up OpenVPN on AWS EC2 and fix DNS leaks on Ubuntu 18.04 LTS",tags:["aws","cybersecurity","linux"],content:"Rolling your own Virtual Private Network (VPN) is complicated. I put a lot of time and effort into setting up OpenVPN on my own AWS EC2 instance, yet was still delighted to outsource the ongoing maintenance when I found my preferred privacy-focused VPN.\nThat said, there\u0026rsquo;s no better way to strive for maximum privacy than a VPN service you control, configure, and maintain yourself. Here\u0026rsquo;s a step-by-step tutorial for setting up your own OpenVPN on AWS EC2, and how to check for and fix DNS leaks.\nSet up OpenVPN on AWS EC2 This post will cover how to set up the OpenVPN Access Server product on AWS Marketplace, running on an Amazon EC2 instance. Then, you\u0026rsquo;ll look at how to fix a known NetworkManager bug in Ubuntu 18.04 that might cause DNS leaks. The whole process should take about fifteen minutes, so grab a ☕ and let\u0026rsquo;s be configuration superheroes.\nNote: IDs and IP addresses shown for demonstration in this tutorial are invalid.\n1. Launch the OpenVPN Access Server on AWS Marketplace The OpenVPN Access Server is available on AWS Marketplace. The Bring Your Own License (BYOL) model doesn\u0026rsquo;t actually require a license for up to two connected devices; to connect more clients, you can get bundled billing for five, ten, or twenty-five clients, or purchase a minimum of ten OpenVPN licenses a la carte for $15/device/year. For most of us, the two free connected devices will suffice; and if using an EC2 Micro instance, your set up will be AWS Free Tier eligible as well.\nStart by clicking Continue to Subscribe for the OpenVPN Access Server, which will bring you to a page that looks like this:\nClick Continue to Configuration.\nYou may notice that the EC2 instance type in the right side bar (and consequently, the Monthly Estimate) isn\u0026rsquo;t the one you want - that\u0026rsquo;s okay, you can change it soon. Just ensure that the Region chosen is where you want the instance to be located. Generally, the closer it is to the physical location of your client (your laptop, in this case), the faster your VPN will be. Click Continue to Launch.\nOn this page, you\u0026rsquo;ll change three things:\n1. The EC2 Instance type Different types of EC2 (Elastic Compute Cloud) instances will offer you different levels of computing power. If you plan to use your instance for something more than just this VPN, you may want to choose something with higher memory or storage capacity, depending on how you plan to use it. You can view each instance offering on the Amazon EC2 Instance Types page.\nFor simple VPN use, the t2.nano or t2.micro instances are likely sufficient. Only the Micro instance is Free Tier eligible.\n2. The Security Group settings A Security Group is a profile, or collection of settings, that Amazon uses to control access to your instance. If you\u0026rsquo;ve set up other AWS products before, you may already have some groups with their own rules defined. You should be careful to understand the reasons for your Security Group settings, as these define how public or private your instance is, and consequently, who has access to it.\nIf you click Create New Based on Seller Settings, the OpenVPN server defines some recommended settings for a default Security Group.\nThe default recommended settings are all 0.0.0.0/0 for TCP ports 22, 943, 443, and 945, and UDP port 1194. OpenVPN offers an explanation of how the ports are used on their website. With the default settings, all these ports are left open to support various features of the OpenVPN server. You may wish to restrict access to these ports to a specific IP address or block of addresses (like that of your own ISP) to increase the security of your instance. However, if your IP address frequently changes (like when you travel and connect to a different WiFi network), restricting the ports may not be as helpful as you hope.\nIn any case, your instance will require SSH keys to connect to, and the OpenVPN server will be password protected. Unless you have other specific security goals, it\u0026rsquo;s fine to accept the default settings for now.\nLet\u0026rsquo;s give the Security Group a name and brief description, so you know what it\u0026rsquo;s for. Then click Save.\n3. The Key Pair settings The aforementioned SSH keys are access credentials that you\u0026rsquo;ll use to connect to your instance. You can create a key pair in this section, or you can choose a key pair you may already be using with AWS.\nTo create a new set of access credentials, click Create a key pair in EC2 to open a new window. Then, click the Create Key Pair blue button. Once you give your key pair a name, it will be created and the private key will automatically download to your machine. It\u0026rsquo;s a file ending with the extension .pem. Store this key in a secure place on your computer. You\u0026rsquo;ll need to refer to it when you connect to your new EC2 instance.\nYou can return to the previous window to select the key pair you just created. If it doesn\u0026rsquo;t show up, hit the little \u0026ldquo;refresh\u0026rdquo; icon next to the drop-down. Once it\u0026rsquo;s selected, hit the shiny yellow Launch button.\nYou should see a message like this:\nGreat stuff! Now that your instance exists, let\u0026rsquo;s make sure you can access it and start up your VPN. For a shortcut to the next step, click on the \u0026ldquo;EC2 Console\u0026rdquo; link in the success message.\n2. Associate an Elastic IP Amazon\u0026rsquo;s Elastic IP Addresses provides you with a public IPv4 address controlled by your account, unlike the public IP address tied to your EC2 instance. It\u0026rsquo;s considered a best practice to create one and associate it with your VPN instance. If anything should go wrong with your instance, or if you want to use a new instance for your VPN in the future, the Elastic IP can be disassociated from the current instance and reassociated with your new one. This makes the transition seamless for your connected clients. Think of the Elastic IP like a web domain address that you register - you can point it at whatever you choose.\nWe can create a new Elastic IP address on the Amazon EC2 Console. If you clicked the link from the success message above, we\u0026rsquo;re already there.\nIf you have more than one instance, take note of the Instance ID of the one you\u0026rsquo;ve just launched.\nIn the left sidebar under Network \u0026amp; Security, choose Elastic IPs. Then click the blue Allocate new address button.\nChoose Amazon Pool, then click Allocate.\nSuccess! Click Close to return to the Elastic IP console.\nNow that you have an Elastic IP, let\u0026rsquo;s associate it with your instance. Select the IP address, then click Actions, and choose Associate address.\nEnsure the Instance option is selected, then click the drop-down menu. You should see your EC2 instance ID there. Select it, then click Associate.\nSuccess! Now that you\u0026rsquo;ll be able to access your VPN instance, let\u0026rsquo;s get your VPN service up and running.\n3. Initialize OpenVPN on the EC2 server First, you\u0026rsquo;ll need to connect to the EC2 instance via your terminal. You\u0026rsquo;ll use the private key you created earlier.\nOpen a new terminal window and navigate to the directory containing the private key .pem file. You\u0026rsquo;ll need to set its permissions with:\nsudo chmod 400 \u0026lt;name\u0026gt;.pem Be sure to substitute \u0026lt;name\u0026gt; with the name of your key.\nThis sets the file permissions to -r-------- so that it can only be read by the user (you). It may help to protect the private key from read and write operations by other users, but more importantly, will prevent AWS from throwing an error when you try to connect to your instance.\nWe can now do just that by running:\nssh -i \u0026lt;name\u0026gt;.pem openvpnas@\u0026lt;elastic ip\u0026gt; The user openvpnas is set up by the OpenVPN Access Server to allow you to connect to your instance. Replace \u0026lt;elastic ip\u0026gt; with the Elastic IP address you just associated.\nWe may get a message saying that the authenticity of your host can\u0026rsquo;t be established. As long as you\u0026rsquo;ve typed the Elastic IP correctly, go ahead and answer yes to the prompt.\nUpon the initial connection to the OpenVPN instance, a set up wizard called Initial Configuration Tool should automatically run. (If, for some reason, it doesn\u0026rsquo;t, or you panic-mashed a button, you can restart it with sudo ovpn-init –ec2.) You\u0026rsquo;ll be asked to accept the agreement, then the wizard will help to walk you through some configuration settings for your VPN server.\nYou may generally accept the default settings, however, there are a couple questions you may like to answer knowledgeably. They are:\nShould client traffic be routed by default through the VPN?\nShould client DNS traffic be routed by default through the VPN?\nThese answers depend on your privacy goals for your VPN.\nWhen asked for your OpenVPN-AS license key, you can leave it blank to use the VPN with up to two clients. If you\u0026rsquo;ve purchased a key, enter it here.\nOnce the configuration wizard finishes running, you should see the message \u0026ldquo;Initial Configuration Complete!\u0026rdquo; Before you move on, you should set a password for your server\u0026rsquo;s administration account. To do this, run:\nsudo passwd openvpn Then enter your chosen password twice. Now we\u0026rsquo;re ready to get connected!\nTo close the SSH connection, type exit.\n4. Connect the client to the VPN To connect your client (in this case, your laptop) to the VPN and start reaping the benefits, you\u0026rsquo;ll need to do two things; first, obtain your connection profile; second, install the openvpn daemon.\n1. Get your .ovpn connection profile You\u0026rsquo;ll need to download a connection profile; this is like a personal configuration file with information, including keys, that the VPN server will need to allow your connection. You can do this by logging in with the password you just set at your Elastic IP address, port 943. This looks like:\nhttps://\u0026lt;elastic ip\u0026gt;:943/ The https part is important; without it, the instance won\u0026rsquo;t send any data.\nWhen you go to this URL, you may see a page warning you that this site\u0026rsquo;s certificate issuer is unknown or invalid. As long as you\u0026rsquo;ve typed your Elastic IP correctly, it\u0026rsquo;s safe to proceed. If you\u0026rsquo;re using Firefox, click Advanced, and then Accept the Risk and Continue. In Chrome, click Advanced, then Proceed to the elastic IP.\nLog in with the username openvpn and the password you just set. You\u0026rsquo;ll now be presented with a link to download your user-locked connection profile:\nWhen you click the link, a file named client.ovpn will download.\n2. Install and start openvpn on your Ubuntu 18.04 client The openvpn daemon will allow your client to connect to your VPN server. It can be installed through the default Ubuntu repositories. Run:\nsudo apt install openvpn In order for OpenVPN to automatically start when you boot up your computer, you\u0026rsquo;ll need to rename and move the connection profile file. I suggest using a symlink to accomplish this, as it leaves your original file more easily accessible for editing, and allows you to store it in any directory you choose. You can create a symlink by running this command in the directory where your file is located:\nsudo ln -s client.ovpn /etc/openvpn/\u0026lt;name\u0026gt;.conf This creates a symbolic link for the connection profile in the appropriate folder for systemd to find it. The \u0026lt;name\u0026gt; can be anything. When the Linux kernel has booted, systemd is used to initialize the services and daemons that the user has set up to run; one of these will now be OpenVPN. Renaming the file with the extension .conf will let the openvpn daemon know to use it as your connection file.\nFor now, you can manually start and connect to OpenVPN by running:\nsudo openvpn --config client.ovpn You\u0026rsquo;ll be asked for a username and password, which will be the same credentials you used before. Once the service finishes starting up, you\u0026rsquo;ll see \u0026ldquo;Initialization Sequence Complete.\u0026rdquo; If you now visit the DNS leak test website, you should see the Elastic IP and the location of your EC2 server. Yay!\nIf you\u0026rsquo;re on a later version of Ubuntu, you may check for DNS leaks by clicking on one of the test buttons. If all the ISPs shown are Amazon and none are your own service provider\u0026rsquo;s, congratulations! No leaks! You can move on to Step 3 in the second section below, after which, you\u0026rsquo;ll be finished.\nIf you\u0026rsquo;re using Ubuntu 18.04 LTS, however, we\u0026rsquo;re not yet done.\nWhat a DNS leak looks like Sites like the DNS leak test website can help you check your configuration and see if the Internet knows more about your location than you\u0026rsquo;d like. On the main page you\u0026rsquo;ll see a big hello, your IP address, and your location, so far as can be determined.\nIf you have a DNS leak, you can see what it looks like by clicking on one of the test buttons on the the DNS leak test page. When you do, you\u0026rsquo;ll see not only your Amazon.com IP addresses, but also your own ISP and location.\nYou can also see the leak by running systemd-resolve --status in your terminal. Your results will contain two lines under different interfaces that both have entries for DNS Servers. It\u0026rsquo;ll look something like this:\nLink 7 (tun0) Current Scopes: DNS LLMNR setting: yes MulticastDNS setting: no DNSSEC setting: no DNSSEC supported: no DNS Servers: 172.31.0.2 DNS Domain: ~. Link 3 (wlp4s0) Current Scopes: none LLMNR setting: yes MulticastDNS setting: no DNSSEC setting: no DNSSEC supported: no DNS Servers: 192.168.0.1 DNS Domain: ~. The DNS leak problem in Ubuntu 18.04 stems from Ubuntu\u0026rsquo;s DNS resolver, systemd-resolved, failing to properly handle your OpenVPN configuration. In order to try and be a good, efficient DNS resolver, systemd-resolved will send DNS lookup requests in parallel to each interface that has a DNS server configuration, and then utilizes the fastest response. In your case, you only want to use your VPN\u0026rsquo;s DNS servers. Sorry, systemd-resolved. You tried.\nHow to fix OpenVPN DNS leak on Ubuntu 18.04 Luckily, there is a fix that you can implement. You\u0026rsquo;ll need to install a few helpers from the Ubuntu repositories, update your configuration file, then set up OpenVPN using NetworkManager. Let\u0026rsquo;s do it!\n1. Install some helpers To properly integrate OpenVPN with systemd-resolved, you\u0026rsquo;ll need a bit more help. In a terminal, run:\nsudo apt install -y openvpn-systemd-resolved network-manager-openvpn network-manager-openvpn-gnome This will install a helper script that integrates OpenVPN and systemd-resolved, a NetworkManager plugin for OpenVPN, and its GUI counterpart for GNOME desktop environment.\n2. Add DNS implementation to your connection profile You\u0026rsquo;ll need to edit the connection profile file you downloaded earlier. Since it\u0026rsquo;s symbolically linked, you can accomplish this by changing the .ovpn file, wherever it\u0026rsquo;s stored. Run vim \u0026lt;name\u0026gt;.ovpn to open it in Vim, then add the following lines at the bottom. Explanation in the comments:\n# Allow OpenVPN to call user-defined scripts script-security 2 # Tell systemd-resolved to send all DNS queries over the VPN dhcp-option DOMAIN-ROUTE . # Use the update-systemd-resolved script when TUN/TAP device is opened, # and also run the script on restarts and before the TUN/TAP device is closed up /etc/openvpn/update-systemd-resolved up-restart down /etc/openvpn/update-systemd-resolved down-pre For the full list of OpenVPN options, see OpenVPN Scripting and Environment Variables. You may also like more information about TUN/TAP.\n3. Set up OpenVPN as NetworkManager system connection Use the GUI to set up your VPN with NetworkManager. Open up Network Settings, which should look something like this:\nThen click the plus sign (+) button. On the window that pops up, counterintuitively, choose Import from file\u0026hellip; instead of the OpenVPN option.\nNavigate to, and then select, your .ovpn file. You should now see something like this:\nAdd your username and password for the server (openvpn and the password you set in the first section\u0026rsquo;s Step 3), and your user key password (the same one again, if you\u0026rsquo;ve followed this tutorial), then click the \u0026ldquo;Add\u0026rdquo; button.\n4. Edit your OpenVPN NetworkManager configuration Nearly there! Now that you\u0026rsquo;ve added the VPN as a NetworkManager connection, you\u0026rsquo;ll need to make a quick change to it. You can see a list of NetworkManager connections by running:\nls -la /etc/NetworkManager/system-connections/* The one for your VPN is probably called openvpn, so let\u0026rsquo;s edit it by running:\nsudo vim /etc/NetworkManager/system-connections/openvpn Under [ipv4], you\u0026rsquo;ll need to add the line dns-priority=-42. It should end up looking like this:\nSetting a negative number is a workaround that prioritizes this DNS server. The actual number is arbitrary (-1 should also work) but I like 42. ¯\\_(ツ)_/¯\n5. Restart, connect, profit In a terminal, run:\nsudo service network-manager restart Then in the Network Settings, click the magic button that turns on the VPN:\nFinally, visit the DNS leak test website and click on Extended test to verify the fix. If everything\u0026rsquo;s working properly, you should now see a list containing only your VPN ISP.\nAnd we\u0026rsquo;re done! Congratulations on rolling your very own VPN server and stopping DNS leaks with OpenVPN. Enjoy surfing in (relative) privacy. Now your only worry at the local coffeeshop is who\u0026rsquo;s watching you surf from the seat behind you.\nIf you enjoyed this post, there\u0026rsquo;s a lot more where it came from! I write about computing, cybersecurity, and leading great technical teams. Subscribe on victoria.dev to see new articles first.\n",url:"https://victoria.dev/blog/how-to-set-up-openvpn-on-aws-ec2-and-fix-dns-leaks-on-ubuntu-18.04-lts/"},"https:\/\/victoria.dev\/neofeed\/29727\/":{title:"29727",tags:[],content:"Wanted: ability to grep IRL, as in, grep -Hrnwiv soy frozen_food_aisle/\n\u0026hellip; Ok ok\n H: with filename r: recursive n: print line number in file w: match the whole word i: ignore case v: invert matching (select non-matching lines)  ",url:"https://victoria.dev/neofeed/29727/"},"https:\/\/victoria.dev\/neofeed\/29626\/":{title:"29626",tags:[],content:"\u0026ldquo;I run Linux\u0026rdquo; is basically \u0026ldquo;I drive stick\u0026rdquo; for nerds. 🤩\n",url:"https://victoria.dev/neofeed/29626/"},"https:\/\/victoria.dev\/neofeed\/29425\/":{title:"29425",tags:[],content:"I know I\u0026rsquo;m really excited about the thing I\u0026rsquo;m working on when I get so hungry that I can\u0026rsquo;t concentrate any longer and have to stop to make food, but mostly I\u0026rsquo;m annoyed that I can\u0026rsquo;t make food and type at the same time.\n",url:"https://victoria.dev/neofeed/29425/"},"https:\/\/victoria.dev\/blog\/how-to-do-twice-as-much-with-half-the-keystrokes-using-.bashrc\/":{title:"How to do twice as much with half the keystrokes using `.bashrc`",tags:["linux","terminal","git"],content:"In my recent post about setting up Ubuntu with Bash scripts, I briefly alluded to the magic of .bashrc. This didn\u0026rsquo;t really do it justice, so here\u0026rsquo;s a quick post that offers a bit more detail about what the Bash configuration file can do.\nMy current configuration hugely improves my workflow, and saves me well over 50% of the keystrokes I would have to employ without it! Let\u0026rsquo;s look at some examples of aliases, functions, and prompt configurations that can improve our workflow by helping us be more efficient with fewer key presses.\nBash aliases A smartly written .bashrc can save a whole lot of keystrokes. You can take advantage of this in the literal sense by using bash aliases, or strings that expand to larger commands. For an indicative example, here is a Bash alias for copying files in the terminal:\n# Always copy contents of directories (r)ecursively and explain (v) what was done alias cp=\u0026#39;cp -rv\u0026#39; The alias command defines the string you\u0026rsquo;ll type, followed by what that string will expand to. You can override existing commands like cp above. On its own, the cp command will only copy files, not directories, and succeeds silently. With this alias, you need not remember to pass those two flags, nor cd or ls the location of our copied file to confirm that it\u0026rsquo;s there! Now, just those two key presses (for c and d) will do all of that for us.\nHere are a few more .bashrc aliases for passing flags with common functions.\n# List contents with colors for file types, (A)lmost all hidden files (without . and ..), in (C)olumns, with class indicators (F) alias ls=\u0026#39;ls --color=auto -ACF\u0026#39; # List contents with colors for file types, (a)ll hidden entries (including . and ..), use (l)ong listing format, with class indicators (F) alias ll=\u0026#39;ls --color=auto -alF\u0026#39; # Explain (v) what was done when moving a file alias mv=\u0026#39;mv -v\u0026#39; # Create any non-existent (p)arent directories and explain (v) what was done alias mkdir=\u0026#39;mkdir -pv\u0026#39; # Always try to (c)ontinue getting a partially-downloaded file alias wget=\u0026#39;wget -c\u0026#39; Aliases come in handy when you want to avoid typing long commands, too. Here are a few I use when working with Python environments:\nalias pym=\u0026#39;python3 manage.py\u0026#39; alias mkenv=\u0026#39;python3 -m venv env\u0026#39; alias startenv=\u0026#39;source env/bin/activate \u0026amp;\u0026amp; which python3\u0026#39; alias stopenv=\u0026#39;deactivate\u0026#39; For further inspiration on ways Bash aliases can save time, I highly recommend the examples in this article.\nBash functions One downside of the aliases above is that they\u0026rsquo;re rather static - they\u0026rsquo;ll always expand to exactly the text declared. For a Bash alias that takes arguments, you\u0026rsquo;ll need to create a function. You can do this like so:\n# Show contents of the directory after changing to it function cd () { builtin cd \u0026#34;$1\u0026#34; ls -ACF } I can\u0026rsquo;t begin to tally how many times I\u0026rsquo;ve typed cd and then ls immediately after to see the contents of the directory I\u0026rsquo;m now in. With this function set up, it all happens with just those two letters! The function takes the first argument, $1, as the location to change directory to, then prints the contents of that directory in nicely formatted columns with file type indicators. The builtin part is necessary to get Bash to allow us to override this default command.\nBash functions are very useful when it comes to downloading or upgrading software, too.\nBash function for downloading extended Hugo Thanks to the static site generator Hugo\u0026rsquo;s excellent ship frequency, I previously spent at least a few minutes every couple weeks downloading the new extended version. With a Bash function, I only need to pass in the version number, and the upgrade happens in a few seconds.\n# Hugo install or upgrade function gethugo () { wget -q -P tmp/ https://github.com/gohugoio/hugo/releases/download/v\u0026#34;$@\u0026#34;/hugo_extended_\u0026#34;$@\u0026#34;_Linux-64bit.tar.gz tar xf tmp/hugo_extended_\u0026#34;$@\u0026#34;_Linux-64bit.tar.gz -C tmp/ sudo mv -f tmp/hugo /usr/local/bin/ rm -rf tmp/ hugo version } The $@ notation simply takes all the arguments given, replacing its spot in the function. To run the above function and download Hugo version 0.57.2, you use the command gethugo 0.57.2.\nBash function for downloading a specific Go version I\u0026rsquo;ve got one for Golang, too:\nfunction getgolang () { sudo rm -rf /usr/local/go wget -q -P tmp/ https://dl.google.com/go/go\u0026#34;$@\u0026#34;.linux-amd64.tar.gz sudo tar -C /usr/local -xzf tmp/go\u0026#34;$@\u0026#34;.linux-amd64.tar.gz rm -rf tmp/ go version } Bash function for adding a GitLab remote Or how about a function that adds a remote origin URL for GitLab to the current repository?\nfunction glab () { git remote set-url origin --add git@gitlab.com:\u0026#34;$@\u0026#34;/\u0026#34;${PWD##*/}\u0026#34;.git git remote -v } With glab username, you can create a new origin URL for the current Git repository with our username on GitLab.com. Pushing to a new remote URL automatically creates a new private GitLab repository, so this is a useful shortcut for creating backups!\nBash functions are really only limited by the possibilities of scripting, of which there are, practically, few limits. If there\u0026rsquo;s anything you do on a frequent basis that requires typing a few lines into a terminal, you can probably create a Bash function for it!\nBash prompt Besides directory contents, it\u0026rsquo;s also useful to see the full path of the directory we\u0026rsquo;re in. The Bash prompt can show us this path, along with other useful information like our current Git branch. To make it more readable, you can define colours for each part of the prompt. Here\u0026rsquo;s how you can set up our prompt in .bashrc to accomplish this:\n# Colour codes are cumbersome, so let\u0026#39;s name them txtcyn=\u0026#39;\\[\\e[0;96m\\]\u0026#39; # Cyan txtpur=\u0026#39;\\[\\e[0;35m\\]\u0026#39; # Purple txtwht=\u0026#39;\\[\\e[0;37m\\]\u0026#39; # White txtrst=\u0026#39;\\[\\e[0m\\]\u0026#39; # Text Reset # Which (C)olour for what part of the prompt? pathC=\u0026#34;${txtcyn}\u0026#34; gitC=\u0026#34;${txtpur}\u0026#34; pointerC=\u0026#34;${txtwht}\u0026#34; normalC=\u0026#34;${txtrst}\u0026#34; # Get the name of our branch and put parenthesis around it gitBranch() { git branch 2\u0026gt; /dev/null | sed -e \u0026#39;/^[^*]/d\u0026#39; -e \u0026#39;s/* \\(.*\\)/(\\1)/\u0026#39; } # Build the prompt export PS1=\u0026#34;${pathC}\\w ${gitC}\\$(gitBranch) ${pointerC}\\$${normalC}\u0026#34; Result:\n~/github/myrepo (master) $ Naming the colours helps to easily identify where one colour starts and stops, and where the next one begins. The prompt that you see in our terminal is defined by the string following export PS1, with each component of the prompt set with an escape sequence. Let\u0026rsquo;s break that down:\n \\w displays the current working directory, \\$(gitBranch) calls the gitBranch function defined above, which displays the current Git branch, \\$ will display a \u0026ldquo;$\u0026rdquo; if you are a normal user or in normal user mode, and a \u0026ldquo;#\u0026rdquo; if you are root.  The full list of Bash escape sequences can help us display many more bits of information, including even the time and date! Bash prompts are highly customizable and individual, so feel free to set it up any way you please.\nHere are a few options that put information front and centre and can help us to work more efficiently.\nFor the procrastination-averse Username and current time with seconds, in 24-hour HH:MM:SS format:\nexport PS1=\u0026#34;${userC}\\u ${normalC}at \\t \u0026gt;\u0026#34; user at 09:35:55 \u0026gt; For those who always like to know where they stand Full file path on a separate line, and username:\nexport PS1=\u0026#34;${pathC}\\w${normalC}\\n\\u:\u0026#34; ~/github/myrepo user: For the minimalist export PS1=\u0026#34;\u0026gt;\u0026#34; \u0026gt; We can build many practical prompts with just the basic escape sequences; once you start to integrate functions with prompts, as in the Git branch example, things can get really complicated. Whether this amount of complication is an addition or a detriment to your productivity, only you can know for sure!\nMany fancy Bash prompts are possible with programs readily available with a quick search. I\u0026rsquo;ve intentionally not provided samples here because, well, if you can tend to get as excited about this stuff as I can, it might be a couple hours before you get back to what you were doing before you started reading this post, and I just can\u0026rsquo;t have that on my conscience. 🥺\nWe\u0026rsquo;ve hopefully struck a nice balance now between time invested and usefulness gained from our Bash configuration file! I hope you use your newly-recovered keystroke capacity for good.\n",url:"https://victoria.dev/blog/how-to-do-twice-as-much-with-half-the-keystrokes-using-.bashrc/"},"https:\/\/victoria.dev\/blog\/how-to-set-up-a-fresh-ubuntu-desktop-using-only-dotfiles-and-bash-scripts\/":{title:"How to set up a fresh Ubuntu desktop using only dotfiles and bash scripts",tags:["linux","terminal"],content:"One of my most favorite things about open source files on GitHub is the ability to see how others do (what some people might call) mundane things, like set up their .bashrc and other dotfiles. While I\u0026rsquo;m not as enthusiastic about ricing as I was when I first came to the Linux side, I still get pretty excited when I find a config setting that makes things prettier and faster, and thus, better.\nI recently came across a few such things, particularly in Tom Hudson\u0026rsquo;s dotfiles. Tom seems to like to script things, and some of those things include automatically setting up symlinks, and installing Ubuntu repository applications and other programs. This got me thinking. Could I automate the set up of a new machine to replicate my current one?\nBeing someone generally inclined to take things apart in order to see how they work, I know I\u0026rsquo;ve messed up my laptop on occasion. (Usually when I\u0026rsquo;m away from home, and my backup hard drive isn\u0026rsquo;t.) On those rare but really inconvenient situations when my computer becomes a shell of its former self, (ba-dum-ching) it\u0026rsquo;d be quite nice to have a fast, simple way of putting Humpty Dumpty back together again, just the way I like.\nIn contrast to creating a disk image and restoring it later, a collection of bash scripts is easier to create, maintain, and move around. They require no special utilities, only an external transportation method. It\u0026rsquo;s like passing along the recipe, instead of the whole bundt cake. (Mmm, cake.)\nAdditionally, functionality like this would be super useful when setting up a virtual machine, or VM, or even just a virtual private server, or VPS. (Both of which, now that I write this, would probably make more forgiving targets for my more destructive experiments\u0026hellip; live and learn!)\nWell, after some grepping and Googling and digging around, I now have a suite of scripts that can do this:\n This is the tail end of a test run of the set up scripts on a fresh Ubuntu desktop, loaded off a bootable USB. It had all my programs and settings restored in under three minutes!\nThis post will cover how to achieve the automatic set up of a computer running Ubuntu Desktop using bash scripts. This exact process was last used on Ubuntu 19.10; see my dotfiles master branch for the latest configuration. The majority of the information covered is applicable to all the Linux desktop flavours, though some syntax may differ. The bash scripts cover three main areas: linking dotfiles, installing software from Ubuntu and elsewhere, and setting up the desktop environment. We\u0026rsquo;ll cover each of these areas and go over the important bits so that you can begin to craft your own scripts.\nDotfiles Dotfiles are what most Linux enthusiasts call configuration files. They typically live in the user\u0026rsquo;s home directory (denoted in bash scripts with the builtin variable $HOME) and control the appearance and behavior of all kinds of programs. The file names begin with ., which denotes hidden files in Linux (hence \u0026ldquo;dot\u0026rdquo; files). Here are some common dotfiles and ways in which they\u0026rsquo;re useful.\n.bashrc The .bashrc file is a list of commands executed at startup by interactive, non-login shells. Interactive vs non-interactive shells can be a little confusing, but aren\u0026rsquo;t necessary for us to worry about here. For our purposes, any time you open a new terminal, see a prompt, and can type commands into it, your .bashrc was executed.\nLines in this file can help improve your workflow by creating aliases that reduce keystrokes, or by displaying a helpful prompt with useful information. It can even run user-created programs, like Eddie. For more ideas, you can have a look at my .bashrc file on GitHub.\n.vimrc The .vimrc dotfile configures the champion of all text editors, Vim. (If you haven\u0026rsquo;t yet wielded the powers of the keyboard shortcuts, I highly recommend a fun game to learn Vim with.)\nIn .vimrc, we can set editor preferences such as display settings, colours, and custom keyboard shortcuts. You can take a look at my .vimrc on GitHub.\nOther dotfiles may be useful depending on the programs you use, such as .gitconfig or .tmux.conf. Exploring dotfiles on GitHub is a great way to get a sense of what\u0026rsquo;s available and useful to you!\nLinking dotfiles We can use a script to create symbolic links, or symlinks for all our dotfiles. This allows us to keep all the files in a central repository, where they can easily be managed, while also providing a sort of placeholder in the spot that our programs expect the configuration file to be found. This is typically, but not always, the user home directory. For example, since I store my dotfiles on GitHub, I keep them in a directory with a path like ~/github/dotfiles/ while the files themselves are symlinked, resulting in a path like ~/.vimrc.\nTo programmatically check for and handle any existing files and symlinks, then create new ones, we can use this elegant shell script. I compliment it only because I blatantly stole the core of it from Tom\u0026rsquo;s setup script, so I can\u0026rsquo;t take the credit for how lovely it is.\nThe symlink.sh script works by attempting to create symlinks for each dotfile in our $HOME. It first checks to see if a symlink already exists, or if a regular file or directory with the same name exists. In the former case, the symlink is removed and remade; in the latter, the file or directory is renamed, then the symlink is made.\nInstalling software One of the beautiful things about exploring shell scripts is discovering how much can be achieved using only the command line. As someone whose first exposure to computers was through a graphical operating system, I find working in the terminal to be refreshingly fast.\nWith Ubuntu, most programs we likely require are available through the default Ubuntu software repositories. We typically search for these with the command apt search \u0026lt;program\u0026gt; and install them with sudo apt install \u0026lt;program\u0026gt;. Some software we\u0026rsquo;d like may not be in the default repositories, or may not be offered there in the most current version. In these cases, we can still install these programs in Ubuntu using a PPA, or Personal Package Archive. We\u0026rsquo;ll just have to be careful that the PPAs we choose are from the official sources.\nIf a program we\u0026rsquo;d like doesn\u0026rsquo;t appear in the default repositories or doesn\u0026rsquo;t seem to have a PPA, we may still be able to install it via command line. A quick search for \u0026ldquo; installation command line\u0026rdquo; should get some answers.\nSince bash scripts are just a collection of commands that we could run individually in the terminal, creating a script to install all our desired programs is as straightforward as putting all the commands into a script file. I chose to organize my installation scripts between the default repositories, which are installed by my aptinstall.sh script, and programs that involve external sources, handled with my programs.sh script.\nSetting up the desktop environment On the recent occasions when I\u0026rsquo;ve gotten a fresh desktop (intentionally or otherwise) I always seem to forget how long it takes to remember, find, and then change all the desktop environment settings. Keyboard shortcuts, workspaces, sound settings, night mode\u0026hellip; it adds up!\nThankfully, all these settings have to be stored somewhere in a non-graphical format, which means that if we can discover how that\u0026rsquo;s done, we can likely find a way to easily manipulate the settings with a bash script. Lo and behold the terminal command, gsettings list-recursively.\nThere are a heck of a lot of settings for GNOME desktop environment. We can make the list easier to scroll through (if, like me, you\u0026rsquo;re sometimes the type of person to say \u0026ldquo;Just let me look at everything and figure out what I want!\u0026quot;) by piping to less: gsettings list-recursively | less. Alternatively, if we have an inkling as to what we might be looking for, we can use grep: gsettings list-recursively | grep 'keyboard'.\nWe can manipulate our settings with the gsettings set command. It can sometimes be difficult to find the syntax for the setting we want, so when we\u0026rsquo;re first building our script, I recommend using the GUI to make the changes, then finding the gsettings line we changed and recording its value.\nFor some inspiration, you can view my desktop.sh settings script on GitHub.\nPutting it all together Having modular scripts (one for symlinks, two for installing programs, another for desktop settings) is useful for both keeping things organized and for being able to run some but not all of the automated set up. For instance, if I were to set up a VPS in which I only use the command line, I wouldn\u0026rsquo;t need to bother with installing graphical programs or desktop settings.\nIn cases where I do want to run all the scripts, however, doing so one-by-one is a little tedious. Thankfully, since bash scripts can themselves be run by terminal commands, we can simply write another master script to run them all!\nHere\u0026rsquo;s my master script to handle the set up of a new Ubuntu desktop machine:\n#!/bin/bash  ./symlink.sh ./aptinstall.sh ./programs.sh ./desktop.sh ## Get all upgrades sudo apt upgrade -y ## See our bash changes source ~/.bashrc ## Fun hello figlet \u0026#34;... and we\u0026#39;re back!\u0026#34; | lolcat I threw in the upgrade line for good measure. It will make sure that the programs installed on our fresh desktop have the latest updates. Now a simple, single bash command will take care of everything!\nYou may have noticed that, while our desktop now looks and runs familiarly, these scripts don\u0026rsquo;t cover one very important area: our files. Hopefully, you have a back up method for those that involves some form of reliable external hardware. If not, and if you tend to put your work in external repository hosts like GitHub or GitLab, I do have a way to automatically clone and back up your GitHub repositories with bash one-liners.\nRelying on external repository hosts doesn\u0026rsquo;t offer 100% coverage, however. Files that you wouldn\u0026rsquo;t put in an externally hosted repository (private or otherwise) consequently can\u0026rsquo;t be pulled. Git ignored objects that can\u0026rsquo;t be generated from included files, like private keys and secrets, will not be recreated. Those files, however, are likely small enough that you could fit a whole bunch on a couple encrypted USB flash drives (and if you don\u0026rsquo;t have private key backups, maybe you ought to do that first?).\nThat said, I hope this post has given you at least some inspiration as to how dotfiles and bash scripts can help to automate setting up a fresh desktop. If you come up with some settings you find useful, please help others discover them by sharing your dotfiles, too!\n",url:"https://victoria.dev/blog/how-to-set-up-a-fresh-ubuntu-desktop-using-only-dotfiles-and-bash-scripts/"},"https:\/\/victoria.dev\/neofeed\/21924\/":{title:"21924",tags:[],content:"Search and replace a word in Vim:\n:%s/\\\u0026lt;word\\\u0026gt;/newword/g\nThe % indicates to look in all lines of the current file; s is for substitute; \\\u0026lt;word\\\u0026gt; matches the whole word; and the g is for globally, ie. every occurrence.\nOr use gc at the end there, if you want to confirm (c) each change.\n",url:"https://victoria.dev/neofeed/21924/"},"https:\/\/victoria.dev\/neofeed\/21823\/":{title:"21823",tags:[],content:"Me: echo --help\nComputer: --help\n😒\n",url:"https://victoria.dev/neofeed/21823/"},"https:\/\/victoria.dev\/blog\/how-to-write-bash-one-liners-for-cloning-and-managing-github-and-gitlab-repositories\/":{title:"How to write Bash one-liners for cloning and managing GitHub and GitLab repositories",tags:["terminal","linux","git","data"],content:"Few things are more satisfying to me than one elegant line of Bash that automates hours of tedious work. As part of some recent explorations into automatically re-creating my laptop with Bash scripts, I wanted to find a way to easily clone my GitHub-hosted repositories to a new machine. After a bit of digging around, I wrote a one-liner that did just that. Then, in the spirit of not putting all our eggs in the same basket, I wrote another one-liner to automatically create and push to GitLab-hosted backups as well. Here they are.\nA Bash one-liner to clone all your GitHub repositories Caveat: you\u0026rsquo;ll need a list of the GitHub repositories you want to clone. The good thing about that is it gives you full agency to choose just the repositories you want on your machine, instead of going in whole-hog.\nYou can easily clone GitHub repositories without entering your password each time by using HTTPS with your 15-minute cached credentials or, my preferred method, by connecting to GitHub with SSH. For brevity I\u0026rsquo;ll assume we\u0026rsquo;re going with the latter, and our SSH keys are set up.\nGiven a list of GitHub URLs in the file gh-repos.txt, like this:\ngit@github.com:username/first-repository.git git@github.com:username/second-repository.git git@github.com:username/third-repository.git We run:\nxargs -n1 git clone \u0026lt; gh-repos.txt This clones all the repositories on the list into the current folder. This same one-liner works for GitLab repositories as well, if you substitute the appropriate URLs.\nWhat\u0026rsquo;s going on here There are two halves to this one-liner: the input, counterintuitively on the right side, and the part that makes stuff happen, on the left. We could make the order of these parts more intuitive (maybe?) by writing the same command like this:\n\u0026lt;gh-repos.txt xargs -n1 git clone To run a command for each line of our input, gh-repos.txt, we use xargs -n1. The tool xargs reads items from input and executes any commands it finds (it will echo if it doesn\u0026rsquo;t find any). By default, it assumes that items are separated by spaces; new lines also works and makes our list easier to read. The flag -n1 tells xargs to use 1 argument, or in our case, one line, per command. We build our command with git clone, which xargs then executes for each line. Ta-da.\nA Bash one-liner to create and push many repositories on GitLab GitLab, unlike GitHub, lets us do this nifty thing where we don\u0026rsquo;t have to use the website to make a new repository first. We can create a new GitLab repository from our terminal. The newly created repository defaults to being set as Private, so if we want to make it Public on GitLab, we\u0026rsquo;ll have to do that manually later.\nThe GitLab docs tell us to push to create a new project using git push --set-upstream, but I don\u0026rsquo;t find this to be very convenient for using GitLab as a backup. As I work with my repositories in the future, I\u0026rsquo;d like to run one command that pushes to both GitHub and GitLab without additional effort on my part.\nTo make this Bash one-liner work, we\u0026rsquo;ll also need a list of repository URLs for GitLab (ones that don\u0026rsquo;t exist yet). We can easily do this by copying our GitHub repository list, opening it up with Vim, and doing a search-and-replace:\ncp gh-repos.txt gl-repos.txt vim gl-repos.txt :%s/\\\u0026lt;github\\\u0026gt;/gitlab/g :wq This produces gl-repos.txt, which looks like:\ngit@gitlab.com:username/first-repository.git git@gitlab.com:username/second-repository.git git@gitlab.com:username/third-repository.git We can create these repositories on GitLab, add the URLs as remotes, and push our code to the new repositories by running:\nawk -F\u0026#39;\\/|(\\.git)\u0026#39; \u0026#39;{system(\u0026#34;cd ~/FULL/PATH/\u0026#34; $2 \u0026#34; \u0026amp;\u0026amp; git remote set-url origin --add \u0026#34; $0 \u0026#34; \u0026amp;\u0026amp; git push\u0026#34;)}\u0026#39; gl-repos.txt Hang tight and I\u0026rsquo;ll explain it; for now, take note that ~/FULL/PATH/ should be the full path to the directory containing our GitHub repositories.\nWe do have to make note of a couple assumptions:\n The name of the directory on your local machine that contains the repository is the same as the name of the repository in the URL (this will be the case if it was cloned with the one-liner above); Each repository is currently checked out to the branch you want pushed, ie. master.  The one-liner could be expanded to handle these assumptions, but it is the humble opinion of the author that at that point, we really ought to be writing a Bash script.\nWhat\u0026rsquo;s going on here Our Bash one-liner uses each line (or URL) in the gl-repos.txt file as input. With awk, it splits off the name of the directory containing the repository on our local machine, and uses these pieces of information to build our larger command. If we were to print the output of awk, we\u0026rsquo;d see:\ncd ~/FULL/PATH/first-repository \u0026amp;\u0026amp; git remote set-url origin --add git@gitlab.com:username/first-repository.git \u0026amp;\u0026amp; git push cd ~/FULL/PATH/second-repository \u0026amp;\u0026amp; git remote set-url origin --add git@gitlab.com:username/second-repository.git \u0026amp;\u0026amp; git push cd ~/FULL/PATH/third-repository \u0026amp;\u0026amp; git remote set-url origin --add git@gitlab.com:username/third-repository.git \u0026amp;\u0026amp; git push Let\u0026rsquo;s look at how we build this command.\nSplitting strings with awk The tool awk can split input based on field separators. The default separator is a whitespace character, but we can change this by passing the -F flag. Besides single characters, we can also use a regular expression field separator. Since our repository URLs have a set format, we can grab the repository names by asking for the substring between the slash character / and the end of the URL, .git.\nOne way to accomplish this is with our regex \\/|(\\.git):\n \\/ is an escaped / character; | means \u0026ldquo;or\u0026rdquo;, telling awk to match either expression; (\\.git) is the capture group at the end of our URL that matches \u0026ldquo;.git\u0026rdquo;, with an escaped . character. This is a bit of a cheat, as \u0026ldquo;.git\u0026rdquo; isn\u0026rsquo;t strictly splitting anything (there\u0026rsquo;s nothing on the other side) but it\u0026rsquo;s an easy way for us to take this bit off.  Once we\u0026rsquo;ve told awk where to split, we can grab the right substring with the field operator. We refer to our fields with a $ character, then by the field\u0026rsquo;s column number. In our example, we want the second field, $2. Here\u0026rsquo;s what all the substrings look like:\n1: git@gitlab.com:username 2: first-repository To use the whole string, or in our case, the whole URL, we use the field operator $0. To write the command, we just substitute the field operators for the repository name and URL. Running this with print as we\u0026rsquo;re building it can help to make sure we\u0026rsquo;ve got all the spaces right.\nawk -F\u0026#39;\\/|(\\.git)\u0026#39; \u0026#39;{print \u0026#34;cd ~/FULL/PATH/\u0026#34; $2 \u0026#34; \u0026amp;\u0026amp; git remote set-url origin --add \u0026#34; $0 \u0026#34; \u0026amp;\u0026amp; git push\u0026#34;}\u0026#39; gl-repos.txt Running the command We build our command inside the parenthesis of system(). By using this as the output of awk, each command will run as soon as it is built and output. The system() function creates a child process that executes our command, then returns once the command is completed. In plain English, this lets us perform the Git commands on each repository, one-by-one, without breaking from our main process in which awk is doing things with our input file. Here\u0026rsquo;s our final command again, all put together.\nawk -F\u0026#39;\\/|(\\.git)\u0026#39; \u0026#39;{system(\u0026#34;cd ~/FULL/PATH/\u0026#34; $2 \u0026#34; \u0026amp;\u0026amp; git remote set-url origin --add \u0026#34; $0 \u0026#34; \u0026amp;\u0026amp; git push\u0026#34;)}\u0026#39; gl-repos.txt Using our backups By adding the GitLab URLs as remotes, we\u0026rsquo;ve simplified the process of pushing to both externally hosted repositories. If we run git remote -v in one of our repository directories, we\u0026rsquo;ll see:\norigin git@github.com:username/first-repository.git (fetch) origin git@github.com:username/first-repository.git (push) origin git@gitlab.com:username/first-repository.git (push) Now, simply running git push without arguments will push the current branch to both remote repositories.\nWe should also note that git pull will generally only try to pull from the remote repository you originally cloned from (the URL marked (fetch) in our example above). Pulling from multiple Git repositories at the same time is possible, but complicated, and beyond the scope of this post. Here\u0026rsquo;s an explanation of pushing and pulling to multiple remotes to help get you started, if you\u0026rsquo;re curious. The Git documentation on remotes may also be helpful.\nTo elaborate on the succinctness of Bash one-liners Bash one-liners, when understood, can be fun and handy shortcuts. At the very least, being aware of tools like xargs and awk can help to automate and alleviate a lot of tediousness in our work. However, there are some downsides.\nIn terms of an easy-to-understand, maintainable, and approachable tool, Bash one-liners suck. They\u0026rsquo;re usually more complicated to write than a Bash script using if or while loops, and certainly more complicated to read. It\u0026rsquo;s likely that when we write them, we\u0026rsquo;ll miss a single quote or closing parenthesis somewhere; and as I hope this post demonstrates, they can take quite a bit of explaining, too. So why use them?\nImagine reading a recipe for baking a cake, step by step. You understand the methods and ingredients, and gather your supplies. Then, as you think about it, you begin to realize that if you just throw all the ingredients at the oven in precisely the right order, a cake will instantly materialize. You try it, and it works!\nThat would be pretty satisfying, wouldn\u0026rsquo;t it?\n",url:"https://victoria.dev/blog/how-to-write-bash-one-liners-for-cloning-and-managing-github-and-gitlab-repositories/"},"https:\/\/victoria.dev\/neofeed\/21522\/":{title:"21522",tags:[],content:"Sooo if you sudo apt remove python3 on Ubuntu, the desktop goes away. 🥺\nsudo apt install ubuntu-desktop brings it back. 😄\n",url:"https://victoria.dev/neofeed/21522/"},"https:\/\/victoria.dev\/neofeed\/21321\/":{title:"21321",tags:[],content:"A quick way to clone a list of @GitHub repos.\nGiven a file, repos.txt with a repository\u0026rsquo;s SSH link on each line (and your SSH keys set up), run:\nxargs -n1 git clone \u0026lt; repos.txt\nWill clone all repositories into the current folder.\n",url:"https://victoria.dev/neofeed/21321/"},"https:\/\/victoria.dev\/neofeed\/21120\/":{title:"21120",tags:[],content:"I wonder how tomatoes feel about being dipped in ketchup\n",url:"https://victoria.dev/neofeed/21120/"},"https:\/\/victoria.dev\/blog\/a-quick-guide-to-changing-your-github-username\/":{title:"A quick guide to changing your GitHub username",tags:["websites","linux","terminal"],content:"This being the 2,38947234th and probably last time I\u0026rsquo;ll change my username, (marriage is permanent, right?) I thought I\u0026rsquo;d better write a quick post on how this transition can be achieved as smoothly as possible. You can read official instructions on how to change your GitHub username here, and they will tell you how to do it and what happens. The following is a quick guide to some things to consider afterwards.\nWhere to make changes  Change username in GitHub account settings. If using GitHub Pages, change name of your \u0026ldquo;username.github.io\u0026rdquo; repository. If using other services that point to your \u0026ldquo;username.github.io\u0026rdquo; repository address, update them. If using Netlify, you may want to sign in and reconnect your repositories. (Mine still worked, but due to a possibly unrelated issue, I\u0026rsquo;m not positive.) Sign in to Travis CI and other integrations (find them in your repository Settings tab -\u0026gt; Integrations \u0026amp; services). This will update your username there. Update your local files and repository links with very carefully executed find and sed commands, and push back changes to GitHub. Redeploy any websites you may have with your updated GitHub link. Fix any links around the web to your profile, your repositories, or Gists you may have shared.  Local file updates Here are some suggestions for strings to search and replace your username in.\n github.com/username (References to your GitHub page in READMEs or in website copy) username.github.io (Links to your GitHub Page) git@github.com:username (Git config remote ssh urls) travis-ci.com/username (Travis badges in READMEs) shields.io/github/.../username (Shields badges in READMEs, types include contributors, stars, tags, and more)  You can quickly identify where the above strings are located using this command for each string:\ngrep -rnw -e 'foobar'\nThis will recursively (r) search all files for strings matching the whole (w) pattern (e) provided and prefix results with the line numbers (n) so you can easily find them.\nUsing find and sed can make these changes much faster. See this article on search and replace.\nEnjoy your new handle! (I hope it sticks.)\n",url:"https://victoria.dev/blog/a-quick-guide-to-changing-your-github-username/"},"https:\/\/victoria.dev\/neofeed\/20819\/":{title:"20819",tags:[],content:"I think you can learn more about someone\u0026rsquo;s true character from their Audible library than their Twitter feed.\n",url:"https://victoria.dev/neofeed/20819/"},"https:\/\/victoria.dev\/neofeed\/20318\/":{title:"20318",tags:[],content:"Add the current git branch to your bash prompt! In .bashrc:\nparse_git_branch() { git branch 2\u0026gt; /dev/null | sed -e \u0026#39;/^[^*]/d\u0026#39; -e \u0026#39;s/* \\(.*\\)/(\\1)/\u0026#39; } if [ \u0026#34;$color_prompt\u0026#34; = yes ]; then PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\[\\033[0;1;38;5;247m\\]\u0026gt; \\[\\033[3;38;5;68m\\]\\W $(parse_git_branch)\\[\\033[00m\\] \u0026#39; else PS1=\u0026#39;${debian_chroot:+($debian_chroot)}\\W $(parse_git_branch)\\[\\033[00m\\] \u0026#39; fi No more accidentally merging master into your this-seems-like-a-good-idea branch!\n",url:"https://victoria.dev/neofeed/20318/"},"https:\/\/victoria.dev\/neofeed\/20117\/":{title:"20117",tags:[],content:"New rule: if non-critical item sits on the to-do list for more than three days, delete it.\n",url:"https://victoria.dev/neofeed/20117/"},"https:\/\/victoria.dev\/neofeed\/19616\/":{title:"19616",tags:[],content:"Confession: I only buy Altoids mints when I need to store something tiny.\n",url:"https://victoria.dev/neofeed/19616/"},"https:\/\/victoria.dev\/neofeed\/19015\/":{title:"19015",tags:[],content:"Big decisions are really just the sum of the many little decisions that were made before them.\n",url:"https://victoria.dev/neofeed/19015/"},"https:\/\/victoria.dev\/neofeed\/17514\/":{title:"17514",tags:[],content:"On a scale of 1-3, how familiar are you with protocol buffers?\nreserved 1; reserved \u0026#34;i_dont_get_it\u0026#34;; int32 basic_familiarity = 2; int32 these_should_be_enums = 3; ",url:"https://victoria.dev/neofeed/17514/"},"https:\/\/victoria.dev\/neofeed\/16913\/":{title:"16913",tags:[],content:"Today on Getting Totally Sidetracked with Linux Commands, a one-liner that prints all subdirectories without a .git in them:\nfind . -maxdepth 1 -type d -exec test '!' -e '{}/.git' ';' -printf \u0026quot;not git repo: %p\\n\u0026quot;\nWith a little help from https://unix.stackexchange.com/questions/525534/\n",url:"https://victoria.dev/neofeed/16913/"},"https:\/\/victoria.dev\/neofeed\/16212\/":{title:"16212",tags:[],content:"Turns out humans also have artificial intelligence. It looks like we all know how to succeed on our own but most of the time we\u0026rsquo;re just referencing past examples.\n",url:"https://victoria.dev/neofeed/16212/"},"https:\/\/victoria.dev\/neofeed\/15611\/":{title:"15611",tags:[],content:"It\u0026rsquo;s a big, wonderful, dangerous, and important world out there\u0026hellip; but today, I am really excited for just one comparatively little, relatively short, but singularly important thing that will hopefully happen tomorrow.\n",url:"https://victoria.dev/neofeed/15611/"},"https:\/\/victoria.dev\/neofeed\/12110\/":{title:"12110",tags:[],content:"If you\u0026rsquo;re \u0026ldquo;paying\u0026rdquo; attention to something, you ought to be getting value in the exchange.\n",url:"https://victoria.dev/neofeed/12110/"},"https:\/\/victoria.dev\/neofeed\/11309\/":{title:"11309",tags:[],content:"11/10 times the problem is user input error\n",url:"https://victoria.dev/neofeed/11309/"},"https:\/\/victoria.dev\/blog\/two-ways-to-deploy-a-public-github-pages-site-from-a-private-hugo-repository\/":{title:"Two ways to deploy a public GitHub Pages site from a private Hugo repository",tags:["ci/cd","git"],content:"Tools like Travis CI and Netlify offer some pretty nifty features, like seamlessly deploying your GitHub Pages site when changes are pushed to its repository. Along with a static site generator like Hugo, keeping a blog up to date is pretty painless.\nI\u0026rsquo;ve used Hugo to build my site for years, but until this past week I\u0026rsquo;d never hooked up my Pages repository to any deployment service. Why? Because using a tool that built my site before deploying it seemed to require having the whole recipe in one place - and if you\u0026rsquo;re using GitHub Pages with the free version of GitHub, that place is public. That means that all my three-in-the-morning bright ideas and messy unfinished (and unfunny) drafts would be publicly available - and no amount of continuous convenience was going to convince me to do that.\nSo I kept things separated, with Hugo\u0026rsquo;s messy behind-the-scenes stuff in a local Git repository, and the generated public/ folder pushing to my GitHub Pages remote repository. Each time I wanted to deploy my site, I\u0026rsquo;d have to get on my laptop and hugo to build my site, then cd public/ \u0026amp;\u0026amp; git add . \u0026amp;\u0026amp; git commit\u0026hellip; etc etc. And all was well, except for the nagging feeling that there was a better way to do this.\nI wrote another article a little while back about using GitHub and Working Copy to make changes to my repositories on my iPad whenever I\u0026rsquo;m out and about. It seemed off to me that I could do everything except deploy my site from my iPad, so I set out to change that.\nA couple three-in-the-morning bright ideas and a revoked access token later (oops), I now have not one but two ways to deploy to my public GitHub Pages repository from an entirely separated, private GitHub repository. In this post, I\u0026rsquo;ll take you through achieving this with Travis CI or using Netlify and Make.\nThere\u0026rsquo;s nothing hackish about it - my public GitHub Pages repository still looks the same as it does when I pushed to it locally from my terminal. Only now, I\u0026rsquo;m able to take advantage of a couple great deployment tools to have the site update whenever I push to my private repo, whether I\u0026rsquo;m on my laptop or out and about with my iPad.\n #YouDidNotPushFromThere\n  This article assumes you have working knowledge of Git and GitHub Pages. If not, you may like to spin off some browser tabs from my articles on using GitHub and Working Copy and building a site with Hugo and GitHub Pages first.\nLet\u0026rsquo;s do it!\nPrivate-to-public GitHub Pages deployment with Travis CI Travis CI has the built-in ability (♪) to deploy to GitHub Pages following a successful build. They do a decent job in the docs of explaining how to add this feature, especially if you\u0026rsquo;ve used Travis CI before\u0026hellip; which I haven\u0026rsquo;t. Don\u0026rsquo;t worry, I did the bulk of the figuring-things-out for you.\n Travis CI gets all its instructions from a configuration file in the root of your repository called .travis.yml You need to provide a GitHub personal access token as a secure encrypted variable, which you can generate using travis on the command line Once your script successfully finishes doing what you\u0026rsquo;ve told it to do (not necessarily what you want it to do but that\u0026rsquo;s a whole other blog post), Travis will deploy your build directory to a repository you can specify with the repo configuration variable.  Setting up the Travis configuration file Create a new configuration file for Travis with the filename .travis.yml (note the leading \u0026ldquo;.\u0026quot;). These scripts are very customizable and I struggled to find a relevant example to use as a starting point - luckily, you don\u0026rsquo;t have that problem!\nHere\u0026rsquo;s my basic .travis.yml:\ngit:depth:falseenv:global:- HUGO_VERSION=\u0026#34;0.54.0\u0026#34;matrix:- YOUR_ENCRYPTED_VARIABLEinstall:- wget -q https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_${HUGO_VERSION}_Linux-64bit.tar.gz- tar xf hugo_${HUGO_VERSION}_Linux-64bit.tar.gz- mv hugo ~/bin/script:- hugo --gc --minifydeploy:provider:pagesskip-cleanup:truegithub-token:$GITHUB_TOKENkeep-history:truelocal-dir:publicrepo:gh-username/gh-username.github.iotarget-branch:masterverbose:trueon:branch:masterThis script downloads and installs Hugo, builds the site with the garbage collection and minify flags, then deploys the public/ directory to the specified repo - in this example, your public GitHub Pages repository. You can read about each of the deploy configuration options here.\nTo add the GitHub personal access token as an encrypted variable, you don\u0026rsquo;t need to manually edit your .travis.yml. The travis gem commands below will encrypt and add the variable for you when you run them in your repository directory.\nFirst, install travis with sudo gem install travis.\nThen generate your GitHub personal access token, copy it (it only shows up once!) and run the commands below in your repository root, substituting your token for the kisses:\ntravis login --pro --github-token xxxxxxxxxxxxxxxxxxxxxxxxxxx travis encrypt GITHUB_TOKEN=xxxxxxxxxxxxxxxxxxxxxxxxxxx --add env.matrix Your encrypted token magically appears in the file. Once you\u0026rsquo;ve committed .travis.yml to your private Hugo repository, Travis CI will run the script and if the build succeeds, will deploy your site to your public GitHub Pages repo. Magic!\nTravis will always run a build each time you push to your private repository. If you don\u0026rsquo;t want to trigger this behavior with a particular commit, add the skip command to your commit message.\nYo that\u0026rsquo;s cool but I like Netlify.\nOkay fine.\nDeploying to a separate repository with Netlify and Make We can get Netlify to do our bidding by using a Makefile, which we\u0026rsquo;ll run with Netlify\u0026rsquo;s build command.\nHere\u0026rsquo;s what our Makefile looks like:\nSHELL:=/bin/bash BASEDIR=$(CURDIR) OUTPUTDIR=public .PHONY: all all: clean get_repository build deploy .PHONY: clean clean: @echo \u0026#34;Removing public directory\u0026#34; rm -rf $(BASEDIR)/$(OUTPUTDIR) .PHONY: get_repository get_repository: @echo \u0026#34;Getting public repository\u0026#34; git clone https://github.com/gh-username/gh-username.github.io.git public .PHONY: build build: @echo \u0026#34;Generating site\u0026#34; hugo --gc --minify .PHONY: deploy deploy: @echo \u0026#34;Preparing commit\u0026#34; @cd $(OUTPUTDIR) \\  \u0026amp;\u0026amp; git config user.email \u0026#34;you@youremail.com\u0026#34; \\  \u0026amp;\u0026amp; git config user.name \u0026#34;Your Name\u0026#34; \\  \u0026amp;\u0026amp; git add . \\  \u0026amp;\u0026amp; git status \\  \u0026amp;\u0026amp; git commit -m \u0026#34;Deploy via Makefile\u0026#34; \\  \u0026amp;\u0026amp; git push -f -q https://$(GITHUB_TOKEN)@github.com/gh-username/gh-username.github.io.git master @echo \u0026#34;Pushed to remote\u0026#34; To preserve the Git history of our separate GitHub Pages repository, we\u0026rsquo;ll first clone it, build our new Hugo site to it, and then push it back to the Pages repository. This script first removes any existing public/ folder that might contain files or a Git history. It then clones our Pages repository to public/, builds our Hugo site (essentially updating the files in public/), then takes care of committing the new site to the Pages repository.\nIn the deploy section, you\u0026rsquo;ll notice lines starting with \u0026amp;\u0026amp;. These are chained commands. Since Make invokes a new sub-shell for each line, it starts over with every new line from our root directory. To get our cd to stick and avoid running our Git commands in the project root directory, we\u0026rsquo;re chaining the commands and using the backslash character to break long lines for readability.\nBy chaining our commands, we\u0026rsquo;re able to configure our Git identity, add all our updated files, and create a commit for our Pages repository.\nSimilarly to using Travis CI, we\u0026rsquo;ll need to pass in a GitHub personal access token to push to our public GitHub Pages repository - only Netlify doesn\u0026rsquo;t provide a straightforward way to encrypt the token in our Makefile.\nInstead, we\u0026rsquo;ll use Netlify\u0026rsquo;s Build Environment Variables, which live safely in our site settings in the Netlify app. We can then call our token variable in the Makefile. We use it to push (quietly, to avoid printing the token in logs) to our Pages repository by passing it in the remote URL.\nTo avoid printing the token in Netlify\u0026rsquo;s logs, we suppress recipe echoing for that line with the leading @ character.\nWith your Makefile in the root of your private GitHub repository, you can set up Netlify to run it for you.\nSetting up Netlify Getting set up with Netlify via the web UI is straightforward. Once you sign in with GitHub, choose the private GitHub repository where your Hugo site lives. The next page Netlify takes you to lets you enter deploy settings:\nYou can specify the build command that will run your Makefile (make all for this example). The branch to deploy and the publish directory don\u0026rsquo;t matter too much in our specific case, since we\u0026rsquo;re only concerned with pushing to a separate repository. You can enter the typical master deploy branch and public publish directory.\nUnder \u0026ldquo;Advanced build settings\u0026rdquo; click \u0026ldquo;New variable\u0026rdquo; to add your GitHub personal access token as a Build Environment Variable. In our example, the variable name is GITHUB_TOKEN. Click \u0026ldquo;Deploy site\u0026rdquo; to make the magic happen.\nIf you\u0026rsquo;ve already previously set up your repository with Netlify, find the settings for Continuous Deployment under Settings \u0026gt; Build \u0026amp; deploy.\nNetlify will build your site each time you push to the private repository. If you don\u0026rsquo;t want a particular commit to trigger a build, add [skip ci] in your Git commit message.\nSame same but different One effect of using Netlify this way is that your site will be built in two places: one is the separate, public GitHub Pages repository that the Makefile pushes to, and the other is your Netlify site that deploys on their CDN from your linked private GitHub repository. The latter is useful if you\u0026rsquo;re going to play with Deploy Previews and other Netlify features, but those are outside the scope of this post.\nThe main point is that your GitHub Pages site is now updated in your public repo. Yay!\nGo forth and deploy fearlessly I hope the effect of this new information is that you feel more able to update your sites, wherever you happen to be. The possibilities are endless - at home on your couch with your laptop, out cafe-hopping with your iPad, or in the middle of a first date on your phone. Endless!\n Don\u0026rsquo;t do stuff on your phone when you\u0026rsquo;re on a date. Not if you want a second one, anyway.\n  ",url:"https://victoria.dev/blog/two-ways-to-deploy-a-public-github-pages-site-from-a-private-hugo-repository/"},"https:\/\/victoria.dev\/neofeed\/09708\/":{title:"09708",tags:[],content:"I sometimes see birds walking and think they must not be very excited about wherever it is they\u0026rsquo;re going.\n",url:"https://victoria.dev/neofeed/09708/"},"https:\/\/victoria.dev\/neofeed\/09507\/":{title:"09507",tags:[],content:"RT if you also have a git branch named \u0026ldquo;0v\u0026rdquo;\n",url:"https://victoria.dev/neofeed/09507/"},"https:\/\/victoria.dev\/neofeed\/08806\/":{title:"08806",tags:[],content:"What I really want out of face-recognition technology is software that blanks my screen if it detects a face other than my own looking at it. Instead, I get Animojis. Glad we\u0026rsquo;ve got our priorities straight.\n",url:"https://victoria.dev/neofeed/08806/"},"https:\/\/victoria.dev\/neofeed\/08505\/":{title:"08505",tags:[],content:"Debugging practice challenge: When you have three VSC windows open and you sneeze and accidentally press a key and you can\u0026rsquo;t tell which pane had focus.\n",url:"https://victoria.dev/neofeed/08505/"},"https:\/\/victoria.dev\/neofeed\/08004\/":{title:"08004",tags:[],content:"I\u0026rsquo;m not that hard to please. I see proper error handling, I am HTTP 200.\n",url:"https://victoria.dev/neofeed/08004/"},"https:\/\/victoria.dev\/neofeed\/07703\/":{title:"07703",tags:[],content:"Google translate camera app, but for Asian food\n",url:"https://victoria.dev/neofeed/07703/"},"https:\/\/victoria.dev\/neofeed\/07402\/":{title:"07402",tags:[],content:"I\u0026rsquo;d use npm more often but my heart\u0026rsquo;s just not init\n",url:"https://victoria.dev/neofeed/07402/"},"https:\/\/victoria.dev\/blog\/a-remote-sync-solution-for-ios-and-linux-git-and-working-copy\/":{title:"A remote sync solution for iOS and Linux: Git and Working Copy",tags:["linux","git"],content:"I\u0026rsquo;m always looking for pockets of time in which I can be productive. If you add up the minutes you spend in limbo while waiting in line, commuting, or waiting for food delivery (just me?), you may just find an extra hour or two in your day.\nTo take full advantage of these bits of time, I needed a solution that let me pick up work on my Git repositories wherever I happen to be. That means a remote sync solution that bridges my iOS devices (iPad and iPhone) and my Linux machine.\nAfter a lot of trial and error, I\u0026rsquo;ve found one that works really well. With synced Git repositories on iOS, I can seamlessly pick up work for any of my repositories on the go. This level of convenience has earned a permanent spot in the list of *ware I use.\nComponents  Working Copy app ($15.99 one-time pro-unlock and well worth it) iA Writer app ($8.99 one-time purchase for iOS, also available on Mac, Windows, and Android) GitHub repositories  Get set up Here are the steps to setting up that I\u0026rsquo;ll walk you through in this article.\n Create your remote repository Clone repository to iPad with Working Copy Open and edit files with iA Writer Push changes back to remote Pull changes from repository on your computer  This system is straightforward to set up whether you\u0026rsquo;re a command line whiz or just getting into Git. Let\u0026rsquo;s do it!\nCreate your remote repository Create a public or private repository on GitHub.\nIf you\u0026rsquo;re creating a new repository, you can follow GitHub\u0026rsquo;s instructions to push some files to it from your computer, or you can add files later from your iOS device.\nClone repository to iOS with Working Copy Download Working Copy from the App Store. It\u0026rsquo;s a fantastic app. Developer Anders Borum has a steady track record of frequent updates and incorporating the latest features for iOS apps, like drag and drop on iPad. I think he\u0026rsquo;s fairly priced his product in light of the work he puts into maintaining and enhancing it.\nIn Working Copy, find the gear icon in the top left corner and touch to open Settings.\nTap on SSH Keys, and you\u0026rsquo;ll see this screen:\nSSH keys, or Secure Shell keys, are access credentials used in the SSH protocol. Your key is a password that your device will use to securely connect with your remote repository host - GitHub, in this example. Since anyone with your SSH keys can potentially pretend to be you and gain access to your files, it\u0026rsquo;s important not to share them accidentally, like in a screenshot on a blog post.\nTap on the second line that looks like WorkingCopy@iPad-xxxxxxxx to get this screen:\nWorking Copy supports easy connection to GitHub. Tap Connect With GitHub to bring up some familiar sign-in screens that will authorize Working Copy to access your account(s).\nOnce connected, tap the + symbol in the top right of the side bar to add a new repository. Choose Clone repository to bring up this screen:\nHere, you can either manually input the remote URL, or simply choose from the list of repositories that Working Copy fetches from your connected account. When you make your choice, the app clones the repository to your device and it will show up in the sidebar. You\u0026rsquo;re connected!\nOpen and edit files with iA Writer One of the (many) reasons I adore iA Writer is its ability to select your freshly cloned remote repository as a Library Location. To enable this, first open your Files app. On the Browse screen, tap the overflow menu (three dots) in the top right and choose Edit.\nTurn on Working Copy as a location option:\nThen in the iA Writer app:\n From the main Library list, in the top right of the sidebar, tap Edit. Tap Add Location\u0026hellip;. A helpful popup appears. Tap OK. From the Working Copy location, tap Select in the top right, then choose the repository folder. Tap Open, then Done.  Your remote repository now appears as a Location in the sidebar. Tap on it to work within this directory.\nWhile inside this location, new files you create (by tapping the pencil-and-paper icon in the top right corner) will be saved to this folder locally. As you work, iA Writer automatically saves your progress. Next, we\u0026rsquo;ll look at pushing those files and changes back to your remote.\nPush changes back to remote Once you\u0026rsquo;ve made changes to your files, open Working Copy again. You should see a yellow dot on your changed repository.\nTap on your repository name, then on Repository Status and Configuration at the top of the sidebar. Your changed files will be indicated by yellow dots or green + symbols. These mean that you\u0026rsquo;ve modified or added files, respectively.\nWorking Copy is a sweet iOS Git client, and you can tap on your files to see additional information including a comparison of changes (\u0026ldquo;diff\u0026rdquo;) as well as status and Git history. You can even edit files right within the app, with syntax highlighting for its many supported languages. For now, we\u0026rsquo;ll look at how to push your changed work to your remote repository.\nOn the Repository Status and Configuration page, you\u0026rsquo;ll see right at the top that there are changes to be committed. If you\u0026rsquo;re new to Git, this is like \u0026ldquo;saving your changes\u0026rdquo; to your Git history, something typically done with the terminal command git commit. You can think of this as saving the files that we\u0026rsquo;ll want to send to the GitHub repository. Tap Commit changes.\nEnter your commit message, and select the files you want to add. Toggle the Push switch to send everything to your remote repository when you commit the files. Then tap Commit.\nYou\u0026rsquo;ll see a progress bar as your files are uploaded, and then a confirmation message on the status screen.\nCongratulations! Your changes are now present in your remote repository on GitHub. You\u0026rsquo;ve successfully synced your files remotely!\nPull changes from repository on your computer To bring your updated files full circle to your computer, you pull them from the GitHub repository. I prefer to use the terminal for this as it\u0026rsquo;s quick and easy, but GitHub also offers a graphical client if terminal commands seem a little alien for now.\nIf you started with the GitHub repository, you can clone it to a folder on your computer by following these instructions.\nStaying in sync When you update your work on your computer, you\u0026rsquo;ll use Git to push your changes to the remote repository. To do this, you can use GitHub\u0026rsquo;s graphical client, or follow these instructions.\nOn your iOS device, Working Copy makes pulling and pushing as simple as a single tap. On the Repository Status and Configuration page, tap on the remote name under Remotes.\nThen tap Synchronize. Working Copy will take care of the details of pushing your committed changes and/or pulling any new changes it finds from the remote repository.\nWork anywhere For a Git-based developer and work-anywhere-aholic like me, this set up couldn\u0026rsquo;t be more convenient. Working Copy really makes staying in sync with my remote repositories seamless, nevermind the ability to work with any of my GitHub repos on the go.\nI most recently used this set up to get some writing done while hanging out in the atrium of Washington DC\u0026rsquo;s National Portrait Gallery, which is pleasantly photogenic.\nHappy working! If you enjoyed this post, there\u0026rsquo;s a lot more where this came from! I write about computing, cybersecurity, and leading great technical teams. You can subscribe to see new articles first.\n",url:"https://victoria.dev/blog/a-remote-sync-solution-for-ios-and-linux-git-and-working-copy/"},"https:\/\/victoria.dev\/blog\/on-doing-great-things\/":{title:"On doing great things",tags:["life"],content:"It\u0026rsquo;s International Women\u0026rsquo;s Day, and I\u0026rsquo;m thinking about Grace Hopper.\nGrace Hopper was an amazing lady who did great things. She envisioned and helped create programming languages that translate English terms into machine code. She persevered in her intention to join the US Navy from the time she was rejected at 34 years old, to being sworn in to the US Navy Reserve three years later, to retiring with the rank of commander at age 60\u0026hellip; then was recalled (twice) and promoted to the rank of captain at the age of 67. She advocated for distributed networks and developed computer testing standards we use today, among other achievements too numerous to list here.\nBy my read, throughout her life, she kept her focus on her work. She did great things because she could do them, and felt some duty to do them. Her work speaks for itself.\nI recently came across a sizeable rock denoting a rather small, quiet park. It looks like this:\nWhen I first saw this park, I thought it in no way did this great lady justice. But upon some reflection, its lack of assumption and grandeur grew on me. And today, it drew to the forefront something that\u0026rsquo;s been on my mind.\nI try and contribute regularly to the wide world of technology, usually through building things, writing, and mentorship. I sometimes get asked to participate in female-focused tech events. I hear things like, \u0026ldquo;too few developers are women,\u0026rdquo; or \u0026ldquo;we need more women in blockchain,\u0026rdquo; or \u0026ldquo;we need more female coders.\u0026rdquo;\nFor some time I haven\u0026rsquo;t been sure how to respond, because while my answer isn\u0026rsquo;t \u0026ldquo;yes,\u0026rdquo; it\u0026rsquo;s not exactly \u0026ldquo;no,\u0026rdquo; either. It\u0026rsquo;s really, \u0026ldquo;no, because\u0026hellip;\u0026rdquo; and it\u0026rsquo;s because I\u0026rsquo;m afraid. I\u0026rsquo;m afraid of misrepresenting myself, my values, and my goals.\nDiscrimination and racism are real things. They exist in the minds and attitudes of a very small percentage of very loud people, as they always will. These people aren\u0026rsquo;t, however, the majority. They are small.\nI think that on the infrequent occasions when we encounter these people, we should do our best to lead by example. We should have open minds, tell our stories, listen to theirs. Try and learn something. That\u0026rsquo;s all.\nWhen I present myself, I don\u0026rsquo;t point out that I\u0026rsquo;m a woman. I don\u0026rsquo;t align myself with \u0026ldquo;women in tech\u0026rdquo; or seek to represent them. I don\u0026rsquo;t go to women-only meetings or support organizations that discriminate against men, or anyone at all. It\u0026rsquo;s not because I\u0026rsquo;m insecure as a woman, or ashamed that I\u0026rsquo;m a woman, or some other inflammatory adjective that lately shows up in conjunction with being female. It\u0026rsquo;s because I\u0026rsquo;ve no reason to point out my gender, any more than needing to point out that my hair is black, or that I\u0026rsquo;m short. It\u0026rsquo;s obvious and simultaneously irrelevant.\nWhen I identify with a group, I talk about the go-getters who wake up at 0500 every day and go work out - no matter the weather, or whether they feel like it. I tell stories about the people I met in different countries around the world, who left home, struck out on their own, and had an adventure, because they saw value in the experience. I identify with people who constantly build things, try things, design and make things, and then share those things with the world, because they love to do so. This is how I see myself. This is what matters to me.\nLike the unassuming park named after an amazing woman, when truly great things are done, they are done relatively quietly. Not done for the fanfare of announcing them to the world, but for the love of the thing itself. So go do great things, please. The world still needs them.\n",url:"https://victoria.dev/blog/on-doing-great-things/"},"https:\/\/victoria.dev\/neofeed\/06601\/":{title:"06601",tags:[],content:"A computer without an Internet connection is just an empty shell.\n",url:"https://victoria.dev/neofeed/06601/"},"https:\/\/victoria.dev\/blog\/git-commit-practices-your-future-self-will-thank-you-for\/":{title:"Git commit practices your future self will thank you for",tags:["git","coding","terminal","tech-team"],content:"A history of clean commits can be evidence of a lot of things: attention to detail, good work ethic, and genuine investment in the project. What do your Git commits say about you?\nHere\u0026rsquo;s how you can create and maintain a clean and orderly Git commit history using message templates, learning how to squash commits, using git stash, and creating annotated commit tags.\nWhat it means to commit responsibly Whether our code will be seen by the entire open source community or just future versions of ourselves, either one will be grateful if we commit responsibly today. Being responsible can mean a lot of things to different people, so I enlisted some of mastodon.technology and dev.to to help round out my list. From those (really great) threads, I distilled these main points:\n Committing responsibly\n Provide and/or use tests to avoid committing bugs or broken builds Write clean code that meets style specifications Use descriptive commit messages that reference related discussion Make only one change per commit and avoid including unrelated changes   Some of the above is achieved through maintaining a short feedback loop that helps you improve your code quality while staying accountable to yourself. I wrote another article that discusses this in detail, especially the part about code review. Other items on this list have to do specifically with making commits in Git. There are some features of Git that can benefit us in these areas, as can harnessing tools like Vim. I\u0026rsquo;ll cover those topics here.\nIf the majority of your Git commits so far have been created with something like git commit -m \u0026quot;Bug fixes\u0026quot; then this is the article for you!\nWrite great Git commit messages with a template I think Linus would be very happy if we didn\u0026rsquo;t use git commit -m \u0026quot;Fix bug\u0026quot; in a public repository ever again. As very well put in this classic post and the seven rules of a great Git commit message:\n A properly formed Git commit subject line should always be able to complete the following sentence:\nIf applied, this commit will your subject line here\n This other classic post also discusses three questions that the body of the commit message should answer:\n Why is it necessary? How does it address the issue? What effects does the patch have?\n This can be a lot to remember to cover, but there\u0026rsquo;s a slick way to have these prompts at hand right when you need it. You can set up a commit message template by using the commit.template configuration value.\nTo set it, configure Git to use a template file (for example, .gitmessage in your home directory), then create the template file with Vim:\ngit config --global commit.template ~/.gitmessage vim ~/.gitmessage When we run git commit without the -m message flag, the editor will open with our helpful template ready to go. Here\u0026rsquo;s my commit message template:\n## If applied, this commit will... ## [Add/Fix/Remove/Update/Refactor/Document] [issue #id] [summary] ## Why is it necessary? (Bug fix, feature, improvements?) - ## How does the change address the issue? - ## What side effects does this change have? - I\u0026rsquo;m a fan of this format because commented lines are not included in the final message. I can simply fill in the blank lines with text and bullet points under the prompts, and it comes out looking something like this:\nFix #16 missing CSS variables - Fix for unstyled elements - Add background color, height for code blocks - Only affects highlight class Reference related discussion Issue trackers in GitHub and Bitbucket both recognize the keywords close, fix, and resolve followed immediately by the issue or pull request number. These keywords conveniently help us close the referenced issue or pull request, and this helps maintain a clear trail of changes. GitLab, and issue trackers like Jira offer similar functionalities.\nUse helpful Vim settings for git commit messages By adding a few lines to our Vim configuration, we can make writing great git commit messages easy. We can add these lines to ~/.vimrc to turn on syntax highlighting in general, and spell check and text wrapping for commit messages in particular:\n\u0026#34; Filetype detection, plugins, and indent rulesfiletype plugin indent on\u0026#34; Syntax highlightingsyntax on\u0026#34; Spell check and line wrap just for git commit messagesautocmd Filetype gitcommit setlocal spell textwidth=72If you\u0026rsquo;re curious, you can find my full ~/.vimrc in my dotfiles.\nOther editors have settings that can help us out as well. I came across these for Sublime Text 3 and language specific settings for VS Code.\nOne change per commit: how to squash Git commits  Still life Git\n  Let\u0026rsquo;s get one thing out of the way first: rewriting Git history just for the sake of having a pretty tree, especially with public repositories, is generally not advisable. It\u0026rsquo;s kind of like going back in time, where changes you make to your version of the project cause it to look completely different from a version that someone else forked from a point in history that you\u0026rsquo;ve now erased - I mean, haven\u0026rsquo;t you seen Back to the Future Part II? (If you\u0026rsquo;d rather maintain that only one Back to the Future movie was ever made, thus sparing your future self from having to watch the sequels, I get it.)\nHere\u0026rsquo;s the main point. If you\u0026rsquo;ve pushed messy commits to a public repository, I say go right ahead and leave them be, instead of complicating things further. (We all learn from our embarrassments, especially the public ones - I\u0026rsquo;m looking at you, past-Vicky.) If your messy commits currently only exist on your local version, great! We can tidy them up into one clean, well-described commit that we\u0026rsquo;ll be proud to push, and no one will be the wiser.\nThere are a couple different ways to squash commits, and choosing the appropriate one depends on what we need to achieve.\nThe following examples are illustrated using git log --graph, with some options for brevity. We can set a handy alias to see this log format in our terminal with:\ngit config --global alias.plog \u0026#34;log --graph --pretty=format:\u0026#39;%h -%d %s %n\u0026#39; --abbrev-commit --date=relative --branches\u0026#34; Then we just do git plog to see the pretty log.\nMethod #1: one commit to rule the master branch This is appropriate when:\n We\u0026rsquo;re committing directly to master We don\u0026rsquo;t intend to open a pull request to merge a feature We don\u0026rsquo;t want to preserve history of branches or changes we haven\u0026rsquo;t yet pushed  This method takes a Git tree that looks like this:\n* 3e8fd79 - (HEAD -\u0026gt; master) Fix a thing | * 4f0d387 - Tweak something | * 0a6b8b3 - Merge branch \u0026#39;new-article\u0026#39; |\\ | * 33b5509 - (new-article) Update article again again | | | * 1782e63 - Update article again | | | * 3c5b6a8 - Update article | | * | f790737 - (master) Tweak unrelated article |/ | * 65af7e7 Add social media link | * 0e3fa32 (origin/master, origin/HEAD) Update theme And makes it look like this:\n* 7f9a127 - (HEAD -\u0026gt; master) Add new article | * 0e3fa32 - (origin/master, origin/HEAD) Update theme Here\u0026rsquo;s how to do it - hold on to your hoverboards, it\u0026rsquo;s super complicated:\ngit reset --soft origin/master git commit Yup that\u0026rsquo;s all. We can delete the unwanted branch with git branch -D new-article.\nMethod #2: not that much This is appropriate when:\n We want to squash the last x commits but not all commits since origin/master We want to open a pull request to merge a branch  This method takes a Git tree that looks like this:\n* 13a070f - (HEAD -\u0026gt; new-article) Finish new article | * 78e728a - Edit article draft | * d62603c - Add example | * 1aeb20e - Update draft | * 5a8442a - Add new article draft | | * 65af7e7 - (master) Add social media link |/ | * 0e3fa32 - (origin/master, origin/HEAD) Update theme And makes it look like this:\n* 90da69a - (HEAD -\u0026gt; new-article) Add new article | | * 65af7e7 - (master) Add social media link |/ | * 0e3fa32 - (origin/master, origin/HEAD) Update theme To squash the last five commits on branch new-article into one, we use:\ngit reset --soft HEAD~5 git commit -m \u0026#34;New message for the combined commit\u0026#34; Where --soft leaves our files untouched and staged, and 5 can be thought of as \u0026ldquo;the number of previous commits I want to combine.\u0026rdquo;\nWe can then do git merge master and create our pull request.\nMethod #3: getting picky Say we had a really confusing afternoon and our Git tree looks like this:\n* dc89918 - (HEAD -\u0026gt; master) Add link | * 9b6780f - Update image asset | * 6379956 - Fix CSS bug | * 16ee1f3 - Merge master into branch |\\ | | | * ccec365 - Update list page | | * | 033dee7 - Fix typo | | * | 90da69a - Add new article |/ | * 0e3fa32 - (origin/master, origin/HEAD) Update theme We want to retain some of this history, but clean up the commits. We also want to change the messages for some of the commits. To achieve this, we\u0026rsquo;ll use git rebase.\nThis is appropriate when:\n We want to squash only some commits We want to edit previous commit messages We want to delete or reorder specific commits  Git rebase is a powerful tool, and handy once we\u0026rsquo;ve got the hang of it. To change all the commits since origin/master, we do:\ngit rebase -i origin/master Or, we can do:\ngit rebase -i 0e3fa32 Where the commit hash is the last commit we want to retain as-is.\nThe -i option lets us run the interactive rebase tool, which launches our editor with, essentially, a script for us to modify. We\u0026rsquo;ll see a list of our commits in reverse order to the git log, with the oldest at the top:\npick 90da69a Add new article pick 033dee7 Fix typo pick ccec365 Update list page pick 6379956 Fix CSS bug pick 9b6780f Update image asset pick dc89918 Add link # Rebase 0e3fa32..dc89918 onto 0e3fa32 (6 commands) # # Commands: # p, pick = use commit # r, reword = use commit, but edit the commit message # e, edit = use commit, but stop for amending # s, squash = use commit, but meld into previous commit # f, fixup = like \u0026#34;squash\u0026#34;, but discard this commit\u0026#39;s log message # x, exec = run command (the rest of the line) using shell # d, drop = remove commit # # These lines can be re-ordered; they are executed from top to bottom. # # If you remove a line here THAT COMMIT WILL BE LOST. # # However, if you remove everything, the rebase will be aborted. # # Note that empty commits are commented out # ~ The comments give us a handy guide as to what we\u0026rsquo;re able to do. For now, let\u0026rsquo;s squash the commits with small changes into the more significant commits. In our editor, we change the script to look like this:\npick 90da69a Add new article squash 033dee7 Fix typo pick ccec365 Update list page squash 6379956 Fix CSS bug squash 9b6780f Update image asset squash dc89918 Add link Once we save the changes, the interactive tool continues to run. It will execute our instructions in sequence. In this case, we see the editor again with the following:\n# This is a combination of 2 commits. # This is the 1st commit message: Add new article # This is the commit message #2: Fix typo # Please enter the commit message for your changes. Lines starting # with \u0026#39;#\u0026#39; will be ignored, and an empty message aborts the commit. # # interactive rebase in progress; onto 0e3fa32 # Last commands done (2 commands done): # pick 90da69a Add new article # squash 033dee7 Fix typo # Next commands to do (4 remaining commands): # pick ccec365 Update list page # squash 6379956 Fix CSS bug # You are currently rebasing branch \u0026#39;master\u0026#39; on \u0026#39;0e3fa32\u0026#39;. # # Changes to be committed: # modified: ... # ~ Here\u0026rsquo;s our chance to create a new commit message for this first squash, if we want to. Once we save it, the interactive tool will go on to the next instructions. Unless\u0026hellip;\n[detached HEAD 3cbad01] Add new article 1 file changed, 129 insertions(+), 19 deletions(-) Auto-merging content/dir/file.md CONFLICT (content): Merge conflict in content/dir/file.md error: could not apply ccec365... Update list page Resolve all conflicts manually, mark them as resolved with \u0026#34;git add/rm \u0026lt;conflicted_files\u0026gt;\u0026#34;, then run \u0026#34;git rebase --continue\u0026#34;. You can instead skip this commit: run \u0026#34;git rebase --skip\u0026#34;. To abort and get back to the state before \u0026#34;git rebase\u0026#34;, run \u0026#34;git rebase --abort\u0026#34;. Could not apply ccec365... Update list page Again, the tool offers some very helpful instructions. Once we fix the merge conflict, we can resume the process with git rebase --continue. Our interactive rebase picks up where it left off.\nOnce all the squashing is done, our Git tree looks like this:\n* 3564b8c - (HEAD -\u0026gt; master) Update list page | * 3cbad01 - Add new article | * 0e3fa32 - (origin/master, origin/HEAD) Update theme Phew, much better.\nGit stash If we\u0026rsquo;re in the middle of some work and it\u0026rsquo;s not a good time to commit, but we need to switch branches, stashing can be a good option. Stashing lets us save our unfinished work without needing to create a half-assed commit. It\u0026rsquo;s like that pile of paper on your desk representing all the stuff you\u0026rsquo;ve been in the middle of doing since two weeks ago. Yup, that one.\nIt\u0026rsquo;s as easy as typing git stash:\nSaved working directory and index state WIP on master: 3564b8c Update list page The dirty work we\u0026rsquo;re in the midst of is safely tucked away, and our working directory is clean - just as it was after our last commit. To see what\u0026rsquo;s in our stash stack, we do git stash list:\nstash@{0}: WIP on master: 3564b8c Update list page stash@{1}: WIP on master: 90da69a Add new article stash@{2}: WIP on cleanup: 0e3fa32 Update theme To restore our work in progress, we use git stash apply. Git will try and apply our most recent stashed work. To apply an older stash, we use git stash apply stash@{1} where 1 is the stash to apply. If changes since stashing our work prevent the stash from reapplying cleanly, Git will give us a merge conflict to resolve.\nApplying a stash doesn\u0026rsquo;t remove it from our list. To remove a stash from our stack, we do git stash drop stash@{0} where 0 is the one we want to remove.\nWe can also use git stash pop to apply the most recent stash and then immediately remove it from the stack.\nTag release versions using annotated Git tags In the spirit of having a beautiful, clean Git history, there\u0026rsquo;s one more thing we can do to help make our commit log inspire infinite joy in its viewers. If you\u0026rsquo;ve never heard of git tag, your master branch history might look like this\u0026hellip;\n* 0377782 - Update theme | * ecf8128 - Add about page (#25) | * 33e432f - Fix #23 navigation bug | * 08b853b - Create blog section | * 63d18b4 - Add theme (#12) | * 233e23f - Add main content (#6) Wouldn\u0026rsquo;t it be nice if it looked like this instead?\n* 0377782 - (tag: v2.1.0) Update theme | * ecf8128 - Add about page (#25) | * 33e432f - Fix #23 navigation bug | * 08b853b - (tag: v2.0.0) Create blog section | * 63d18b4 - Add theme (#12) | * 233e23f - (tag: v1.1.0) Add main content (#6) We can tag Git commits with anything, but tags are especially helpful for semantic versioning of releases. Sites like GitHub and GitLab have pages for repositories that list tags, letting viewers of our project browse the release versions. This can be helpful for public projects to differentiate major releases, updates with bug fixes, or beta versions.\nThere are two types of Git tags: lightweight and annotated. For adding a version tag to commits, we use annotated Git tags.\nThe Git tag documentation explains it this way:\n Tag objects (created with -a, -s, or -u) are called \u0026ldquo;annotated\u0026rdquo; tags; they contain a creation date, the tagger name and e-mail, a tagging message, and an optional GnuPG signature. Whereas a \u0026ldquo;lightweight\u0026rdquo; tag is simply a name for an object (usually a commit object).\nAnnotated tags are meant for release while lightweight tags are meant for private or temporary object labels. For this reason, some git commands for naming objects (like git describe) will ignore lightweight tags by default.\n We can think of lightweight tags as bookmarks, and annotated tags as signed releases.\nFor public repositories, annotated tags allow us to:\n See who tagged the commit, which may differ from the commit author See all the tags with git describe Avoid conflicting tag names  To create an annotated Git tag and attach it to our current (last) commit, we do:\ngit tag -a v1.2.0 -m \u0026#34;Clever release title\u0026#34; This tags the commit on our local repository. To push all annotated tags to the remote, we do:\ngit push --follow-tags We can also set our Git configuration to push our annotated tags by default:\ngit config --global push.followTags true If we then want to skip pushing tags this time, we pass --no-follow-tags.\nPractice responsible commits A little time invested in getting familiar with these tools and practices can make your commits even more useful and well-crafted. With a little practice, these processes will become second nature. You can make it even easier by creating a personal commit checklist on paper to keep handy while you work - or if that isn\u0026rsquo;t fun enough, make it an interactive pre-commit hook.\nCreating clean, useful, and responsible Git commits says a lot about you. Especially in remote work, Git commits may be a primary way that people interact with you over projects. With a little practice and effort, you can make your commit habits an even better reflection of your best work - work that is evidently created with care and pride.\nIf you enjoyed this post, there\u0026rsquo;s a lot more where it came from! I write about computing, cybersecurity, and leading great technical teams. Subscribe on victoria.dev to see new articles first, and check out the ones below!\n",url:"https://victoria.dev/blog/git-commit-practices-your-future-self-will-thank-you-for/"},"https:\/\/victoria.dev\/blog\/an-automatic-interactive-pre-commit-checklist-in-the-style-of-infomercials\/":{title:"An automatic interactive pre-commit checklist, in the style of infomercials",tags:["git","coding","terminal"],content:"What\u0026rsquo;s that, you say? You\u0026rsquo;ve become tired of regular old boring paper checklists? Well, my friend, today is your lucky day! You, yes, you, can become the proud owner of a brand-spanking-new automatic interactive pre-commit hook checklist! You\u0026rsquo;re gonna love this! Your life will be so much easier! Just wait until your friends see you.\nWhat\u0026rsquo;s a pre-commit hook Did you know that nearly 1 out of 5 coders are too embarrassed to ask this question? Don\u0026rsquo;t worry, it\u0026rsquo;s perfectly normal. In the next 60 seconds we\u0026rsquo;ll tell you all you need to know to pre-commit with confidence.\nA Git hook is a feature of Git that triggers custom scripts at useful moments. They can be used for all kinds of reasons to help you automate your work, and best of all, you already have them! In every repository that you initialize with git init, you\u0026rsquo;ll have a set of example scripts living in .git/hooks. They all end with .sample and activating them is as easy as renaming the file to remove the .sample part.\nGit hooks are not copied when a repository is cloned, so you can make them as personal as you like.\nThe useful moment in particular that we\u0026rsquo;ll talk about today is the pre-commit. This hook is run after you do git commit, and before you write a commit message. Exiting this hook with a non-zero status will abort the commit, which makes it extremely useful for last-minute quality checks. Or, a bit of fun. Why not both!\nHow do I get a pre-commit checklist I only want the best for my family and my commits, and that\u0026rsquo;s why I choose an interactive pre-commit checklist. Not only is it fun to use, it helps to keep my projects safe from unexpected off-spec mistakes!\nIt\u0026rsquo;s so easy! I just write a bash script that can read user input, and plop it into .git/hooks as a file named pre-commit. Then I do chmod +x .git/hooks/pre-commit to make it executable, and I\u0026rsquo;m done!\nOh look, here comes an example bash script now!\n#!/bin/sh  echo \u0026#34;Would you like to play a game?\u0026#34; # Read user input, assign stdin to keyboard exec \u0026lt; /dev/tty while read -p \u0026#34;Have you double checked that only relevant files were added? (Y/n) \u0026#34; yn; do case $yn in [Yy] ) break;; [Nn] ) echo \u0026#34;Please ensure the right files were added!\u0026#34;; exit 1;; * ) echo \u0026#34;Please answer y (yes) or n (no):\u0026#34; \u0026amp;\u0026amp; continue; esac done while read -p \u0026#34;Has the documentation been updated? (Y/n) \u0026#34; yn; do case $yn in [Yy] ) break;; [Nn] ) echo \u0026#34;Please add or update the docs!\u0026#34;; exit 1;; * ) echo \u0026#34;Please answer y (yes) or n (no):\u0026#34; \u0026amp;\u0026amp; continue; esac done while read -p \u0026#34;Do you know which issue or PR numbers to reference? (Y/n) \u0026#34; yn; do case $yn in [Yy] ) break;; [Nn] ) echo \u0026#34;Better go check those tracking numbers!\u0026#34;; exit 1;; * ) echo \u0026#34;Please answer y (yes) or n (no):\u0026#34; \u0026amp;\u0026amp; continue; esac done exec \u0026lt;\u0026amp;- Take my money Don\u0026rsquo;t delay! Take advantage right now of this generous one-time offer! An interactive pre-commit hook checklist can be yours, today, for the low, low price of\u0026hellip; free? Wait, who wrote this script?\n",url:"https://victoria.dev/blog/an-automatic-interactive-pre-commit-checklist-in-the-style-of-infomercials/"},"https:\/\/victoria.dev\/blog\/how-to-set-up-a-short-feedback-loop-as-a-solo-coder\/":{title:"How to set up a short feedback loop as a solo coder",tags:["coding","life"],content:"I\u0026rsquo;ve spent the last couple years as a solo freelance developer. Comparing this experience to previously working in companies, I\u0026rsquo;ve noticed that those of us who work alone can have fewer iterative opportunities for improvement than developers who work on teams. Integral to having opportunities to improve is the concept of a short feedback loop: a process of incorporating new learning from observation and previous experience continuously over a short period of time. This process has to be manufactured by people working mostly alone, instead of, as is often the case, adopted when you join a team.\nIn this post I hope to share what I\u0026rsquo;ve learned about setting yourself up to improve quickly and continuously as a solo coder.\nAbout feedback loops United States Air Force Colonel John Boyd developed the concept of the OODA loop, OODA being an acronym for observe, orient, decide, act. In military operations, this illustrates a process of decision-making based on the constant ingestion of new information:\nObserve: Obtain raw information about unfolding circumstances and the current environment. Orient: Put raw observations in context. Consider such things as relevancy to the current situation and previously gained knowledge and expertise. Decide: Make a plan for moving towards your goal. Act: Execute the plan.\nSince it\u0026rsquo;s a loop, the act stage leads directly back into the observe stage. This is the critical \u0026ldquo;feed back\u0026rdquo; concept that enables increasingly successful iterations. It\u0026rsquo;s widely applicable beyond military operations - you may recognize it as the origin of the PDCA (plan-do-check-act) method.\nI like the OODA loop for being a succinct illustration of a general feedback loop. Many concepts and working methods build on the idea of feedback loops, including DevOps and Agile software development methods.\nDevelopment team feedback loop Let\u0026rsquo;s look at what some components of a feedback loop for a developer on a team might look like:\n Direction from product owners or reviews from users Daily scrum/standup with whole team Prioritization with developer team Individual coding and testing Peer code review Deployment and performance monitoring  Implicit in these steps is the support of co-workers and management - in other words, someone to answer to. How can a solo freelance developer create a similar environment of accountability?\nSolo developer feedback loop Here are some possible steps that an individual freelance developer can implement to create a short feedback loop:\n Build discipline Clarify concrete top-level goals Prioritize and plan mid-level and low-level goals Automate your work Block out time for code review Block out time for process review Update your goals and processes with the results of your reviews  I\u0026rsquo;ll cover each of these stages in detail below.\nBuild discipline More of a prerequisite than a stage in itself, building discipline is what enables our short feedback loop to work. Nothing else in this article will be helpful unless we have the skill to do something we don\u0026rsquo;t want to do. Discipline is most certainly a skill. It can be learned, trained, and improved just like any other.\nWhy is discipline so important? Because when we\u0026rsquo;re crunching to get a project completed this Friday evening, we\u0026rsquo;re not going to want to write a good commit message. We\u0026rsquo;re not going to want to clean up the code comments. We just want to see the darn thing go, Hello, git push -f. It\u0026rsquo;s in those moments that discipline enables us to not miss an opportunity to practice, learn, and improve our work process. Discipline helps us avoid Friday night commits that turn into Monday morning git reset --hards.\nClarify concrete top-level goals Whether working for a client or bootstrapping our own best-new-app-ever, we won\u0026rsquo;t be able to measure any progress or improvements without something to measure them against.\nWhen I\u0026rsquo;m discussing a new project with a client, I always speak in terms of concrete achievements. This could take the form of accomplishing a specific feature by a certain date, or deciding what the MVP looks like to a user. This is as much to my benefit as my client\u0026rsquo;s. By agreeing, in writing, what will be achieved and when, my client and I have clearly defined top-level goals and can both assess how the project is progressing. When I\u0026rsquo;m working for myself, I treat myself as I would a client. I make a commitment, in writing, describing what will be achieved, and when. This can be something as simple as a goals list for the week, or as detailed as a kanban board.\nThe point of having a concrete goal, however, is not to stick to it at all costs. It\u0026rsquo;s important to set an expectation, with ourselves and with our clients, that the goals will be revisited at mutually-agreeable dates over the course of the project. This enables the all-important \u0026ldquo;feed back\u0026rdquo; part of the loop.\nPrioritize and plan mid-level and low-level goals Few goals are achieved all in one step. Even the simple process of making a peanut butter and jelly sandwich (a favourite computer programming teaching example) can be broken down into successively smaller, more precise instructions. While we humans may not require the granularity that a computer program does, goals that are chunked into time-boxed, achievable steps are much more easily digested. 🥪\nStart with the mid-level goals, and make each step concrete. If the goal is to release a new open source web app, for example, the steps might look like this:\n Complete app JavaScript Create front end and stylesheet Do local tests Set up cloud server Deploy app to cloud Do tests Add repository to GitHub Post on Hacker News Profit!!!  Each of the above examples encapsulates many smaller, low-level goals - we can think of these as our to-do list items. For example, \u0026ldquo;Set up cloud server\u0026rdquo; might involve:\n Research cloud providers Decide on service and sign up Set up server/instance Add integrations Test deployment  Our parameters for chunk sizes and what constitutes a \u0026ldquo;step\u0026rdquo; may be different from one another, and will likely change from project to project. If your mid-level and low-level steps clearly define a concrete path for achieving the top-level goals you set, then you\u0026rsquo;re in good shape. Later on, evaluating the decision process that brought us to these mid-level and low-level goals enables us to bring our feedback loop full circle.\nAutomate your work I recently read a great article entitled Manual Work is a Bug. It discusses a process by which successful developers document and eventually automate their work. The beauty of this idea is in its simplicity. By writing down the things we do manually, we\u0026rsquo;re able to correct and refine our processes. By refining our processes, we can more easily translate them into code snippets and scripts. With a collection of scripts that we can string together, we can automate our work.\nAutomating work isn\u0026rsquo;t only about saving time. It reduces haven\u0026rsquo;t-had-my-coffee-yet errors, minimizes cognitive load allowing more room for creativity, and allows our processes to be repeatable across collaborators and projects. It help shorten our feedback loop by ensuring we aren\u0026rsquo;t doing the same thing three times in three different ways.\nWe can begin to automate by starting our own personal wiki. If we build a habit of writing down every manual thing we do, no matter how basic it may seem at the time, we give ourselves more opportunities to spot patterns, and thus possible integrations and improvements.\nThe first time we do something manually, we write out the steps. The second time, we follow the steps. This gives us the opportunity to correct and refine them based on what we\u0026rsquo;ve learned since the first time. Over successive iterations, we might replace parts of manual commands with variables; we might find handy snippets of bash scripts that automate just a part of our task. As long as we keep revising and improving our personal wiki, we\u0026rsquo;re moving towards automation.\nBlock out time for code review It\u0026rsquo;s all too easy to commit messy code when we work alone. We think, who\u0026rsquo;s going to see it? I\u0026rsquo;ll fix it later. Each time that happens, though, we\u0026rsquo;re building a habit. It\u0026rsquo;s a bad one.\nWorking alone means there\u0026rsquo;s no one likely to give feedback on our commits when we\u0026rsquo;re doing something that doesn\u0026rsquo;t make sense, or that could be improved. Instead, we have to actively seek out opportunities to improve. Open source communities are amazing for this. There\u0026rsquo;s a wealth of information available to us in terms of coding styles, examples of refactored code, and a smorgasbord of snippets that achieve that-thing-we-were-trying-to-do but in fewer lines. We can learn all we please, if we just block out the time to do it.\nSchedule your own code review at a time that makes sense for you and the project you\u0026rsquo;re working on. This might be each time you finish a fix or feature, or at regular intervals daily or weekly. If you have someone who can help, book them. There are also chatrooms full of people happy to lend a hand.\nDo some research on basic best practices for what you\u0026rsquo;re working on. Set yourself a time limit though, and take what you read with a grain of salt. There\u0026rsquo;s a lot of rabbit holes in that field. As a starting point, I\u0026rsquo;d recommend learning about DRY code, and watching Uncle Bob demand professionalism in software development.\nCode review checklist Here\u0026rsquo;s my personal code review checklist, based off some general best practices. Feel free to use it as a starting point for your own!\n Victoria\u0026rsquo;s Code Review Extravaganza!\n This solves a high-priority item. This is a complete implementation that follows the specification. Off-topic changes were not included and have been added to backlog. Variable names are meaningful and there are no magic numbers. Correct and useful error messages are returned at every opportunity. No debugging print statements were left in. This code is DRY and modular. This code is secure. Private and public code are well separated. This code is its own documentation, or the documentation is up to date. A five-year-old could follow this, seriously it\u0026rsquo;s that readable. Unit tests successfully pass. Master was merged into the branch and tested. Formatting follows style guidelines. I cannot find any further edge cases or known defects. I would be happy if this code was publicly attributed to me. I fully understand what the code does and the impact of the changes I made. I actually verified that it actually does what I said it does.   Here is an excellent example of cleaning up code with some of the above points in mind.\nBlock out time for process review Just as we learn from reviewing our code, we refine our processes by reviewing them as well. Process review is most beneficial when visited at regular intervals throughout the project, not just after the project\u0026rsquo;s completion. For short-term projects, a good starting point for scheduling process reviews is at each half-mark - once midway through, and again after completion. Long-term projects may have reviews at each quarter-mark.\nProcess review questions Process review can be as simple as a short list of questions:\n What were my top-level goals for this period? Did I meet them? What were my mid-level and low-level goals for this period? Did I meet them? Would I have been better served with different or more specific goals? Why? Did I successfully remove or automate obstacles? Did I stick to my code review schedule? Why or why not? How might I remove obstacles next time?  Setting aside dedicated time for our process review can help us to answer questions like these thoughtfully and honestly. This allows us to squeeze out every bit of learning we can from our review, helping to shorten our feedback loop.\nUpdate your goals and processes with the results of your reviews All the performance data in the world is no good to us if we don\u0026rsquo;t put it into practice. With each successive code review, we can refine and add to our checklist. With what we learn from each process review, we can fine tune and improve our processes. The more we can invent concrete and observable ways to implement our learning, the more success we\u0026rsquo;ll have.\nMaking a conscious effort to utilize and practice the things we\u0026rsquo;ve learned is the final, vital, component of our feedback loop. The more often we incorporate new learning, the shorter our loop becomes, allowing us to improve that much faster.\n",url:"https://victoria.dev/blog/how-to-set-up-a-short-feedback-loop-as-a-solo-coder/"},"https:\/\/victoria.dev\/blog\/adorable-bookmarklets-want-to-help-delete-your-social-media-data\/":{title:"Adorable bookmarklets want to help delete your social media data",tags:["javascript"],content:"A little while ago I wrote about a Lambda function I called ephemeral for deleting my old tweets. While it\u0026rsquo;s a great project for someone familiar with or wanting to learn to use Lambda, it isn\u0026rsquo;t simple for a non-technical person to set up. There are services out there that will delete your tweets for you, but require your access credentials. There didn\u0026rsquo;t seem to be anything that provided convenience without also requiring authentication.\nSo, I went oldschool and created the ephemeral bookmarklet.\nIf that didn\u0026rsquo;t make you instantly nostalgic, a bookmarklet is a little application that lives as a bookmark in your web browser. You \u0026ldquo;install\u0026rdquo; it by dragging the link to your bookmarks toolbar, or right-clicking on the link and choosing \u0026ldquo;Bookmark this link\u0026rdquo; (Firefox). You click it to execute the program on the current page.\nHere\u0026rsquo;s what the ephemeral bookmarklet will do:\n The ephemeral bookmarklet is part of a new suite of tools for personal data management that I\u0026rsquo;m co-creating with Adam Drake. You can get all the bookmarklets on this page, and they\u0026rsquo;re also open source on GitHub.\nThere are currently bookmarklets for managing your data on LinkedIn and Twitter. We\u0026rsquo;re looking for testers and contributors to help make this a comprehensive toolset for your social media data management. If you write code, I invite you to contribute and help this toolset grow.\n∩{｡◕‿◕｡}∩ \u0026ndash; Bookmarklet says hi!\n",url:"https://victoria.dev/blog/adorable-bookmarklets-want-to-help-delete-your-social-media-data/"},"https:\/\/victoria.dev\/blog\/a-coffee-break-introduction-to-time-complexity-of-algorithms\/":{title:"A coffee-break introduction to time complexity of algorithms",tags:["algorithms","computing","go"],content:"Just like writing your very first for loop, understanding time complexity is an integral milestone to learning how to write efficient complex programs. Think of it as having a superpower that allows you to know exactly what type of program might be the most efficient in a particular situation - before even running a single line of code.\nThe fundamental concepts of complexity analysis are well worth studying. You\u0026rsquo;ll be able to better understand how the code you\u0026rsquo;re writing will interact with the program\u0026rsquo;s input, and as a result, you\u0026rsquo;ll spend a lot less wasted time writing slow and problematic code. It won\u0026rsquo;t take long to go over all you need to know in order to start writing more efficient programs - in fact, we can do it in about fifteen minutes. You can go grab a coffee right now (or tea, if that\u0026rsquo;s your thing) and I\u0026rsquo;ll take you through it before your coffee break is over. Go ahead, I\u0026rsquo;ll wait.\nAll set? Let\u0026rsquo;s do it!\nWhat is \u0026ldquo;time complexity\u0026rdquo; anyway The time complexity of an algorithm is an approximation of how long that algorithm will take to process some input. It describes the efficiency of the algorithm by the magnitude of its operations. This is different than the number of times an operation repeats; I\u0026rsquo;ll expand on that later. Generally, the fewer operations the algorithm has, the faster it will be.\nWe write about time complexity using Big O notation, which looks something like O(n). There\u0026rsquo;s rather a lot of math involved in its formal definition, but informally we can say that Big O notation gives us our algorithm\u0026rsquo;s approximate run time in the worst case, or in other words, its upper bound.[2] It is inherently relative and comparative.[3] We\u0026rsquo;re describing the algorithm\u0026rsquo;s efficiency relative to the increasing size of its input data, n. If the input is a string, then n is the length of the string. If it\u0026rsquo;s a list of integers, n is the length of the list.\nIt\u0026rsquo;s easiest to picture what Big O notation represents with a graph:\n Lines made with the very excellent Desmos graph calculator. You can play with this graph here.\n  Here are the main important points to remember as you read the rest of this article:\n Time complexity is an approximation An algorithm\u0026rsquo;s time complexity approximates its worst case run time  Determining time complexity There are different classes of complexity that we can use to quickly understand an algorithm. I\u0026rsquo;ll illustrate some of these classes using nested loops and other examples.\nPolynomial time complexity A polynomial, from the Greek poly meaning \u0026ldquo;many,\u0026rdquo; and Latin nomen meaning \u0026ldquo;name,\u0026rdquo; describes an expression comprised of constant variables, and addition, multiplication, and exponentiation to a non-negative integer power.[4] That\u0026rsquo;s a super math-y way to say that it contains variables usually denoted by letters, and symbols that look like these:\nThe below classes describe polynomial algorithms. Some have food examples.\nConstant A constant time algorithm doesn\u0026rsquo;t change its running time in response to the input data. No matter the size of the data it receives, the algorithm takes the same amount of time to run. We denote this as a time complexity of O(1).\n Here\u0026rsquo;s one example of a constant algorithm that takes the first item in a slice.\nfunc takeCupcake(cupcakes []int) int { return cupcakes[0] }  Choice of flavours are: vanilla cupcake, strawberry cupcake, mint chocolate cupcake, lemon cupcake, and wibbly wobbly, timey wimey cupcake.\n  With this constant-time algorithm, no matter how many cupcakes are on offer, you just get the first one. Oh well. Flavours are overrated anyway.\nLinear The running duration of a linear algorithm is constant. It will process the input in n number of operations. This is often the best possible (most efficient) case for time complexity where all the data must be examined.\n Here\u0026rsquo;s an example of code with time complexity of O(n):\nfunc eatChips(bowlOfChips int) { for chip := 0; chip \u0026lt;= bowlOfChips; chip++ { // dip chip  } } Here\u0026rsquo;s another example of code with time complexity of O(n):\nfunc eatChips(bowlOfChips int) { for chip := 0; chip \u0026lt;= bowlOfChips; chip++ { // double dip chip  } } It doesn\u0026rsquo;t matter whether the code inside the loop executes once, twice, or any number of times. Both these loops process the input by a constant factor of n, and thus can be described as linear.\n Don\u0026rsquo;t double dip in a shared bowl.\n  Quadratic  Now here\u0026rsquo;s an example of code with time complexity of O(n2):\nfunc pizzaDelivery(pizzas int) { for pizza := 0; pizza \u0026lt;= pizzas; pizza++ { // slice pizza  for slice := 0; slice \u0026lt;= pizza; slice++ { // eat slice of pizza  } } } Because there are two nested loops, or nested linear operations, the algorithm process the input n2 times.\nCubic  Extending on the previous example, this code with three nested loops has time complexity of O(n3):\nfunc pizzaDelivery(boxesDelivered int) { for pizzaBox := 0; pizzaBox \u0026lt;= boxesDelivered; pizzaBox++ { // open box  for pizza := 0; pizza \u0026lt;= pizzaBox; pizza++ { // slice pizza  for slice := 0; slice \u0026lt;= pizza; slice++ { // eat slice of pizza  } } } }  Seriously though, who delivers unsliced pizza??\n  Logarithmic A logarithmic algorithm is one that reduces the size of the input at every step. We denote this time complexity as O(log n), where log, the logarithm function, is this shape:\n One example of this is a binary search algorithm that finds the position of an element within a sorted array. Here\u0026rsquo;s how it would work, assuming we\u0026rsquo;re trying to find the element x:\n If x matches the middle element m of the array, return the position of m If x doesn\u0026rsquo;t match m, see if m is larger or smaller than x  If larger, discard all array items greater than m If smaller, discard all array items smaller than m   Continue by repeating steps 1 and 2 on the remaining array until x is found  I find the clearest analogy for understanding binary search is imagining the process of locating a book in a bookstore aisle. If the books are organized by author\u0026rsquo;s last name and you want to find \u0026ldquo;Terry Pratchett,\u0026rdquo; you know you need to look for the \u0026ldquo;P\u0026rdquo; section.\nYou can approach the shelf at any point along the aisle and look at the author\u0026rsquo;s last name there. If you\u0026rsquo;re looking at a book by Neil Gaiman, you know you can ignore all the rest of the books to your left, since no letters that come before \u0026ldquo;G\u0026rdquo; in the alphabet happen to be \u0026ldquo;P.\u0026rdquo; You would then move down the aisle to the right any amount, and repeat this process until you\u0026rsquo;ve found the Terry Pratchett section, which should be rather sizable if you\u0026rsquo;re at any decent bookstore because wow did he write a lot of books.\nQuasilinear  Often seen with sorting algorithms, the time complexity O(n log n) can describe a data structure where each operation takes O(log n) time. One example of this is quick sort, a divide-and-conquer algorithm.\nQuick sort works by dividing up an unsorted array into smaller chunks that are easier to process. It sorts the sub-arrays, and thus the whole array. Think about it like trying to put a deck of cards in order. It\u0026rsquo;s faster if you split up the cards and get five friends to help you.\nNon-polynomial time complexity The below classes of algorithms are non-polynomial.\nFactorial  An algorithm with time complexity O(n!) often iterates through all permutations of the input elements. One common example is a brute-force search seen in the travelling salesman problem. It tries to find the least costly path between a number of points by enumerating all possible permutations and finding the ones with the lowest cost.\nExponential An exponential algorithm often also iterates through all subsets of the input elements. It is denoted O(2n) and is often seen in brute-force algorithms. It is similar to factorial time except in its rate of growth, which as you may not be surprised to hear, is exponential. The larger the data set, the more steep the curve becomes.\n In cryptography, a brute-force attack may systematically check all possible elements of a password by iterating through subsets. Using an exponential algorithm to do this, it becomes incredibly resource-expensive to brute-force crack a long password versus a shorter one. This is one reason that a long password is considered more secure than a shorter one.\nThere are further time complexity classes less commonly seen that I won\u0026rsquo;t cover here, but you can read about these and find examples in this handy table.\nRecursion time complexity As I described in my article explaining recursion using apple pie, a recursive function calls itself under specified conditions. Its time complexity depends on how many times the function is called and the time complexity of a single function call. In other words, it\u0026rsquo;s the product of the number of times the function runs and a single execution\u0026rsquo;s time complexity.\nHere\u0026rsquo;s a recursive function that eats pies until no pies are left:\nfunc eatPies(pies int) int { if pies == 0 { return pies } return eatPies(pies - 1) } The time complexity of a single execution is constant. No matter how many pies are input, the program will do the same thing: check to see if the input is 0. If so, return, and if not, call itself with one fewer pie.\nThe initial number of pies could be any number, and we need to process all of them, so we can describe the input as n. Thus, the time complexity of this recursive function is the product O(n).\n This function\u0026rsquo;s return value is zero, plus some indigestion.\n  Worst case time complexity So far, we\u0026rsquo;ve talked about the time complexity of a few nested loops and some code examples. Most algorithms, however, are built from many combinations of these. How do we determine the time complexity of an algorithm containing many of these elements strung together?\nEasy. We can describe the total time complexity of the algorithm by finding the largest complexity among all of its parts. This is because the slowest part of the code is the bottleneck, and time complexity is concerned with describing the worst case for the algorithm\u0026rsquo;s run time.\nSay we have a program for an office party. If our program looks like this:\npackage main import \u0026#34;fmt\u0026#34; func takeCupcake(cupcakes []int) int { fmt.Println(\u0026#34;Have cupcake number\u0026#34;,cupcakes[0]) return cupcakes[0] } func eatChips(bowlOfChips int) { fmt.Println(\u0026#34;Have some chips!\u0026#34;) for chip := 0; chip \u0026lt;= bowlOfChips; chip++ { // dip chip  } fmt.Println(\u0026#34;No more chips.\u0026#34;) } func pizzaDelivery(boxesDelivered int) { fmt.Println(\u0026#34;Pizza is here!\u0026#34;) for pizzaBox := 0; pizzaBox \u0026lt;= boxesDelivered; pizzaBox++ { // open box  for pizza := 0; pizza \u0026lt;= pizzaBox; pizza++ { // slice pizza  for slice := 0; slice \u0026lt;= pizza; slice++ { // eat slice of pizza  } } } fmt.Println(\u0026#34;Pizza is gone.\u0026#34;) } func eatPies(pies int) int { if pies == 0 { fmt.Println(\u0026#34;Someone ate all the pies!\u0026#34;) return pies } fmt.Println(\u0026#34;Eating pie...\u0026#34;) return eatPies(pies - 1) } func main() { takeCupcake([]int{1, 2, 3}) eatChips(23) pizzaDelivery(3) eatPies(3) fmt.Println(\u0026#34;Food gone. Back to work!\u0026#34;) } We can describe the time complexity of all the code by the complexity of its most complex part. This program is made up of functions we\u0026rsquo;ve already seen, with the following time complexity classes:\n   Function Class Big O     takeCupcake constant O(1)   eatChips linear O(n)   pizzaDelivery cubic O(n3)   eatPies linear (recursive) O(n)    To describe the time complexity of the entire office party program, we choose the worst case. This program would have the time complexity O(n3).\nHere\u0026rsquo;s the office party soundtrack, just for fun.\nHave cupcake number 1 Have some chips! No more chips. Pizza is here! Pizza is gone. Eating pie... Eating pie... Eating pie... Someone ate all the pies! Food gone. Back to work! P vs NP, NP-complete, and NP-hard You may come across these terms in your explorations of time complexity. Informally, P (for Polynomial time), is a class of problems that is quick to solve. NP, for Nondeterministic Polynomial time, is a class of problems where the answer can be quickly verified in polynomial time. NP encompasses P, but also another class of problems called NP-complete, for which no fast solution is known.[5] Outside of NP but still including NP-complete is yet another class called NP-hard, which includes problems that no one has been able to verifiably solve with polynomial algorithms.[6]\n P vs NP Euler diagram, by Behnam Esfahbod, CC BY-SA 3.0\n  P versus NP is an unsolved, open question in computer science.\nAnyway, you don\u0026rsquo;t generally need to know about NP and NP-hard problems to begin taking advantage of understanding time complexity. They\u0026rsquo;re a whole other Pandora\u0026rsquo;s box.\nApproximate the efficiency of an algorithm before you write the code So far, we\u0026rsquo;ve identified some different time complexity classes and how we might determine which one an algorithm falls into. So how does this help us before we\u0026rsquo;ve written any code to evaluate?\nBy combining a little knowledge of time complexity with an awareness of the size of our input data, we can take a guess at an efficient algorithm for processing our data within a given time constraint. We can base our estimation on the fact that a modern computer can perform some hundreds of millions of operations in a second.[1] The following table from the Competitive Programmer\u0026rsquo;s Handbook offers some estimates on required time complexity to process the respective input size in a time limit of one second.\n   Input size Required time complexity for 1s processing time     n ≤ 10 O(n!)   n ≤ 20 O(2n)   n ≤ 500 O(n3)   n ≤ 5000 O(n2)   n ≤ 106 O(n log n) or O(n)   n is large O(1) or O(log n)    Keep in mind that time complexity is an approximation, and not a guarantee. We can save a lot of time and effort by immediately ruling out algorithm designs that are unlikely to suit our constraints, but we must also consider that Big O notation doesn\u0026rsquo;t account for constant factors. Here\u0026rsquo;s some code to illustrate.\nThe following two algorithms both have O(n) time complexity.\nfunc makeCoffee(scoops int) { for scoop := 0; scoop \u0026lt;= scoops; scoop++ { // add instant coffee  } } func makeStrongCoffee(scoops int) { for scoop := 0; scoop \u0026lt;= 3*scoops; scoop++ { // add instant coffee  } } The first function makes a cup of coffee with the number of scoops we ask for. The second function also makes a cup of coffee, but it triples the number of scoops we ask for. To see an illustrative example, let\u0026rsquo;s ask both these functions for a cup of coffee with a million scoops.\nHere\u0026rsquo;s the output of the Go test:\nBenchmark_makeCoffee-4 1000000000 0.29 ns/op Benchmark_makeStrongCoffee-4 1000000000 0.86 ns/op Our first function, makeCoffee, completed in an average 0.29 nanoseconds. Our second function, makeStrongCoffee, completed in an average of 0.86 nanoseconds. While those may both seem like pretty small numbers, consider that the stronger coffee took near three times longer to make. This should make sense intuitively, since we asked it to triple the scoops. Big O notation alone wouldn\u0026rsquo;t tell you this, since the constant factor of the tripled scoops isn\u0026rsquo;t accounted for.\nImprove time complexity of existing code Becoming familiar with time complexity gives us the opportunity to write code, or refactor code, to be more efficient. To illustrate, I\u0026rsquo;ll give a concrete example of one way we can refactor a bit of code to improve its time complexity.\nLet\u0026rsquo;s say a bunch of people at the office want some pie. Some people want pie more than others. The amount that everyone wants some pie is represented by an int \u0026gt; 0:\ndiners := []int{2, 88, 87, 16, 42, 10, 34, 1, 43, 56} Unfortunately, we\u0026rsquo;re bootstrapped and there are only three forks to go around. Since we\u0026rsquo;re a cooperative bunch, the three people who want pie the most will receive the forks to eat it with. Even though they\u0026rsquo;ve all agreed on this, no one seems to want to sort themselves out and line up in an orderly fashion, so we\u0026rsquo;ll have to make do with everybody jumbled about.\nWithout sorting the list of diners, return the three largest integers in the slice.\nHere\u0026rsquo;s a function that solves this problem and has O(n2) time complexity:\nfunc giveForks(diners []int) []int { // make a slice to store diners who will receive forks  var withForks []int // loop over three forks  for i := 1; i \u0026lt;= 3; i++ { // variables to keep track of the highest integer and where it is  var max, maxIndex int // loop over the diners slice  for n := range diners { // if this integer is higher than max, update max and maxIndex  if diners[n] \u0026gt; max { max = diners[n] maxIndex = n } } // remove the highest integer from the diners slice for the next loop  diners = append(diners[:maxIndex], diners[maxIndex+1:]...) // keep track of who gets a fork  withForks = append(withForks, max) } return withForks } This program works, and eventually returns diners [88 87 56]. Everyone gets a little impatient while it\u0026rsquo;s running though, since it takes rather a long time (about 120 nanoseconds) just to hand out three forks, and the pie\u0026rsquo;s getting cold. How could we improve it?\nBy thinking about our approach in a slightly different way, we can refactor this program to have O(n) time complexity:\nfunc giveForks(diners []int) []int { // make a slice to store diners who will receive forks  var withForks []int // create variables for each fork  var first, second, third int // loop over the diners  for i := range diners { // assign the forks  if diners[i] \u0026gt; first { third = second second = first first = diners[i] } else if diners[i] \u0026gt; second { third = second second = diners[i] } else if diners[i] \u0026gt; third { third = diners[i] } } // list the final result of who gets a fork  withForks = append(withForks, first, second, third) return withForks } Here\u0026rsquo;s how the new program works:\nInitially, diner 2 (the first in the list) is assigned the first fork. The other forks remain unassigned.\nThen, diner 88 is assigned the first fork instead. Diner 2 gets the second one.\nDiner 87 isn\u0026rsquo;t greater than first which is currently 88, but it is greater than 2 who has the second fork. So, the second fork goes to 87. Diner 2 gets the third fork.\nContinuing in this violent and rapid fork exchange, diner 16 is then assigned the third fork instead of 2, and so on.\nWe can add a print statement in the loop to see how the fork assignments play out:\n0 0 0 2 0 0 88 2 0 88 87 2 88 87 16 88 87 42 88 87 42 88 87 42 88 87 42 88 87 43 [88 87 56] This program is much faster, and the whole epic struggle for fork domination is over in 47 nanoseconds.\nAs you can see, with a little change in perspective and some refactoring, we\u0026rsquo;ve made this simple bit of code faster and more efficient.\nWell, it looks like our fifteen minute coffee break is up! I hope I\u0026rsquo;ve given you a comprehensive introduction to calculating time complexity. Time to get back to work, hopefully applying your new knowledge to write more effective code! Or maybe just sound smart at your next office party. :)\nSources \u0026ldquo;If I have seen further it is by standing on the shoulders of Giants.\u0026rdquo; \u0026ndash;Isaac Newton, 1675\n Antti Laaksonen. Competitive Programmer\u0026rsquo;s Handbook (pdf), 2017 Wikipedia: Big O notation StackOverflow: What is a plain English explanation of “Big O” notation? Wikipedia: Polynomial Wikipedia: NP-completeness Wikipedia: NP-hardness Desmos graph calculator  ",url:"https://victoria.dev/blog/a-coffee-break-introduction-to-time-complexity-of-algorithms/"},"https:\/\/victoria.dev\/blog\/knapsack-problem-algorithms-for-my-real-life-carry-on-knapsack\/":{title:"Knapsack problem algorithms for my real-life carry-on knapsack",tags:["algorithms","data","go"],content:"The knapsack problem I\u0026rsquo;m a nomad and live out of one carry-on bag. This means that the total weight of all my worldly possessions must fall under airline cabin baggage weight limits - usually 10kg. On some smaller airlines, however, this weight limit drops to 7kg. Occasionally, I have to decide not to bring something with me to adjust to the smaller weight limit.\nAs a practical exercise, deciding what to leave behind (or get rid of altogether) entails laying out all my things and choosing which ones to keep. That decision is based on the item\u0026rsquo;s usefulness to me (its worth) and its weight.\n This is all my stuff, and my Minaal Carry-on bag.\n  Being a programmer, I\u0026rsquo;m aware that decisions like this could be made more efficiently by a computer. It\u0026rsquo;s done so frequently and so ubiquitously, in fact, that many will recognize this scenario as the classic packing problem or knapsack problem. How do I go about telling a computer to put as many important items in my bag as possible while coming in at or under a weight limit of 7kg? With algorithms! Yay!\nI\u0026rsquo;ll discuss two common approaches to solving the knapsack problem: one called a greedy algorithm, and another called dynamic programming (a little harder, but better, faster, stronger\u0026hellip;).\nLet\u0026rsquo;s get to it.\nThe set up I prepared my data in the form of a CSV file with three columns: the item\u0026rsquo;s name (a string), a representation of its worth (an integer), and its weight in grams (an integer). There are 40 items in total. I represented worth by ranking each item from 40 to 1, with 40 being the most important and 1 equating with something like \u0026ldquo;why do I even have this again?\u0026rdquo; (If you\u0026rsquo;ve never listed out all your possessions and ranked them by order of how useful they are to you, I highly recommend you try it. It can be a very revealing exercise.)\nTotal weight of all items and bag: 9003g\nBag weight: 1415g\nAirline limit: 7000g\nMaximum weight of items I can pack: 5585g\nTotal possible worth of items: 820\nThe challenge: Pack as many items as the limit allows while maximizing the total worth.\nData structures Reading in a file Before we can begin thinking about how to solve the knapsack problem, we have to solve the problem of reading in and storing our data. Thankfully, the Go standard library\u0026rsquo;s io/ioutil package makes the first part straightforward.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; ) func check(e error) { if e != nil { panic(e) } } func readItems(path string) { dat, err := ioutil.ReadFile(path) check(err) fmt.Print(string(dat)) } The ReadFile() function takes a file path and returns the file\u0026rsquo;s contents and an error (nil if the call is successful) so we\u0026rsquo;ve also created a check() function to handle any errors that might be returned. In a real-world application we probably would want to do something more sophisticated than panic, but that\u0026rsquo;s not important right now.\nCreating a struct Now that we\u0026rsquo;ve got our data, we should probably do something with it. Since we\u0026rsquo;re working with real-life items and a real-life bag, let\u0026rsquo;s create some types to represent them and make it easier to conceptualize our program. A struct in Go is a typed collection of fields. Here are our two types:\ntype item struct { name string worth, weight int } type bag struct { bagWeight, currItemsWeight, maxItemsWeight, totalWeight int items []item } It is helpful to use field names that are very descriptive. You can see that the structs are set up just as we\u0026rsquo;ve described the things they represent. An item has a name (string), and a worth and weight (integers). A bag has several fields of type int representing its attributes, and also has the ability to hold items, represented in the struct as a slice of item type thingamabobbers.\nParsing and storing our data Several comprehensive Go packages exist that we could use to parse our CSV data\u0026hellip; but where\u0026rsquo;s the fun in that? Let\u0026rsquo;s go basic with some string splitting and a for loop. Here\u0026rsquo;s our updated readItems() function:\nfunc readItems(path string) []item { dat, err := ioutil.ReadFile(path) check(err) lines := strings.Split(string(dat), \u0026#34;\\n\u0026#34;) itemList := make([]item, 0) for i, v := range lines { if i == 0 { continue } s := strings.Split(v, \u0026#34;,\u0026#34;) newItemWorth, _ := strconv.Atoi(s[1]) newItemWeight, _ := strconv.Atoi(s[2]) newItem := item{name: s[0], worth: newItemWorth, weight: newItemWeight} itemList = append(itemList, newItem) } return itemList } Using strings.Split, we split our dat on newlines. We then create an empty itemList to hold our items.\nIn our for loop, we skip the first line of our CSV file (the headers) then iterate over each line. We use strconv.Atoi (read \u0026ldquo;A to i\u0026rdquo;) to convert the values for each item\u0026rsquo;s worth and weight into integers. We then create a newItem with these field values and append it to the itemList. Finally, we return itemList.\nHere\u0026rsquo;s what our set up looks like so far:\npackage main import ( \u0026#34;io/ioutil\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;strings\u0026#34; ) type item struct { name string worth, weight int } type bag struct { bagWeight, currItemsWeight, maxItemsWeight, totalWeight, totalWorth int items []item } func check(e error) { if e != nil { panic(e) } } func readItems(path string) []item { dat, err := ioutil.ReadFile(path) check(err) lines := strings.Split(string(dat), \u0026#34;\\n\u0026#34;) itemList := make([]item, 0) for i, v := range lines { if i == 0 { continue // skip the headers on the first line  } s := strings.Split(v, \u0026#34;,\u0026#34;) newItemWorth, _ := strconv.Atoi(s[1]) newItemWeight, _ := strconv.Atoi(s[2]) newItem := item{name: s[0], worth: newItemWorth, weight: newItemWeight} itemList = append(itemList, newItem) } return itemList } Now that we\u0026rsquo;ve got our data structures set up, let\u0026rsquo;s get packing (🥁) on the first approach.\nGreedy algorithm A greedy algorithm is the most straightforward approach to solving the knapsack problem, in that it is a one-pass algorithm that constructs a single final solution. At each stage of the problem, the greedy algorithm picks the option that is locally optimal, meaning it looks like the most suitable option right now. It does not revise its previous choices as it progresses through our data set.\nBuilding our greedy algorithm The steps of the algorithm we\u0026rsquo;ll use to solve our knapsack problem are:\n Sort items by worth, in descending order. Start with the highest worth item. Put items into the bag until the next item on the list cannot fit. Try to fill any remaining capacity with the next item on the list that can fit.  If you read my article about solving problems and making paella, you\u0026rsquo;ll know that I always start by figuring out what the next most important question is. In this case, there are three main operations we need to figure out how to do:\n Sort items by worth. Put an item in the bag. Check to see if the bag is full.  The first one is just a docs lookup away. Here\u0026rsquo;s how we sort a slice in Go:\nsort.Slice(is, func(i, j int) bool { return is[i].worth \u0026gt; is[j].worth }) The sort.Slice() function orders our items according to the less function we provide. In this case, it will order the highest worth items before the lowest worth items.\nGiven that we don\u0026rsquo;t want to put an item in the bag if it doesn\u0026rsquo;t fit, we\u0026rsquo;ll complete the last two tasks in reverse. First, we\u0026rsquo;ll check to see if the item fits. If so, it goes in the bag.\nfunc (b *bag) addItem(i item) error { if b.currItemsWeight+i.weight \u0026lt;= b.maxItemsWeight { b.currItemsWeight += i.weight b.items = append(b.items, i) return nil } return errors.New(\u0026#34;could not fit item\u0026#34;) } Notice the * in our first line there. That indicates that bag is a pointer receiver (as opposed to a value receiver). It\u0026rsquo;s a concept that can be slightly confusing if you\u0026rsquo;re new to Go. Here are some things to consider that might help you decide when to use a value receiver and when to use a pointer receiver. For the purposes of our addItem() function, this case applies:\n If the method needs to mutate the receiver, the receiver must be a pointer.\n Our use of a pointer receiver tells our function we want to operate on this specific bag in particular, not a new bag. It\u0026rsquo;s important because without it, every item would always fit in a newly created bag! A little detail like this can make the difference between code that works and code that keeps you up until 4am chugging Red Bull and muttering to yourself. (Go to bed on time even if your code doesn\u0026rsquo;t work - you\u0026rsquo;ll thank me later.)\nNow that we\u0026rsquo;ve got our components, let\u0026rsquo;s put together our greedy algorithm:\nfunc greedy(is []item, b bag) { sort.Slice(is, func(i, j int) bool { return is[i].worth \u0026gt; is[j].worth }) for i := range is { b.addItem(is[i]) } b.totalWeight = b.bagWeight + b.currItemsWeight for _, v := range b.items { b.totalWorth += v.worth } } Then in our main() function, we\u0026rsquo;ll create our bag, read in our data, and call our greedy algorithm. Here\u0026rsquo;s what it looks like, all set up and ready to go:\nfunc main() { minaal := bag{bagWeight: 1415, currItemsWeight: 0, maxItemsWeight: 5585} itemList := readItems(\u0026#34;objects.csv\u0026#34;) greedy(itemList, minaal) } Greedy algorithm results So how does this algorithm do when it comes to efficiently packing our bag to maximize its total worth? Here\u0026rsquo;s the result:\nTotal weight of bag and items: 6987g\nTotal worth of packed items: 716\nHere are the items our greedy algorithm chose, sorted by worth:\n   Item Worth Weight     Lenovo X1 Carbon (5th Gen) 40 112   10 pairs thongs 39 80   5 Underarmour Strappy 38 305   1 pair Uniqlo leggings 37 185   2 Lululemon Cool Racerback 36 174   Chargers and cables in Mini Bomber Travel Kit 35 665   The Roost Stand 34 170   ThinkPad Compact Bluetooth Keyboard with trackpoint 33 460   Seagate Backup PlusSlim 32 159   1 pair black denim shorts 31 197   2 pairs Nike Pro shorts 30 112   2 pairs Lululemon shorts 29 184   Isabella T-Strap Croc sandals 28 200   2 Underarmour HeatGear CoolSwitch tank tops 27 138   5 pairs black socks 26 95   2 pairs Injinji Women\u0026rsquo;s Run Lightweight No-Show Toe Socks 25 54   1 fancy tank top 24 71   1 light and stretchylong-sleeve shirt (Gap Fit) 23 147   Uniqlo Ultralight Down insulating jacket 22 235   Patagonia Torrentshell 21 301   Lightweight Merino Wool Buff 20 50   1 LBD (H\u0026amp;M) 19 174   Field Notes Pitch Black Memo Book Dot-Graph 18 68   Innergie PocketCell USB-C 6000mAh power bank 17 14   JBL Reflect Mini Bluetooth Sport Headphones 13 14   Oakley Latch Sunglasses 11 30   Petzl E+LITE Emergency Headlamp 8 27    It\u0026rsquo;s clear that the greedy algorithm is a straightforward way to quickly find a feasible solution. For small data sets, it will probably be close to the optimal solution. The algorithm packed a total item worth of 716 (104 points less than the maximum possible value), while filling the bag with just 13g left over.\nAs we learned earlier, the greedy algorithm doesn\u0026rsquo;t improve upon the solution it returns. It simply adds the next highest worth item it can to the bag.\nLet\u0026rsquo;s look at another method for solving the knapsack problem that will give us the optimal solution - the highest possible total worth under the weight limit.\nDynamic programming The name \u0026ldquo;dynamic programming\u0026rdquo; can be a bit misleading. It\u0026rsquo;s not a style of programming, as the name might cause you to infer, but simply another approach.\nDynamic programming differs from the straightforward greedy algorithm in a few key ways. Firstly, a dynamic programming bag packing solution enumerates the entire solution space with all possibilities of item combinations that could be used to pack our bag. Where a greedy algorithm chooses the most optimal local solution, dynamic programming algorithms are able to find the most optimal global solution.\nSecondly, dynamic programming uses memoization to store the results of previously computed operations and returns the cached result when the operation occurs again. This allows it to \u0026ldquo;remember\u0026rdquo; previous combinations. This takes less time than it would to re-compute the answer again.\nBuilding our dynamic programming algorithm To use dynamic programming to find the optimal recipe for packing our bag, we\u0026rsquo;ll need to:\n Create a matrix representing all subsets of the items (the solution space) with rows representing items and columns representing the bag\u0026rsquo;s remaining weight capacity Loop through the matrix and calculate the worth that can be obtained by each combination of items at each stage of the bag\u0026rsquo;s capacity Examine the completed matrix to determine which items to add to the bag in order to produce the maximum possible worth for the bag in total  It will be most helpful to visualize our solution space. Here\u0026rsquo;s a representation of what we\u0026rsquo;re building with our code:\n The empty knapsackian multiverse.\n  In Go, we can create this matrix as a slice of slices.\nmatrix := make([][]int, numItems+1) // rows representing items for i := range matrix { matrix[i] = make([]int, capacity+1) // columns representing grams of weight } We\u0026rsquo;ve padded the rows and columns by 1 so that the indicies match the item and weight numbers.\nNow that we\u0026rsquo;ve created our matrix, we\u0026rsquo;ll fill it by looping over the rows and the columns:\n// loop through table rows for i := 1; i \u0026lt;= numItems; i++ { // loop through table columns  for w := 1; w \u0026lt;= capacity; w++ { // do stuff in each element  } } Then for each element, we\u0026rsquo;ll calculate the worth value to ascribe to it. We do this with code that represents the following:\n If the item at the index matching the current row fits within the weight capacity represented by the current column, take the maximum of either:\n The total worth of the items already in the bag or, The total worth of all the items in the bag except the item at the previous row index, plus the new item\u0026rsquo;s worth   In other words, as our algorithm considers one of the items, we\u0026rsquo;re asking it to decide whether this item added to the bag would produce a higher total worth than the last item it added to the bag, at the bag\u0026rsquo;s current total weight. If this current item is a better choice, put it in - if not, leave it out.\nHere\u0026rsquo;s the code that accomplishes this:\n// if weight of item matching this index can fit at the current capacity column... if is[i-1].weight \u0026lt;= w { // worth of this subset without this item  valueOne := float64(matrix[i-1][w]) // worth of this subset without the previous item, and this item instead  valueTwo := float64(is[i-1].worth + matrix[i-1][w-is[i-1].weight]) // take maximum of either valueOne or valueTwo  matrix[i][w] = int(math.Max(valueOne, valueTwo)) // if the new worth is not more, carry over the previous worth } else { matrix[i][w] = matrix[i-1][w] } This process of comparing item combinations will continue until every item has been considered at every possible stage of the bag\u0026rsquo;s increasing total weight. When all the above have been considered, we\u0026rsquo;ll have enumerated the solution space - filled the matrix - with all possible total worth values.\nWe\u0026rsquo;ll have a big chart of numbers, and in the last column at the last row we\u0026rsquo;ll have our highest possible value.\n A strictly representative representation of the filled matrix.\n  That\u0026rsquo;s great, but how do we find out which combination of items were put in the bag to achieve that worth?\nGetting our optimized item list To see which items combine to create our optimal packing list, we\u0026rsquo;ll need to examine our matrix in reverse to the way we created it. Since we know the highest possible value is in the last row in the last column, we\u0026rsquo;ll start there. To find the items, we:\n Get the value of the current cell Compare the value of the current cell to the value in the cell directly above it If the values differ, there was a change to the bag items; find the next cell to examine by moving backwards through the columns according to the current item\u0026rsquo;s weight (find the value of the bag before this current item was added) If the values match, there was no change to the bag items; move up to the cell in the row above and repeat  The nature of the action we\u0026rsquo;re trying to achieve lends itself well to a recursive function. If you recall from my previous article about making apple pie, recursive functions are simply functions that call themselves under certain conditions. Here\u0026rsquo;s what it looks like:\nfunc checkItem(b *bag, i int, w int, is []item, matrix [][]int) { if i \u0026lt;= 0 || w \u0026lt;= 0 { return } pick := matrix[i][w] if pick != matrix[i-1][w] { b.addItem(is[i-1]) checkItem(b, i-1, w-is[i-1].weight, is, matrix) } else { checkItem(b, i-1, w, is, matrix) } } Our checkItem() function calls itself if the condition we described in step 4 is true. If step 3 is true, it also calls itself, but with different arguments.\nRecursive functions require a base case. In this example, we want the function to stop once we run out of values of worth to compare. Thus our base case is when either i or w are 0.\nHere\u0026rsquo;s how the dynamic programming approach looks when it\u0026rsquo;s all put together:\nfunc checkItem(b *bag, i int, w int, is []item, matrix [][]int) { if i \u0026lt;= 0 || w \u0026lt;= 0 { return } pick := matrix[i][w] if pick != matrix[i-1][w] { b.addItem(is[i-1]) checkItem(b, i-1, w-is[i-1].weight, is, matrix) } else { checkItem(b, i-1, w, is, matrix) } } func dynamic(is []item, b *bag) *bag { numItems := len(is) // number of items in knapsack  capacity := b.maxItemsWeight // capacity of knapsack  // create the empty matrix  matrix := make([][]int, numItems+1) // rows representing items  for i := range matrix { matrix[i] = make([]int, capacity+1) // columns representing grams of weight  } // loop through table rows  for i := 1; i \u0026lt;= numItems; i++ { // loop through table columns  for w := 1; w \u0026lt;= capacity; w++ { // if weight of item matching this index can fit at the current capacity column...  if is[i-1].weight \u0026lt;= w { // worth of this subset without this item  valueOne := float64(matrix[i-1][w]) // worth of this subset without the previous item, and this item instead  valueTwo := float64(is[i-1].worth + matrix[i-1][w-is[i-1].weight]) // take maximum of either valueOne or valueTwo  matrix[i][w] = int(math.Max(valueOne, valueTwo)) // if the new worth is not more, carry over the previous worth  } else { matrix[i][w] = matrix[i-1][w] } } } checkItem(b, numItems, capacity, is, matrix) // add other statistics to the bag  b.totalWorth = matrix[numItems][capacity] b.totalWeight = b.bagWeight + b.currItemsWeight return b } Dynamic programming results We expect that the dynamic programming approach will give us a more optimized solution than the greedy algorithm. So did it? Here are the results:\nTotal weight of bag and items: 6982g\nTotal worth of packed items: 757\nHere are the items our dynamic programming algorithm chose, sorted by worth:\n   Item Worth Weight     10 pairs thongs 39 80   5 Underarmour Strappy 38 305   1 pair Uniqlo leggings 37 185   2 Lululemon Cool Racerback 36 174   Chargers and cables in Mini Bomber Travel Kit 35 665   The Roost Stand 34 170   ThinkPad Compact Bluetooth Keyboard with trackpoint 33 460   Seagate Backup Plus Slim 32 159   1 pair black denim shorts 31 197   2 pairs Nike Pro shorts 30 112   2 pairs Lululemon shorts 29 184   Isabella T-Strap Croc sandals 28 200   2 Underarmour HeatGear CoolSwitch tank tops 27 138   5 pairs black socks 26 95   2 pairs Injinji Women\u0026rsquo;s Run Lightweight No-Show Toe Socks 25 54   1 fancy tank top 24 71   1 light and stretchy long-sleeve shirt (Gap Fit) 23 147   Uniqlo Ultralight Down insulating jacket 22 235   Patagonia Torrentshell 21 301   Lightweight Merino Wool Buff 20 50   1 LBD (H\u0026amp;M) 19 174   Field Notes Pitch Black Memo Book Dot-Graph 18 68   Innergie PocketCell USB-C 6000mAh power bank 17 148   Important papers 16 228   Deuter First Aid Kit Active 15 144   Stanley Classic Vacuum Camp Mug 16oz 14 454   JBL Reflect Mini Bluetooth Sport Headphones 13 14   Anker SoundCore nano Bluetooth Speaker 12 80   Oakley Latch Sunglasses 11 30   Ray Ban Wayfarer Classic 10 45   Petzl E+LITE Emergency Headlamp 8 27   Peak Design Cuff Camera Wrist Strap 6 26   Travelon Micro Scale 5 125   Humangear GoBites Duo 3 22    There\u0026rsquo;s an obvious improvement to our dynamic programming solution over what the greedy algorithm gave us. Our total worth of 757 is 41 points greater than the greedy algorithm\u0026rsquo;s solution of 716, and for a few grams less weight too!\nInput sort order While testing my dynamic programming solution, I implemented the Fisher-Yates shuffle algorithm on the input before passing it into my function, just to ensure that the answer wasn\u0026rsquo;t somehow dependent on the sort order of the input. Here\u0026rsquo;s what the shuffle looks like in Go:\nrand.Seed(time.Now().UnixNano()) for i := range itemList { j := rand.Intn(i + 1) itemList[i], itemList[j] = itemList[j], itemList[i] } Of course I then realized that Go 1.10 now has a built-in shuffle\u0026hellip; it works precisely the same way and looks like this:\nrand.Shuffle(len(itemList), func(i, j int) { itemList[i], itemList[j] = itemList[j], itemList[i] }) So did the order in which the items were processed affect the outcome? Well\u0026hellip;\nSuddenly\u0026hellip; a rogue weight appears! As it turns out, in a way, the answer did depend on the order of the input. When I ran my dynamic programming algorithm several times, I sometimes saw a different total weight for the bag, though the total worth remained at 757. I initially thought this was a bug before examining the two sets of items that accompanied the two different total weight values. Everything was the same except for a few changes that collectively added up to a different item subset accounting for 14 of the 757 worth points.\nIn this case, there were two equally optimal solutions based only on the success metric of the highest total possible worth. Shuffling the input seemed to affect the placement of the items in the matrix and thus, the path that the checkItem() function took as it went through the matrix to find the chosen items. Since the success metric of having the highest possible worth was the same in both item sets, we don\u0026rsquo;t have a single unique solution - there\u0026rsquo;s two!\nAs an academic exercise, both these sets of items are correct answers. We may choose to optimize further by another metric, say, the total weight of all the items. The highest possible worth at the least possible weight could be seen as an ideal solution.\nHere\u0026rsquo;s the second, lighter, dynamic programming result:\nTotal weight of bag and items: 6955g\nTotal worth of packed items: 757\n   Item Worth Weight     10 pairs thongs 39 80   5 Underarmour Strappy 38 305   1 pair Uniqlo leggings 37 185   2 Lululemon Cool Racerback 36 174   Chargers and cables in Mini Bomber Travel Kit 35 665   The Roost Stand 34 170   ThinkPad Compact Bluetooth Keyboard with trackpoint 33 460   Seagate Backup Plus Slim 32 159   1 pair black denim shorts 31 197   2 pairs Nike Pro shorts 30 112   2 pairs Lululemon shorts 29 184   Isabella T-Strap Croc sandals 28 200   2 Underarmour HeatGear CoolSwitch tank tops 27 138   5 pairs black socks 26 95   2 pairs Injinji Women\u0026rsquo;s Run Lightweight No-Show Toe Socks 25 54   1 fancy tank top 24 71   1 light and stretchy long-sleeve shirt (Gap Fit) 23 147   Uniqlo Ultralight Down insulating jacket 22 235   Patagonia Torrentshell 21 301   Lightweight Merino Wool Buff 20 50   1 LBD (H\u0026amp;M) 19 174   Field Notes Pitch Black Memo Book Dot-Graph 18 68   Innergie PocketCell USB-C 6000mAh power bank 17 148   Important papers 16 228   Deuter First Aid Kit Active 15 144   JBL Reflect Mini Bluetooth Sport Headphones 13 14   Anker SoundCore nano Bluetooth Speaker 12 80   Oakley Latch Sunglasses 11 30   Ray Ban Wayfarer Classic 10 45   Zip bag of toiletries 9 236   Petzl E+LITE Emergency Headlamp 8 27   Peak Design Cuff Camera Wrist Strap 6 26   Travelon Micro Scale 5 125   BlitzWolf Bluetooth Tripod/Monopod 4 150   Humangear GoBites Duo 3 22   Vapur Bottle 1L 1 41    Which approach is better? Go benchmarking The Go standard library\u0026rsquo;s testing package makes it straightforward for us to benchmark these two approaches. We can find out how long it takes each algorithm to run, and how much memory each uses. Here\u0026rsquo;s a simple main_test.go file:\npackage main import ( \u0026#34;testing\u0026#34; ) func Benchmark_greedy(b *testing.B) { itemList := readItems(\u0026#34;objects.csv\u0026#34;) for i := 0; i \u0026lt; b.N; i++ { minaal := bag{bagWeight: 1415, currItemsWeight: 0, maxItemsWeight: 5585} greedy(itemList, minaal) } } func Benchmark_dynamic(b *testing.B) { itemList := readItems(\u0026#34;objects.csv\u0026#34;) for i := 0; i \u0026lt; b.N; i++ { minaal := bag{bagWeight: 1415, currItemsWeight: 0, maxItemsWeight: 5585} dynamic(itemList, \u0026amp;minaal) } } We can run go test -bench=. -benchmem to see these results:\nBenchmark_greedy-4 1000000 1619 ns/op 2128 B/op 9 allocs/op Benchmark_dynamic-4 1000 1545322 ns/op 2020332 B/op 49 allocs/op Greedy algorithm performance After running the greedy algorithm 1,000,000 times, the speed of the algorithm was reliably measured to be 0.001619 milliseconds (translation: very fast). It required 2128 Bytes or 2-ish kilobytes of memory and 9 distinct memory allocations per iteration.\nDynamic programming performance The dynamic programming algorithm was run 1,000 times. Its speed was measured to be 1.545322 milliseconds or 0.001545322 seconds (translation: still pretty fast). It required 2,020,332 Bytes or 2-ish Megabytes, and 49 distinct memory allocations per iteration.\nThe verdict Part of choosing the right approach to solving any programming problem is taking into account the size of the input data set. In this case, it\u0026rsquo;s a small one. In this scenario, a one-pass greedy algorithm will always be faster and less resource-needy than dynamic programming, simply because it has fewer steps. Our greedy algorithm was almost two orders of magnitude faster and less memory-hungry than our dynamic programming algorithm.\nNot having those extra steps, however, means that getting the best possible solution from the greedy algorithm is unlikely.\nIt\u0026rsquo;s clear that the dynamic programming algorithm gave us better numbers: a lower weight, and higher overall worth.\n    Greedy algorithm Dynamic programming     Total weight: 6987g 6955g   Total worth: 716 757    Where dynamic programming on small data sets lacks in performance, it makes up in optimization. The question then becomes whether that additional optimization is worth the performance cost.\n\u0026ldquo;Better,\u0026rdquo; of course, is a subjective judgement. If speed and low resource usage is our success metric, then the greedy algorithm is clearly better. If the total worth of items in the bag is our success metric, then dynamic programming is clearly better. However, our scenario is a practical one, and only one of these algorithm designs returned an answer I\u0026rsquo;d choose. In optimizing for the overall greatest possible total worth of the items in the bag, the dynamic programming algorithm left out my highest-worth, but also heaviest, item: my laptop. The chargers and cables, Roost stand, and keyboard that were included aren\u0026rsquo;t much use without it.\nBetter algorithm design There\u0026rsquo;s a simple way to alter the dynamic programming approach so that the laptop is always included: we can modify the data so that the worth of the laptop is greater than the sum of the worth of all the other items. (Try it out!)\nPerhaps in re-designing the dynamic programming algorithm to be more practical, we might choose another success metric that better reflects an item\u0026rsquo;s importance, instead of a subjective worth value. There are many possible metrics we can use to represent the value of an item. Here are a few examples of a good proxy:\n Amount of time spent using the item Initial cost of purchasing the item Cost of replacement if the item were lost today Dollar value of the product of using the item  By the same token, the greedy algorithm\u0026rsquo;s results might be improved with the use of one of these alternate metrics.\nOn top of choosing an appropriate approach to solving the knapsack problem in general, it is helpful to design our algorithm in a way that translates the practicalities of a scenario into code.\nThere are many considerations for better algorithm design beyond the scope of this introductory post. One of these is time complexity, and I\u0026rsquo;ve written about it here. A future algorithm may very well decide my bag\u0026rsquo;s contents on the next trip, but we\u0026rsquo;re not quite there yet. Stay tuned!\n",url:"https://victoria.dev/blog/knapsack-problem-algorithms-for-my-real-life-carry-on-knapsack/"},"https:\/\/victoria.dev\/blog\/why-im-automatically-deleting-my-old-tweets-using-aws-lambda\/":{title:"Why I\u0027m automatically deleting my old tweets using AWS Lambda",tags:["life","aws","go"],content:"From now on, my tweets are ephemeral. Here’s why I’m deleting all my old tweets, and the AWS Lambda function I’m using to do all this for free.\nClick here to skip to the code part.\nStuff and opinions I\u0026rsquo;ve only been a one-bag nomad for a little over a year and a half. Before that, I lived as most people do in an apartment or a house. I owned furniture, more clothing than I strictly needed, and enough \u0026ldquo;stuff\u0026rdquo; to fill at least a few moving boxes. If I went to live somewhere else, moving for school or family or work, I packed up all my things and brought them with me. Over the years, I accumulated more and more stuff.\nAdopting what many would call a minimalist lifestyle has rapidly changed a lot of my longstanding views. Giving away all my stuff (an idea I once thought to be interesting in principle but practically a little bit ridiculous) has become normal. It\u0026rsquo;s normal for me, now, to not own things that I don\u0026rsquo;t use on a regular basis. I don\u0026rsquo;t keep wall shelves packed with old books or dishes or clothing or childhood toys because those items aren\u0026rsquo;t relevant to me anymore. I just keep fond memories, instead.\nImagine, for a moment, that I still lived in a house. Imagine that in that house, on the fridge, is a drawing I made when I was six-years-old. In the bottom right corner of that drawing scribbled in green crayon are the words \u0026ldquo;broccoli is dumb - Victoria, Age 6.\u0026rdquo;\nIf you were in my house and saw that drawing on the fridge, would you assume that the statement \u0026ldquo;broccoli is dumb\u0026rdquo; comprised an accurate and current account of my opinions on broccoli? Of course not. I was six when I wrote that. I\u0026rsquo;ve had plenty of time to change my mind.\nSocial media isn\u0026rsquo;t social I have a friend whom I\u0026rsquo;ve known since we were both in kindergarten. We went through grade school together, then spoke to and saw each other on infrequent occasions across the years. We\u0026rsquo;re both adults now. Sometimes when we chat, we\u0026rsquo;ll recall some amusing memory from when we were younger. The nature of memory being what it is, I have no illusion that what we recall is recounted with much accuracy. Our impressions of things that happened - mistakes we made and moments of victory alike - are coloured by the experiences we\u0026rsquo;ve had since then, and all the things we\u0026rsquo;ve learned. An awkward moment at a school colleague\u0026rsquo;s birthday party becomes an example of a child learning to socialize, instead of the world-ending moment of embarrassment it probably felt like at the time.\nThis is how memory works. In a sense, it gets updated, as well it should. People living in small communities remember things that their neighbour did many years ago, but recall them in the context of who their neighbour is now, and what their current relationship is like. This re-colouring of history is an important part of how people heal, make good decisions, and socialize.\nSocial media does not do this. Your perfectly preserved tweet from five days or five years ago can be recalled with absolute accuracy. For most people, this is not particularly worrying. We tend to tweet about pretty mundane things - things that pop into mind when we\u0026rsquo;re bored and want someone to notice us. Individually, usually, our old tweets are pretty insignificant. In aggregate, however, they paint a pretty complete picture of a person\u0026rsquo;s random, unintentionally telling thoughts. This is the problem.\nThe assumption made of things written in social media and on Twitter specifically is a very different assumption than you might make about someone\u0026rsquo;s notepad scribble from last week. I\u0026rsquo;m not endeavoring to speculate why - I\u0026rsquo;ve just seen enough cases of someone getting publicly flogged for something they posted years ago to know that it does happen. This is weird. If you wouldn\u0026rsquo;t assume that a notepad scribble from last week or a crayon drawing from decades ago reflects the essence of who someone is now, why would you assume that an old tweet does?\nYou are not the same person you were last month - you\u0026rsquo;ve seen things, read things, understood and learned things that have, in some small way, changed you. While a person may have the same sense of self and identity through most of their life, even this grows and changes over the years. We change our opinions, our desires, our habits. We are not stagnant beings, and we should not let ourselves be represented as such, however unintentionally.\nEphemeral tweets If you look at my Twitter profile page today, you\u0026rsquo;ll see fewer tweets there than you have fingers (I hope). I\u0026rsquo;m using ephemeral - a lightweight utility I wrote for use on AWS Lambda - to delete all my tweets older than a few days. I\u0026rsquo;m doing this for the same reason that I don\u0026rsquo;t hang on to stuff that I no longer use - that stuff isn\u0026rsquo;t relevant to me anymore. It doesn\u0026rsquo;t represent me, either.\nThe code that makes up ephemeral is written in Go. AWS Lambda creates an environment for each Lambda function, so ephemeral utilizes environment variables for your private Twitter API keys and the maximum age of the tweets you want to keep, represented in hours, like 72h.\nvar ( consumerKey = getenv(\u0026#34;TWITTER_CONSUMER_KEY\u0026#34;) consumerSecret = getenv(\u0026#34;TWITTER_CONSUMER_SECRET\u0026#34;) accessToken = getenv(\u0026#34;TWITTER_ACCESS_TOKEN\u0026#34;) accessTokenSecret = getenv(\u0026#34;TWITTER_ACCESS_TOKEN_SECRET\u0026#34;) maxTweetAge = getenv(\u0026#34;MAX_TWEET_AGE\u0026#34;) logger = log.New() ) func getenv(name string) string { v := os.Getenv(name) if v == \u0026#34;\u0026#34; { panic(\u0026#34;missing required environment variable \u0026#34; + name) } return v } The program uses the anaconda library. It fetches your timeline up to the Twitter API\u0026rsquo;s limit of 200 tweets per request, then compares each tweet\u0026rsquo;s date of creation to your MAX_TWEET_AGE variable to decide whether it\u0026rsquo;s old enough to be deleted. After deleting all the expired tweets, the Lambda function terminates.\nfunc deleteFromTimeline(api *anaconda.TwitterApi, ageLimit time.Duration) { timeline, err := getTimeline(api) if err != nil { log.Error(\u0026#34;Could not get timeline\u0026#34;) } for _, t := range timeline { createdTime, err := t.CreatedAtTime() if err != nil { log.Error(\u0026#34;Couldn\u0026#39;t parse time \u0026#34;, err) } else { if time.Since(createdTime) \u0026gt; ageLimit { _, err := api.DeleteTweet(t.Id, true) log.Info(\u0026#34;DELETED: Age - \u0026#34;, time.Since(createdTime).Round(1*time.Minute), \u0026#34; - \u0026#34;, t.Text) if err != nil { log.Error(\u0026#34;Failed to delete! \u0026#34;, err) } } } } log.Info(\u0026#34;No more tweets to delete.\u0026#34;) } Read the full code here.\nFor a use case like this, AWS Lambda has a free tier that costs nothing. If you\u0026rsquo;re any level of developer, it\u0026rsquo;s an extremely useful tool to become familiar with. For a full walkthrough with screenshots of how to set up a Lambda function that tweets for you, you can read this article. The set up for ephemeral is the same, it just has an opposite function. :)\nI forked ephemeral from Adam Drake\u0026rsquo;s Harold, a Twitter tool that has many useful functions beyond keeping your timeline trimmed. If you have more than 200 tweets to delete at first pass, please use Harold to do that first. You can run Harold with the deletetimeline flag from your terminal.\nYou may like to first download all your tweets before deleting them for sentimental value.\nWhy use Twitter at all In anticipation of the question, let me say that yes, I do use Twitter besides just as a bucket for my Lambda functions to fill and empty. It has its benefits, most related to what I perceive to be its original intended purpose: to be a means of near-instant communication for short, digestible pieces of information reaching a widespread pool of people.\nI use it as a way to keep tabs on what\u0026rsquo;s happening right now. I use it to comment on, joke about, and commiserate with things tweeted by the people I follow right now. By keeping my timeline restricted to only the most recent few days, I feel like I\u0026rsquo;m using Twitter more like it was meant to be used: a way to join the conversation and see what\u0026rsquo;s happening in the world right now - instead of just another place to amass more \u0026ldquo;stuff.\u0026rdquo;\n",url:"https://victoria.dev/blog/why-im-automatically-deleting-my-old-tweets-using-aws-lambda/"},"https:\/\/victoria.dev\/blog\/running-a-free-twitter-bot-on-aws-lambda\/":{title:"Running a free Twitter bot on AWS Lambda",tags:["aws","go"],content:"If you read About time, you\u0026rsquo;ll know that I\u0026rsquo;m a big believer in spending time now on building things that save time in the future. To this end I built a simple Twitter bot in Go that would occasionally post links to my articles and keep my account interesting even when I\u0026rsquo;m too busy to use it. The tweets help drive traffic to my sites, and I don\u0026rsquo;t have to lift a finger.\nI ran the bot on an Amazon EC2 instance for about a month. My AWS usage has historically been pretty inexpensive (less than the price of a coffee in most of North America), so I was surprised when the little instance I was using racked up a bill 90% bigger than the month before. I don\u0026rsquo;t think AWS is expensive, to be clear, but still\u0026hellip; I\u0026rsquo;m cheap. I want my Twitter bot, and I want it for less.\nI\u0026rsquo;d been meaning to explore AWS Lamda, and figured this was a good opportunity. Unlike an EC2 instance that is constantly running (and charging you for it), Lambda charges you per request and according to the duration of time your function takes to run. There\u0026rsquo;s a free tier, too, and the first 1 million requests, plus a certain amount of compute time, are free. Roughly translated to running a Twitter bot that posts for you, say, twice a day, your monthly cost for using Lambda would total\u0026hellip; carry the one\u0026hellip; nothing. I\u0026rsquo;ve been running my Lambda function for a couple weeks now, completely free.\nWhen recently it came to me to take the reigns of the @freeCodeCampTO Twitter, I decided to employ a similar strategy, and also use this opportunity to document the process for you, dear reader.\nSo if you\u0026rsquo;re currently using a full-time running instance for a task that could be served by a cron job, this is the article for you. I\u0026rsquo;ll cover how to write your function for Lambda, how to get it set up to run automatically, and as a sweet little bonus, a handy bash script that updates your function from the command line whenever you need to make a change. Let\u0026rsquo;s do it!\nIs Lambda right for you When I wrote the code for my Twitter bot in Go, I intended to have it run on an AWS instance and borrowed heavily from Francesc\u0026rsquo;s awesome Just for Func episode. Some time later I modified it to randomly choose an article from my RSS feeds and tweet the link, twice a day. I wanted to do something similar for the @freeCodeCampTO bot, and have it tweet an inspiring quote about programming every morning.\nThis is a good use case for Lambda because:\n The program should execute once It runs on a regular schedule, using time as a trigger It doesn\u0026rsquo;t need to run constantly  The important thing to keep in mind is that Lambda runs a function once in response to an event that you define. The most widely applicable trigger is a simple cron expression, but there are many other trigger events you can hook up. You can get an overview here.\nWrite a Lambda function I found this really straightforward to do in Go. First, grab the aws-lambda-go library:\ngo get github.com/aws/aws-lambda-go/lambda Then make this your func main():\nfunc main() { lambda.Start(tweetFeed) } Where tweetFeed is the name of the function that makes everything happen. While I won\u0026rsquo;t go into writing the whole Twitter bot here, you can view my code on GitHub.\nSetting up AWS Lambda I\u0026rsquo;m assuming you already have an AWS account. If not, first things first here: https://aws.amazon.com/free\n1. Create your function Find AWS Lambda in the list of services, then look for this shiny button:\nWe\u0026rsquo;re going to author a function from scratch. Name your function, then under Runtime choose \u0026ldquo;Go 1.x\u0026rdquo;.\nUnder Role name write any name you like. It\u0026rsquo;s a required field but irrelevant for this use case.\nClick Create function.\n2. Configure your function You\u0026rsquo;ll see a screen for configuring your new function. Under Handler enter the name of your Go program.\nIf you scroll down, you\u0026rsquo;ll see a spot to enter environment variables. This is a great place to enter the Twitter API tokens and secrets, using the variable names that your program expects. The AWS Lambda function will create the environment for you using the variables you provide here.\nNo further settings are necessary for this use case. Click Save at the top of the page.\n3. Upload your code You can upload your function code as a zip file on the configuration screen. Since we\u0026rsquo;re using Go, you\u0026rsquo;ll want to go build first, then zip the resulting executable before uploading that to Lambda.\n\u0026hellip;Of course I\u0026rsquo;m not going to do that manually every time I want to tweak my function. That\u0026rsquo;s what awscli and this bash script is for!\nupdate.sh\ngo build \u0026amp;\u0026amp; \\ zip fcc-tweet.zip fcc-tweet \u0026amp;\u0026amp; \\ rm fcc-tweet \u0026amp;\u0026amp; \\ aws lambda update-function-code --function-name fcc-tweet --zip-file fileb://fcc-tweet.zip \u0026amp;\u0026amp; \\ rm fcc-tweet.zip Now whenever I make a tweak, I just run bash update.sh.\nIf you\u0026rsquo;re not already using AWS Command Line Interface, do pip install awscli and thank me later. Find instructions for getting set up and configured in a few minutes here under Quick Configuration.\n4. Test your function Wanna see it go? Of course you do! Click \u0026ldquo;Configure test events\u0026rdquo; in the dropdown at the top.\nSince you\u0026rsquo;ll use a time-based trigger for this function, you don\u0026rsquo;t need to enter any code to define test events in the popup window. Simply write any name under Event name and empty the JSON in the field below. Then click Create.\nClick Test at the top of the page, and if everything is working correctly you should see\u0026hellip;\n5. Set up CloudWatch Events To run our function as we would a cron job - as a regularly scheduled time-based event - we\u0026rsquo;ll use CloudWatch. Click CloudWatch Events in the Designer sidebar.\nUnder Configure triggers, you\u0026rsquo;ll create a new rule. Choose a descriptive name for your rule without spaces or punctuation, and ensure Schedule expression is selected. Then input the time you want your program to run as a rate expression, or cron expression.\nA cron expression looks like this: cron(0 12 * * ? *)\n   Minutes Hours Month Day of week Year In English     0 12 * ? * Run at noon (UTC) every day    For more on how to write your cron expressions, read this.\nSee the current time in UTC.\nIf you want your program to run twice a day, say once at 10am and again at 3pm, you\u0026rsquo;ll need to set two separate CloudWatch Events triggers and cron expression rules.\nClick Add.\nWatch it go That\u0026rsquo;s all you need to get your Lambda function up and running! Now you can sit back, relax, and do more important things than share your RSS links on Twitter.\n",url:"https://victoria.dev/blog/running-a-free-twitter-bot-on-aws-lambda/"},"https:\/\/victoria.dev\/blog\/moving-to-a-new-domain-without-breaking-old-links-with-aws-disqus\/":{title:"Moving to a new domain without breaking old links with AWS \u0026 Disqus",tags:["aws","websites"],content:"I started blogging about my nomadic travels last year, and so far the habit has stuck. Like all side projects, I won\u0026rsquo;t typically invest heavily in setting up web properties before I can be reasonably certain that such an investment is worth my time or enjoyment. In other words: don\u0026rsquo;t buy the domain until you\u0026rsquo;ve proven to yourself that you\u0026rsquo;ll stick with it!\nAfter some months of regular posting I felt I was ready to commit (short courtship, I know, but we\u0026rsquo;re all adults here) and I bought a dedicated domain, herOneBag.com.\nUp until recently, my #NomadLyfe blog was just a subdirectory of my main personal site. Now it\u0026rsquo;s all grown up and ready to strike out into the world alone! Here\u0026rsquo;s the setup for the site:\n Static site in Amazon Web Services S3 bucket Route 53 handling the DNS CloudFront for distribution and a custom SSL certificate Disqus for comments  If you\u0026rsquo;d like a walk-through for how to set up a new domain with this structure, it\u0026rsquo;s over here: Hosting your static site with AWS S3, Route 53, and CloudFront. In this post, I\u0026rsquo;ll just detail how I managed to move my blog to the new site without breaking the old links or losing any comments.\nPreserve old links with redirection rules I wanted to avoid breaking links that have been posted around the web by forwarding visitors to the new URL. The change looks like this:\nOld URL: https://victoria.dev/meta/5-bag-lessons/\nNew URL: https://heronebag.com/blog/5-bag-lessons/\nYou can see that the domain name as well as the subdirectory have changed, but the slug for the blog post remains the same. (I love static sites.)\nTo redirect links from the old site, we\u0026rsquo;ll need to set redirection rules in the old site\u0026rsquo;s S3 bucket. AWS provides a way to set up a conditional redirect. This is set in the \u0026ldquo;Redirection rules\u0026rdquo; section of your S3 bucket\u0026rsquo;s properties, under \u0026ldquo;Static website hosting.\u0026rdquo; You can find the documentation here.\nThere are a few examples given, but none that represent the redirect I want. In addition to changing the prefix of the object key, we\u0026rsquo;re also changing the domain. The latter is achieved with the \u0026lt;HostName\u0026gt; tag.\nTo redirect requests for the old blog URL to the new top level domain, we\u0026rsquo;ll use the code below.\n\u0026lt;RoutingRules\u0026gt; \u0026lt;RoutingRule\u0026gt; \u0026lt;Condition\u0026gt; \u0026lt;KeyPrefixEquals\u0026gt;oldblog/\u0026lt;/KeyPrefixEquals\u0026gt; \u0026lt;/Condition\u0026gt; \u0026lt;Redirect\u0026gt; \u0026lt;HostName\u0026gt;newdomain.com\u0026lt;/HostName\u0026gt; \u0026lt;ReplaceKeyPrefixWith\u0026gt;newblog/\u0026lt;/ReplaceKeyPrefixWith\u0026gt; \u0026lt;/Redirect\u0026gt; \u0026lt;/RoutingRule\u0026gt; \u0026lt;/RoutingRules\u0026gt; This rule ensures that requests for olddomain.com/oldblog/specific-blog-post will redirect to newdomain.com/newblog/specific-blog-post.\nMigrate Disqus comments Disqus provides a tool for migrating the comment threads from your old blog site to the new one. You can find it in your Disqus admin tools at your-short-name.disqus.com/admin/discussions/migrate/.\nTo migrate posts from the old blog address to the new one, we\u0026rsquo;ll use the URL mapper tool. Click \u0026ldquo;Start URL mapper,\u0026rdquo; then \u0026ldquo;you can download a CSV here.\u0026rdquo;\nDisqus has decent instructions for how this tool works, and you can read them here. Basically, you\u0026rsquo;ll input the new blog URLs into the second column of the CSV file you downloaded, then pass it back to Disqus to process. If you\u0026rsquo;re using a program to edit the CSV, be sure to save the resulting file in CSV format.\nUnless you have a bazillion URLs, the tool works pretty quickly, and you\u0026rsquo;ll get an email when it\u0026rsquo;s finished. Don\u0026rsquo;t forget to update the name of your site in the Disqus admin, too.\nTransfer other settings Update links in your social profiles and any other sites you may have around the web. If you\u0026rsquo;re using other services attached to your website like Google Analytics or IFTTT, don\u0026rsquo;t forget to update those details too!\n",url:"https://victoria.dev/blog/moving-to-a-new-domain-without-breaking-old-links-with-aws-disqus/"},"https:\/\/victoria.dev\/blog\/a-unicode-substitution-cipher-algorithm\/":{title:"A Unicode substitution cipher algorithm",tags:["algorithms","javascript"],content:"Full transparency: I occasionally waste time messing around on Twitter. (Gasp! Shock!) One of the ways I waste time messing around on Twitter is by writing my name in my profile with different Unicode character \u0026ldquo;fonts,\u0026rdquo; 𝖑𝖎𝖐𝖊 𝖙𝖍𝖎𝖘 𝖔𝖓𝖊. I previously did this by searching for different Unicode characters on Google, then one-by-one copying and pasting them into the \u0026ldquo;Name\u0026rdquo; field on my Twitter profile. Since this method of wasting time was a bit of a time waster, I decided (in true programmer fashion) to write a tool that would help me save some time while wasting it.\nI originally dubbed the tool \u0026ldquo;uni-pretty,\u0026rdquo; (based on LEGO\u0026rsquo;s Unikitty from a movie \u0026ndash; a pun that absolutely no one got) but have since renamed it fancy unicode. It builds from this GitHub repo. It lets you type any characters into a field and then converts them into Unicode characters that also represent letters, giving you fancy \u0026ldquo;fonts\u0026rdquo; that override a website\u0026rsquo;s CSS, like in your Twitter profile. (Sorry, Internet.)\nThe tool\u0026rsquo;s first naive iteration existed for about twenty minutes while I copy-pasted Unicode characters into a data structure. This approach of storing the characters in the JavaScript file, called hard-coding, is fraught with issues. Besides having to store every character from every font style, it\u0026rsquo;s painstaking to build, hard to update, and more code means it\u0026rsquo;s susceptible to more possible errors.\nFortunately, working with Unicode means that there\u0026rsquo;s a way to avoid the whole mess of having to store all the font characters: Unicode numbers are sequential. More importantly, the special characters in Unicode that could be used as fonts (meaning that there\u0026rsquo;s a matching character for most or all of the letters of the alphabet) are always in the following sequence: capital A-Z, lowercase a-z.\nFor example, in the fancy Unicode above, the lowercase letter \u0026ldquo;L\u0026rdquo; character has the Unicode number U+1D591 and HTML code \u0026amp;#120209;. The next letter in the sequence, a lowercase letter \u0026ldquo;M,\u0026rdquo; has the Unicode number U+1D592 and HTML code \u0026amp;#120210;. Notice how the numbers in those codes increment by one.\nWhy\u0026rsquo;s this relevant? Since each special character can be referenced by a number, and we know that the order of the sequence is always the same (capital A-Z, lowercase a-z), we\u0026rsquo;re able to produce any character simply by knowing the first number of its font sequence (the capital \u0026ldquo;A\u0026rdquo;). If this reminds you of anything, you can borrow my decoder pin.\nIn cryptography, the Caesar cipher (or shift cipher) is a simple method of encryption that utilizes substitution of one character for another in order to encode a message. This is typically done using the alphabet and a shift \u0026ldquo;key\u0026rdquo; that tells you which letter to substitute for the original one. For example, if I were trying to encode the word \u0026ldquo;cat\u0026rdquo; with a right shift of 3, it would look like this:\nc a t f d w With this concept, encoding our plain text letters as a Unicode \u0026ldquo;font\u0026rdquo; is a simple process. All we need is an array to reference our plain text letters with, and the first index of our Unicode capital \u0026ldquo;A\u0026rdquo; representation. Since some Unicode numbers also include letters (which are sequential, but an unnecessary complication) and since the intent is to display the page in HTML, we\u0026rsquo;ll use the HTML code number \u0026amp;#120172;, with the extra bits removed for brevity.\nvar plain = [\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;D\u0026#39;, \u0026#39;E\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;G\u0026#39;, \u0026#39;H\u0026#39;, \u0026#39;I\u0026#39;, \u0026#39;J\u0026#39;, \u0026#39;K\u0026#39;, \u0026#39;L\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;N\u0026#39;, \u0026#39;O\u0026#39;, \u0026#39;P\u0026#39;, \u0026#39;Q\u0026#39;, \u0026#39;R\u0026#39;, \u0026#39;S\u0026#39;, \u0026#39;T\u0026#39;, \u0026#39;U\u0026#39;, \u0026#39;V\u0026#39;, \u0026#39;W\u0026#39;, \u0026#39;X\u0026#39;, \u0026#39;Y\u0026#39;, \u0026#39;Z\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;j\u0026#39;, \u0026#39;k\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;n\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;p\u0026#39;, \u0026#39;q\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;s\u0026#39;, \u0026#39;t\u0026#39;, \u0026#39;u\u0026#39;, \u0026#39;v\u0026#39;, \u0026#39;w\u0026#39;, \u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;, \u0026#39;z\u0026#39;]; var fancyA = 120172; Since we know that the letter sequence of the fancy Unicode is the same as our plain text array, any letter can be found by using its index in the plain text array as an offset from the fancy capital \u0026ldquo;A\u0026rdquo; number. For example, capital \u0026ldquo;B\u0026rdquo; in fancy Unicode is the capital \u0026ldquo;A\u0026rdquo; number, 120172 plus B\u0026rsquo;s index, which is 1: 120173.\nHere\u0026rsquo;s our conversion function:\nfunction convert(string) { // Create a variable to store our converted letters  let converted = []; // Break string into substrings (letters)  let arr = string.split(\u0026#39;\u0026#39;); // Search plain array for indexes of letters  arr.forEach(element =\u0026gt; { let i = plain.indexOf(element); // If the letter isn\u0026#39;t a letter (not found in the plain array)  if (i == -1) { // Return as a whitespace  converted.push(\u0026#39; \u0026#39;); } else { // Get relevant character from fancy number + index  let unicode = fancyA + i; // Return as HTML code  converted.push(\u0026#39;\u0026amp;#\u0026#39; + unicode + \u0026#39;;\u0026#39;); } }); // Print the converted letters as a string  console.log(converted.join(\u0026#39;\u0026#39;)); } A neat possibility for this method of encoding requires a departure from my original purpose, which was to create a human-readable representation of the original string. If the purpose was instead to produce a cipher, this could be done by using any Unicode index in place of fancyA as long as the character indexed isn\u0026rsquo;t a representation of a capital \u0026ldquo;A.\u0026rdquo;\nHere\u0026rsquo;s the same code set up with a simplified plain text array, and a non-letter-representation Unicode key:\nvar plain = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;f\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;h\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;j\u0026#39;, \u0026#39;k\u0026#39;, \u0026#39;l\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;n\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;p\u0026#39;, \u0026#39;q\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;s\u0026#39;, \u0026#39;t\u0026#39;, \u0026#39;u\u0026#39;, \u0026#39;v\u0026#39;, \u0026#39;w\u0026#39;, \u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;, \u0026#39;z\u0026#39;]; var key = 9016; You might be able to imagine that decoding a cipher produced by this method would be relatively straightforward, once you knew the encoding secret. You\u0026rsquo;d simply need to subtract the key from the HTML code numbers of the encoded characters, then find the relevant plain text letters at the remaining indexes.\nWell, that\u0026rsquo;s it for today. Be sure to drink your Ovaltine and we\u0026rsquo;ll see you right here next Monday at 5:45!\nOh, and\u0026hellip; ⍔⍠⍟⍘⍣⍒⍥⍦⍝⍒⍥⍚⍠⍟⍤ ⍒⍟⍕ ⍨⍖⍝⍔⍠⍞⍖ ⍥⍠ ⍥⍙⍖ ⍔⍣⍪⍡⍥⍚⍔ ⍦⍟⍚⍔⍠⍕⍖ ⍤⍖⍔⍣⍖⍥ ⍤⍠⍔⍚⍖⍥⍪\n:)\n",url:"https://victoria.dev/blog/a-unicode-substitution-cipher-algorithm/"},"https:\/\/victoria.dev\/blog\/hosting-your-static-site-with-aws-s3-route-53-and-cloudfront\/":{title:"Hosting your static site with AWS S3, Route 53, and CloudFront",tags:["aws","websites"],content:"Some time ago I decided to stop freeloading on GitHub pages and move one of my sites to Amazon Web Services (AWS). It turns out that I\u0026rsquo;m still mostly freeloading (yay free tier) so it amounted to a learning experience. Here are the components that let me host and serve the site at my custom domain with HTTPS.\n Static site in Amazon Web Services S3 bucket Route 53 handling the DNS CloudFront for distribution and a custom SSL certificate  I set all that up most of a year ago. At the time, I found the AWS documentation to be rather fragmented and inconvenient to follow - it was hard to find what you were looking for without knowing what a specific setting might be called, or where it was, or if it existed at all. When I recently set up a new site and stumbled through this process again, I didn\u0026rsquo;t find it any easier. Hopefully this post can help to collect the relevant information into a more easily followed process and serve as an accompanying guide to save future me (and you) some time.\nRather than replace existing documentation, this post is meant to supplement it. Think of me as your cool tech-savvy friend on the phone with you at 4am, troubleshooting your website. (Please don\u0026rsquo;t actually call me at 4am.) I\u0026rsquo;ll walk through the set up while providing links for the documentation that was ultimately helpful (mostly so I can find it again later\u0026hellip;).\nHosting a static site with Amazon S3 and a custom domain If you\u0026rsquo;re starting from scratch, you\u0026rsquo;ll need an AWS account. It behooves you to get one, even if you don\u0026rsquo;t like paying for services - there\u0026rsquo;s a free tier that will cover most of the experimental stuff you\u0026rsquo;re going to want to do in the first year, and even the things I do pay for cost me less than a dollar a month. You can sign up at https://aws.amazon.com/free.\nGetting your static site hosted and available at your custom domain is your first mission, should you choose to accept it. Your instructions are here.\nCreating the buckets for site hosting on S3 is the most straightforward part of this process in my opinion, and the AWS documentation walkthrough covers what you\u0026rsquo;ll need to do quite well. It gets a little unclear around Step 3: Create and Configure Amazon Route 53 Hosted Zone, so come back and read on once you\u0026rsquo;ve reached that point. I\u0026rsquo;ll make some tea in the meantime.\n\u0026hellip; 🎶 🎵\nReady? Cool. See, I\u0026rsquo;m here for you.\nSet up Route 53 The majority of the work in this section amounts to creating the correct record sets for your custom domain. If you\u0026rsquo;re already familiar with how record sets work, the documentation is a bit of a slog. Here\u0026rsquo;s how it should look when you\u0026rsquo;re finished:\nThe \u0026ldquo;NS\u0026rdquo; and \u0026ldquo;SOA\u0026rdquo; records are created automatically for you. The only records you need to create are the \u0026ldquo;A\u0026rdquo; records.\nHop over to Route 53 and follow this walkthrough to create a \u0026ldquo;hosted zone.\u0026rdquo; The value of the NS (Name Servers) records are what you\u0026rsquo;ll have to provide to your domain name registrar. Your registrar is wherever you bought your custom domain, such as this super subtle Namecheap.com affiliate link right here. (Thanks for your support! 😊)\nIf you created two buckets in the first section (one for yourdomain.com and one for www.yourdomain.com), you\u0026rsquo;ll need two separate A records in Route 53. Initially, these have the value of the endpoints for your matching S3 buckets (looks like s3-website.us-east-2.amazonaws.com). Later, you\u0026rsquo;ll change them to your CloudFront domain name.\nIf you went with Namecheap as your registrar, Step 4 looks like this:\nWaiting is the hardest part\u0026hellip; I\u0026rsquo;ve gotten into the habit of working on another project or setting up the DNS change before going to bed so that changes have time to propagate without me feeling like I need to fiddle with it. ^^;\nWhen the transfer\u0026rsquo;s ready, you\u0026rsquo;ll see your site at http://yourdomain.com. Next, you\u0026rsquo;ll want to set up CloudFront so that becomes https://yourdomain.com.\nSet up CloudFront and SSL Here are the instructions for setting up CloudFront. There are a few important points to make sure you don\u0026rsquo;t miss on the \u0026ldquo;Create Distribution\u0026rdquo; page:\n Origin Domain Name: Make sure to use your S3 bucket endpoint, and not select the bucket from the dropdown menu that appears. Viewer Protocol Policy: If you want requests for http://yourdomain.com to always result in https://yourdomain.com, choose \u0026ldquo;Redirect HTTP to HTTPS.\u0026rdquo; Alternate Domain Names: Enter yourdomain.com and www.yourdomain.com on separate lines. SSL Certificate: See below. Default Root Object: Enter the name of the html file that should be returned when your users go to https://yourdomain.com. This is usually \u0026ldquo;index.html\u0026rdquo;.  SSL Certificate To show your content with HTTPS at your custom domain, you\u0026rsquo;ll need to choose \u0026ldquo;Custom SSL Certificate.\u0026rdquo; You can easily get an SSL Certificate with AWS Certificate Manager. Click on \u0026ldquo;Request or Import a Certificate with ACM\u0026rdquo; to get started in a new window.\nHere are instructions for setting up a certificate. I don\u0026rsquo;t think they\u0026rsquo;re very good, personally. Don\u0026rsquo;t worry, I got you.\nTo account for \u0026ldquo;www.yourdomain.com\u0026rdquo; as well as any subdomains, you\u0026rsquo;ll want to add two domain names to the certificate, like so:\nClick \u0026ldquo;Next.\u0026rdquo; You\u0026rsquo;ll be asked to choose a validation method. Choose \u0026ldquo;DNS validation\u0026rdquo; and click \u0026ldquo;Review.\u0026rdquo; If everything is as it should be, click \u0026ldquo;Confirm and request.\u0026rdquo;\nYou\u0026rsquo;ll see a page, \u0026ldquo;Validation\u0026rdquo; that looks like this. You\u0026rsquo;ll have to click the little arrow next to both domain names to get the important information to show:\nUnder both domain names, click the button for \u0026ldquo;Create record in Route 53.\u0026rdquo; This will automatically create a CNAME record set in Route 53 with the given values, which ACM will then check in order to validate that you own those domains. You could create the records manually, if you wanted to for some reason. I don\u0026rsquo;t know, maybe you\u0026rsquo;re killing time. ¯\\_(ツ)_/¯\nClick \u0026ldquo;Continue.\u0026rdquo; You\u0026rsquo;ll see a console that looks like this:\nIt may take some time for the validation to complete, at which point the \u0026ldquo;Pending validation\u0026rdquo; status will change to \u0026ldquo;Issued.\u0026rdquo; Again with the waiting. You can close this window to return to the CloudFront set up. Once the certificate is validated, you\u0026rsquo;ll see it in the dropdown menu under \u0026ldquo;Custom SSL Certificate.\u0026rdquo; You can click \u0026ldquo;Create Distribution\u0026rdquo; to finish setting up CloudFront.\nIn your CloudFront Distributions console, you\u0026rsquo;ll see \u0026ldquo;In Progress\u0026rdquo; until AWS has done its thing. Once it\u0026rsquo;s done, it\u0026rsquo;ll change to \u0026ldquo;Deployed.\u0026rdquo;\nOne last thing Return to your Route 53 console and click on \u0026ldquo;Hosted zones\u0026rdquo; in the sidebar, then your domain name from the list. For both A records, change the \u0026ldquo;Alias Target\u0026rdquo; from the S3 endpoint to your CloudFront distribution domain, which should look something like dj4p1rv6mvubz.cloudfront.net. It appears in the dropdown after you clear the field.\nYou\u0026rsquo;re done Well, usually. If you navigate to your new HTTPS domain and don\u0026rsquo;t see your beautiful new site where it should be, here are some things you can do:\n Check S3 bucket policy - ensure that the bucket for yourdomain.com in the S3 console shows \u0026ldquo;Public\u0026rdquo; in the \u0026ldquo;Access\u0026rdquo; column. Check S3 bucket index document - In the \u0026ldquo;metadata\u0026rdquo; tab for the bucket, then \u0026ldquo;Static website hosting\u0026rdquo;. Usually \u0026ldquo;index.html\u0026rdquo;. Check CloudFront Origin - the \u0026ldquo;Origin\u0026rdquo; column in the CloudFront Console should show the S3 bucket\u0026rsquo;s endpoint (s3-website.us-east-2.amazonaws.com), not the bucket name (yourdomain.com.s3.amazonaws.com). Check CloudFront Default Root Object - clicking on the distribution name should take you to a details page that shows \u0026ldquo;Default Root Object\u0026rdquo; in the list with the value that you set, usually \u0026ldquo;index.html\u0026rdquo;. Wait. Sometimes changes take up to 48hrs to propagate. ¯\\_(ツ)_/¯  I hope that helps you get set up with your new static site on AWS! If you found this post helpful, there\u0026rsquo;s a lot more where this came from. You can subscribe to victoria.dev to see new posts first!\n",url:"https://victoria.dev/blog/hosting-your-static-site-with-aws-s3-route-53-and-cloudfront/"},"https:\/\/victoria.dev\/blog\/about-time\/":{title:"About time",tags:["life"],content:"This morning I read an article that\u0026rsquo;s been making the rounds lately: Modern Media Is a DoS Attack on Your Free Will.\nIt\u0026rsquo;s made me think, which I must admit, I at first didn\u0026rsquo;t like. See, when I wake up in the morning (and subsequently wake up my computer) the first thing I do is go on Twitter to catch up on everything I missed while I was asleep. All this before my first coffee, mind you. Links on Twitter usually lead to stories on Medium, newly released apps on ProductHunt, and enticing sales on a new gadget or two on Amazon. Wherever it goes, in those blissfully half-awake mental recesses, the last thing I\u0026rsquo;m trying to do is think.\nHowever, yesterday, I also happened to listen to a podcast from freeCodeCamp. It was #7: The code I\u0026rsquo;m still ashamed of. This lead to thoughts on the responsibilities of programmers - the people tasked with designing and building apps and systems meant to steer the very course of your life.\nThis morning, the combined swirling mess of notions brought on by these two sources of information had, even before my first coffee, the unfortunate effect of making me think.\nMostly, I thought about intention, and time.\nI don\u0026rsquo;t believe it\u0026rsquo;s wildly inaccurate to say that when you go about doing something in your daily life, you have a general awareness of your reason for doing it. If you leave your building and go down the street to Starbucks and buy a coffee, more often than not, it\u0026rsquo;s because you wanted a coffee. If you go to the corner store and buy a litre of milk, you probably intend to drink it. If you find yourself nicely dressed on a Friday night waiting at a well-decorated restaurant to meet another human being with whom you share an apparent mutual attraction, I can risk a guess that you\u0026rsquo;re after some form of pleasant human interaction.\nIn each of these, and many more examples you can think up, the end goal is clearly defined. There is an expected final step to the process; an expected response; a return value.\nWhat is the return value of opening up the Twitter app? Browsing Facebook? Instagram? In fact, any social media?\nThe concrete answer is that there isn\u0026rsquo;t one. Perhaps in those of us with resilient self-discipline, there may at least be some sort of time limitation. That\u0026rsquo;s the most we can hope for, however, and no wonder - that\u0026rsquo;s what these and other similar services have been designed for. They\u0026rsquo;re built to be open-ended black-holes for our most precious resource\u0026hellip; time.\n In the case of the Analytical Engine we have undoubtedly to lay out a certain capital of analytical labour in one particular line; but this is in order that the engine may bring us in a much larger return in another line.\nAda Augusta (Ada Lovelace) - Notes on Sketch of The Analytical Engine\n Okay, so I did some more reading. Specifically, #ThrowbackThursday to the mid 1800\u0026rsquo;s and something my good friend Ada Lovelace once scribbled in a book. Widely considered one of the first computer programmers, she and Charles Babbage pioneered many concepts that programmers today take for granted. The one I\u0026rsquo;m going to hang my point on is, I think, nicely encapsulated in the above quote: the things programmers make are supposed to save you time.\nSave it. Not lose it.\nI think Ada and Charles would agree that, observing the effects of social media apps, clickbait news sites, and many other forms of attention-hogging interactivity that we haven\u0026rsquo;t even classified yet - something\u0026rsquo;s gone horribly wrong.\nWhat if, as programmers, we actually did something about it?\nConsider that collectively - no, even individually - we who design and build the workings of modern technology have an incredible amount of power. The next indie app that goes viral on ProductHunt will consume hundreds of hours of time from its users. Where is all that untapped, pure potential going to? Some open-ended, inoffensive amusement? Another advertising platform thinly veiled as a game? Perhaps another drop of oil to smooth the machinery of The Great Engine of Commerce?\nI get it - programmers will build what they\u0026rsquo;re paid to build. That\u0026rsquo;s capitalism, that\u0026rsquo;s feeding your family, survival - life. I\u0026rsquo;m not trying to suggest we all quit our jobs, go live in the woods, and volunteer as humanitarians. That would be nice, but it\u0026rsquo;s impractical.\nBut we all have side projects. Free time. What are you doing with yours?\n Before I\u0026rsquo;m accused of being too hand-wavy and idealistic, I want to offer a concrete suggestion. Build things that save time. Not in the \u0026ldquo;I\u0026rsquo;ve made yet another to-do list app for you to download,\u0026rdquo; kind of way, but in the \u0026ldquo;Here\u0026rsquo;s a one-liner to automate this mundane thing that would have taken you hours,\u0026rdquo; kind of way. Here, have a shameless plug.\nI also really like this idea from the first article I mentioned, so hang on tight while I bring this full circle:\n What’s one concrete thing companies could do now to stop subverting our attention?\nI would just like to know what is the ultimate design goal of that site or that system that’s shaping my behavior or thinking. What are they really designing my experience for? Companies will say that their goal is to make the world open and connected or whatever. These are lofty marketing claims. But if you were to actually look at the dashboards that they’re designing, the high-level metrics they’re designing for, you probably wouldn’t see those things. You’d see other things, like frequency of use, time on site, this type of thing. If there was some way for the app to say, to the user, “Here’s generally what this app wants from you, from an attentional point of view,” that would be huge. It would probably be the primary way I would decide which apps I download and use.\n There are so many ways I\u0026rsquo;d love to see this put into practice, from the obvious to the subversive. A little position: sticky; banner? A custom meta tag in the header? Maybe a call to action like this takes more introspection and honesty than a lot of app makers are ready for\u0026hellip; but maybe it just takes a little of our time.\n",url:"https://victoria.dev/blog/about-time/"},"https:\/\/victoria.dev\/blog\/batch-renaming-images-including-image-resolution-with-awk\/":{title:"Batch renaming images, including image resolution, with awk",tags:["terminal","linux","ci/cd"],content:"The most recent item on my list of \u0026ldquo;Geeky things I did that made me feel pretty awesome\u0026rdquo; is an hour\u0026rsquo;s adventure that culminated in this code:\n$ file IMG* | awk \u0026#39;BEGIN{a=0} {print substr($1, 1, length($1)-5),a++\u0026#34;_\u0026#34;substr($8,1, length($8)-1)}\u0026#39; | while read fn fr; do echo $(rename -v \u0026#34;s/$fn/img_$fr/g\u0026#34; *); done IMG_20170808_172653_425.jpg renamed as img_0_4032x3024.jpg IMG_20170808_173020_267.jpg renamed as img_1_3024x3506.jpg IMG_20170808_173130_616.jpg renamed as img_2_3024x3779.jpg IMG_20170808_173221_425.jpg renamed as img_3_3024x3780.jpg IMG_20170808_173417_059.jpg renamed as img_4_2956x2980.jpg IMG_20170808_173450_971.jpg renamed as img_5_3024x3024.jpg IMG_20170808_173536_034.jpg renamed as img_6_4032x3024.jpg IMG_20170808_173602_732.jpg renamed as img_7_1617x1617.jpg IMG_20170808_173645_339.jpg renamed as img_8_3024x3780.jpg IMG_20170909_170146_585.jpg renamed as img_9_3036x3036.jpg IMG_20170911_211522_543.jpg renamed as img_10_3036x3036.jpg IMG_20170913_071608_288.jpg renamed as img_11_2760x2760.jpg IMG_20170913_073205_522.jpg renamed as img_12_2738x2738.jpg // ... etc etc The last item on the aforementioned list is \u0026ldquo;TODO: come up with a shorter title for this list.\u0026rdquo;\nI previously wrote about the power of command line tools like sed. This post expands on how to string all this magical functionality into one big, long, rainbow-coloured, viscous stream of awesome.\nRename files The tool that actually handles the renaming of our files is, appropriately enough, rename. The syntax is: rename -n \u0026quot;s/original_filename/new_filename/g\u0026quot; * where -n does a dry-run, and substituting -v would rename the files. The s indicates our substitution string, and g for \u0026ldquo;global\u0026rdquo; finds all occurrences of the string. The * matches zero or more occurrences of our search-and-replace parameters.\nWe\u0026rsquo;ll come back to this later.\nGet file information When I run $ file IMG_20170808_172653_425.jpg in the image directory, I get this output:\nIMG_20170808_172653_425.jpg: JPEG image data, baseline, precision 8, 4032x3024, frames 3 Since we can get the image resolution (\u0026ldquo;4032x3024\u0026rdquo; above), we know that we\u0026rsquo;ll be able to use it in our new filename.\nIsolate the information we want I love awk for its simplicity. It takes lines of text and makes individual bits of information available to us with built in variables that we can then refer to as column numbers denoted by $1, $2, etc. By default, awk splits up columns on whitespace. To take the example above:\n| 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 | ------------------------------------------------------------------------------------------------------------- | IMG_20170808_172653_425.jpg: | JPEG | image | data, | baseline, | precision | 8, | 4032x3024, | frames | 3 | We can denote different values to use as a splitter with, for example, -F',' if we wanted to use commas as the column divisions. For our current project, spaces are fine.\nThere are a couple issues we need to solve before we can plug the information into our new filenames. Column $1 has the original filename we want, but there\u0026rsquo;s an extra \u0026ldquo;:\u0026rdquo; character on the end. We don\u0026rsquo;t need the \u0026ldquo;.jpg\u0026rdquo; either. Column $8 has an extra \u0026ldquo;,\u0026rdquo; that we don\u0026rsquo;t want as well. To get just to information we need, we\u0026rsquo;ll take a substring of the column with substr():\nsubstr($1, 1, length($1)-5) - This gives us the file name from the beginning of the string to the end of the string, minus 5 characters (\u0026ldquo;length minus 5\u0026rdquo;). substr($8,1, length($8)-1) - This gives us the image size, without the extra comma (\u0026ldquo;length minus 1\u0026rdquo;).\nAvoid duplicate file names To ensure that two images with the same resolutions don\u0026rsquo;t create identical, competing file names, we\u0026rsquo;ll append a unique incrementing number to the filename.\nBEGIN{a=0} - Using BEGIN tells awk to run the following code only once, at the (drumroll) beginning. Here, we\u0026rsquo;re declaring the variable a to be 0. a++ - Later in our code, at the appropriate spot for our file name, we call a and increment it.\nWhen awk prints a string, it concatenates everything that isn\u0026rsquo;t separated by a comma. {print a b c} would create \u0026ldquo;abc\u0026rdquo; and {print a,b,c} would create \u0026ldquo;a b c\u0026rdquo;, for example.\nWe can add additional characters to our file name, such as an underscore, by inserting it in quotations: \u0026quot;_\u0026quot;.\nString it all together To feed the output of one command into another command, we use \u0026ldquo;pipe,\u0026rdquo; written as |.\nIf we only used pipe in this instance, all our data from file and awk would get fed into rename all at once, making for one very, very long and probably non-compiling file name. To run the rename command line by line, we can use while and read. Similarly to awk, read takes input and splits it into variables we can assign and use. In our code, it takes the first bit of output from awk (the original file name) and assigns that the variable name $fn. It takes the second output (our incrementing number and the image resolution) and assigns that to $fr. The variable names are arbitrary; you can call them whatever you want.\nTo run our rename commands as if we\u0026rsquo;d manually entered them in the terminal one by one, we can use echo $(some command). Finally, done ends our while loop.\nBonus round: rainbow output I wasn\u0026rsquo;t kidding with that \u0026ldquo;rainbow-coloured\u0026rdquo; bit\u0026hellip;\np install lolcat Here\u0026rsquo;s our full code:\nle IMG* | awk \u0026#39;BEGIN{a=0} {print substr($1, 1, length($1)-5),a++\u0026#34;_\u0026#34;substr($8,1, length($8)-1)}\u0026#39; | while read fn fs; do echo $(rename -v \u0026#34;s/$fn/img_$fs/g\u0026#34; *); done | lolcat Enjoy!\n",url:"https://victoria.dev/blog/batch-renaming-images-including-image-resolution-with-awk/"},"https:\/\/victoria.dev\/blog\/how-to-code-a-satellite-algorithm-and-cook-paella-from-scratch\/":{title:"How to code a satellite algorithm and cook paella from scratch",tags:["algorithms","coding","javascript"],content:"What if I told you that by the end of this article, you\u0026rsquo;ll be able to calculate the orbital period of satellites around Earth using their average altitudes and\u0026hellip; You tuned out already, didn\u0026rsquo;t you?\nOkay, how about this: I\u0026rsquo;m going to teach you how to make paella!\nAnd you\u0026rsquo;ll have written a function that does the stuff I mentioned above, just like I did for a freeCodeCamp challenge.\nI promise there\u0026rsquo;s an overarching moral lesson that will benefit you every day for the rest of your life. Or at least, feed you for one night. Let\u0026rsquo;s get started.\nThe only thing I know about paella is that it\u0026rsquo;s an emoticon  Unless you\u0026rsquo;re reading this on a Samsung phone, in which case you\u0026rsquo;re looking at a Korean hotpot.\n  One of my favorite things about living in the world today is that it\u0026rsquo;s totally fine to know next-to-nothing about something. A hundred years ago you might have gone your whole life not knowing anything more about paella other than that it\u0026rsquo;s an emoticon.* But today? You can simply look it up.\n*That was a joke.\nAs with all things in life, when we are unsure, we turn to the internet - in this case, the entry for paella on Wikipedia, which reads:\n Paella \u0026hellip;is a Valencian rice dish. Paella has ancient roots, but its modern form originated in the mid-19th century near the Albufera lagoon on the east coast of Spain adjacent to the city of Valencia. Many non-Spaniards view paella as Spain\u0026rsquo;s national dish, but most Spaniards consider it to be a regional Valencian dish. Valencians, in turn, regard paella as one of their identifying symbols.\n At this point, you\u0026rsquo;re probably full of questions. Do I need to talk to a Valencian? Should I take an online course on the history of Spain? What type of paella should I try to make? What is the common opinion of modern chefs when it comes to paella types?\nIf you set out with the intention of answering all these questions, one thing is certain: you\u0026rsquo;ll never end up actually making paella. You\u0026rsquo;ll spend hours upon hours typing questions into search engines and years later wake up with a Masters in Valencian Cuisine.\nThe \u0026ldquo;Most Important Question\u0026rdquo; method When I talk to myself out loud in public (doesn\u0026rsquo;t everyone?) I refer to this as \u0026ldquo;MIQ\u0026rdquo; (rhymes with \u0026ldquo;Nick\u0026rdquo;). I also imagine MIQ to be a rather crunchy and quite adorable anthropomorphized tortilla chip. Couldn\u0026rsquo;t tell you why.\nMIQ swings his crunchy triangular body around to point me in the right direction, and the right direction always takes the form of the most important question that you need to ask yourself at any stage of problem solving. The first most important question is always this:\nWhat is the scope of the objective I want to achieve?\nWell, you want to make paella.\nThe next MIQ then becomes: how much do I actually need to know about paella in order to start making it?\nYou\u0026rsquo;ve heard this advice before: any big problem can be broken down into multiple, but more manageable, bite-size problems. In this little constellation of bite-size problems, there\u0026rsquo;s only one that you need to solve in order to get most of the way to a complete solution.\nIn the case of making paella, we need a recipe. That\u0026rsquo;s a bite-size problem that a search engine can solve for us:\n Simple Paella Recipe\n In a medium bowl, mix together 2 tablespoons olive oil, paprika, oregano, and salt and pepper. Stir in chicken pieces to coat. Cover, and refrigerate. Heat 2 tablespoons olive oil in a large skillet or paella pan over medium heat. Stir in garlic, red pepper flakes, and rice. Cook, stirring, to coat rice with oil, about 3 minutes. Stir in saffron threads, bay leaf, parsley, chicken stock, and lemon zest. Bring to a boil, cover, and reduce heat to medium low. Simmer 20 minutes. Meanwhile, heat 2 tablespoons olive oil in a separate skillet over medium heat. Stir in marinated chicken and onion; cook 5 minutes. Stir in bell pepper and sausage; cook 5 minutes. Stir in shrimp; cook, turning the shrimp, until both sides are pink. Spread rice mixture onto a serving tray. Top with meat and seafood mixture. (allrecipes.com)   And voila! Believe it or not, we\u0026rsquo;re most of the way there already.\nHaving a set of step-by-step instructions that are easy to understand is really most of the work done. All that\u0026rsquo;s left is to go through the motions of gathering the ingredients and then making paella. From this point on, your MIQs may become fewer and far between, and they may slowly decrease in importance in relation to the overall problem. (Where do I buy paprika? How do I know when sausage is cooked? How do I set the timer on my phone for 20 minutes? How do I stop thinking about this delicious smell? Which Instagram filter best captures the ecstasy of this paella right now?)\n The answer to that last one is Nashville\n  I still know nothing about calculating the orbital periods of satellites Okay. Let\u0026rsquo;s examine the problem:\n Return a new array that transforms the element\u0026rsquo;s average altitude into their orbital periods.\nThe array will contain objects in the format {name: \u0026lsquo;name\u0026rsquo;, avgAlt: avgAlt}.\nYou can read about orbital periods on wikipedia.\nThe values should be rounded to the nearest whole number. The body being orbited is Earth.\nThe radius of the earth is 6367.4447 kilometers, and the GM value of earth is 398600.4418 km3s-2.\norbitalPeriod([{name : \u0026quot;sputnik\u0026quot;, avgAlt : 35873.5553}]) should return [{name: \u0026quot;sputnik\u0026quot;, orbitalPeriod: 86400}].\n Well, as it turns out, in order to calculate the orbital period of satellites, we also need a recipe. Amazing, the things you can find on the internet these days.\nCourtesy of dummies.com (yup! #noshame), here\u0026rsquo;s our recipe:\n It\u0026rsquo;s kind of cute, in a way.\n  That might look pretty complicated, but as we\u0026rsquo;ve already seen, we just need to answer the next MIQ: how much do I actually need to know about this formula in order to start using it?\nIn the case of this challenge, not too much. We\u0026rsquo;re already given earthRadius, and avgAlt is part of our arguments object. Together, they form the radius, r. With a couple search queries and some mental time-travel to your elementary math class, we can describe this formula in a smattering of English:\nT, the orbital period, equals 2 multiplied by Pi, in turn multiplied by the square root of the radius, r cubed, divided by the gravitational mass, GM.\nJavaScript has a Math.PI property, as well as Math.sqrt() function and Math.pow() function. Using those combined with simple calculation, we can represent this equation in a single line assigned to a variable:\nvar orbitalPeriod = 2 * Math.PI * (Math.sqrt(Math.pow((earthRadius + avgAlt), 3) / GM)); From the inside out:\n Add earthRadius and avgAlt Cube the result of step 1 Divide the result of step 2 by GM Take the square root of the result of step 3 Multiply 2 times Pi times the result of step 4 Assign the returned value to orbitalPeriod  Believe it or not, we\u0026rsquo;re already most of the way there.\nThe next MIQ for this challenge is to take the arguments object, extract the information we need, and return the result of our equation in the required format. There are a multitude of ways to do this, but I\u0026rsquo;m happy with a straightforward for loop:\nfunction orbitalPeriod(arr) { var resultArr = []; for (var teapot = 0; teapot \u0026lt; arguments[0].length; teapot++) { var GM = 398600.4418; var earthRadius = 6367.4447; var avgAlt = arguments[0][teapot][\u0026#39;avgAlt\u0026#39;]; var name = arguments[0][teapot][\u0026#39;name\u0026#39;]; var orbitalPeriod = 2 * Math.PI * (Math.sqrt(Math.pow((earthRadius + avgAlt), 3) / GM)); var result = { name: name, orbitalPeriod: Math.round(orbitalPeriod) } resultArr.push(result); } return resultArr; } If you need a refresher on iterating through arrays, have a look at my article on iterating, featuring breakfast arrays! (5 minutes read)\nDon\u0026rsquo;t look now, but you just gained the ability to calculate the orbital period of satellites. You could even do it while making paella, if you wanted to. Seriously. Put it on your resume.\nTl;dr: the overarching moral lesson Whether it\u0026rsquo;s cooking, coding, or anything else, problems may at first seem confusing, insurmountable, or downright boring. If you\u0026rsquo;re faced with such a challenge, just remember: they\u0026rsquo;re a lot more digestible with a side of bite-sized MIQ chips.\n",url:"https://victoria.dev/blog/how-to-code-a-satellite-algorithm-and-cook-paella-from-scratch/"},"https:\/\/victoria.dev\/blog\/making-sandwiches-with-closures-in-javascript\/":{title:"Making sandwiches with closures in JavaScript",tags:["javascript","coding"],content:"Say you\u0026rsquo;re having a little coding get-together, and you need some sandwiches. You happen to know that everyone prefers a different type of sandwich, like chicken, ham, or peanut butter and mayo. You could make all these sandwiches yourself, but that would be tedious and boring.\nLuckily, you know of a nearby sandwich shop that delivers. They have the ability and ingredients to make any kind of sandwich in the world, and all you have to do is order through their app.\nThe sandwich shop looks like this:\nfunction makeMeASandwich(x) { var ingredients = x.join(\u0026#39; \u0026#39;); return function barry() { return ingredients.concat(\u0026#39; sandwich\u0026#39;); } } Notice that we have an outer function, makeMeASandwich() that takes an argument, x. This outer function has the local variable ingredients, which is just x mushed together.\nBarry? Who\u0026rsquo;s Barry? He\u0026rsquo;s the guy who works at the sandwich shop. You\u0026rsquo;ll never talk with Barry directly, but he\u0026rsquo;s the reason your sandwiches are made, and why they\u0026rsquo;re so delicious. Barry takes ingredients and mushes them together with \u0026quot; sandwich\u0026quot;.\nThe reason Barry is able to access the ingredients is because they\u0026rsquo;re in his outer scope. If you were to take Barry out of the sandwich shop, he\u0026rsquo;d no longer be able to access them. This is an example of lexical scoping: \u0026ldquo;Nested functions have access to variables declared in their outer scope.\u0026rdquo; (MDN)\nBarry, happily at work in the sandwich shop, is an example of a closure.\n Closures are functions that refer to independent (free) variables (variables that are used locally, but defined in an enclosing scope). In other words, these functions \u0026lsquo;remember\u0026rsquo; the environment in which they were created. (MDN)\n When you order, the app submits your sandwich request like so:\nvar pbm = makeMeASandwich([\u0026#39;peanut butter\u0026#39;, \u0026#39;mayo\u0026#39;]); pbm(); And in thirty-minutes-or-it\u0026rsquo;s-free, you get: peanut butter mayo sandwich.\nThe nice thing about the sandwich shop app is that it remembers the sandwiches you\u0026rsquo;ve ordered before. Your peanut butter and mayo sandwich is now available to you as pbm() for you to order anytime. It\u0026rsquo;s pretty convenient since, each time you order, there\u0026rsquo;s no need to specify that the sandwich you want is the same one you got before with peanut butter and mayo and it\u0026rsquo;s a sandwich. Using pbm() is much more concise.\nLet\u0026rsquo;s order the sandwiches you need for the party:\nvar pmrp = makeMeASandwich([\u0026#39;prosciutto\u0026#39;, \u0026#39;mozzarella\u0026#39;, \u0026#39;red pepper\u0026#39;]); var pbt = makeMeASandwich([\u0026#39;peanut butter\u0026#39;, \u0026#39;tuna\u0026#39;]); var hm = makeMeASandwich([\u0026#39;ham\u0026#39;]); var pbm = makeMeASandwich([\u0026#39;peanut butter\u0026#39;, \u0026#39;mayo\u0026#39;]); pmrp(); pbt(); hm(); pbm(); Your order confirmation reads:\nprosciutto mozzarella red pepper sandwich peanut butter tuna sandwich ham sandwich peanut butter mayo sandwich Plot twist! The guy who wanted a ham sandwich now wants a ham and cheese sandwich. Luckily, the sandwich shop just released a new version of their app that will let you add cheese to any sandwich.\nWith this added feature, the sandwich shop now looks like this:\nfunction makeMeASandwich(x) { var ingredients = x.join(\u0026#39; \u0026#39;); var slices = 0; function barry() { return ingredients.concat(\u0026#39; sandwich\u0026#39;); } function barryAddCheese() { slices += 2; return ingredients.concat(\u0026#39; sandwich with \u0026#39;, slices, \u0026#39; slices of cheese\u0026#39;); } return { noCheese: function() { return barry(); }, addCheese: function() { return barryAddCheese(); } } } You amend the order to look like this:\npmrp.noCheese(); pbt.noCheese(); hm.addCheese(); pbm.noCheese(); And your order confirmation reads:\nprosciutto mozzarella red pepper sandwich peanut butter tuna sandwich ham sandwich with 2 slices of cheese peanut butter mayo sandwich You\u0026rsquo;ll notice that when you order a sandwich with cheese, Barry puts 2 slices of cheese on it. In this way, the sandwich shop controls how much cheese you get. You can\u0026rsquo;t get to Barry to tell him you want more than 2 slices at a time. That\u0026rsquo;s because your only access to the sandwich shop is through the public functions noCheese or addCheese.\nOf course, there\u0026rsquo;s a way to cheat the system\u0026hellip;\nhm.addCheese(); hm.addCheese(); hm.addCheese(); By ordering the same ham sandwich with cheese three times, you get: ham sandwich with 6 slices of cheese.\nThis happens because the sandwich shop app recognizes the variable hm as the same sandwich each time, and increases the number of cheese slices it tells Barry to add.\nThe app could prevent you from adding lots of cheese to the same sandwich, either by adding a maximum or by appending unique order numbers to the variable names\u0026hellip; but this is our fantasy sandwich shop, and we get to pile on as much cheese as we want.\nBy using closures, we can have JavaScript emulate private methods found in languages like Ruby and Java. Closures are a useful way to extend the functionality of JavaScript, and also order sandwiches.\n",url:"https://victoria.dev/blog/making-sandwiches-with-closures-in-javascript/"},"https:\/\/victoria.dev\/blog\/understanding-array.prototype.reduce-and-recursion-using-apple-pie\/":{title:"Understanding Array.prototype.reduce() and recursion using apple pie",tags:["javascript","coding"],content:"I was having trouble understanding reduce() and recursion in JavaScript, so I wrote this article to explain it to myself (hey, look, recursion!). I hope you find my examples both helpful and delicious.\nGiven an array with nested arrays:\nvar arr = [1, [2], [3, [[4]]]] We want to produce this:\nvar flat = [1, 2, 3, 4] Using for loops and if statements Naively, if we know the maximum number of nested arrays we\u0026rsquo;ll encounter (there are 4 in this example), we can use for loops to iterate through each array item, then if statements to check if each item is in itself an array, and so on\u0026hellip;\nfunction flatten() { var flat = []; for (var i=0; i\u0026lt;arr.length; i++) { if (Array.isArray(arr[i])) { for (var ii=0; ii\u0026lt;arr[i].length; ii++) { if (Array.isArray(arr[i][ii])) { for (var iii=0; iii\u0026lt;arr[i][ii].length; iii++) { for (var iiii=0; iiii\u0026lt;arr[i][ii][iii].length; iiii++) { if (Array.isArray(arr[i][ii][iii])) { flat.push(arr[i][ii][iii][iiii]); } else { flat.push(arr[i][ii][iii]); } } } } else { flat.push(arr[i][ii]); } } } else { flat.push(arr[i]); } } } // [1, 2, 3, 4] \u0026hellip;Which works, but of course looks ridiculous. Besides looking ridiculous, a) it only works if we know how many nested arrays we\u0026rsquo;ll process, b) it\u0026rsquo;s hard to read and harder to understand, and c) can you imagine having to debug this mess?! (Gee, I think there\u0026rsquo;s an extra i somewhere.)\nUsing reduce JavaScript has a couple methods we can use to make our code a little less ridiculous. One of these is reduce() and it looks like this:\nvar flat = arr.reduce(function(done,curr){ return done.concat(curr); }, []); // [ 1, 2, 3, [ [ 4 ] ] ] It\u0026rsquo;s a lot less code, but we haven\u0026rsquo;t taken care of some of the nested arrays. Let\u0026rsquo;s first walk through reduce() together and examine what it does to see how we\u0026rsquo;ll correct this.\n Array.prototype.reduce() The reduce() method applies a function against an accumulator and each element in the array (from left to right) to reduce it to a single value. (MDN)\n It\u0026rsquo;s not quite as complicated as it seems. Let\u0026rsquo;s think of reduce() as an out-of-work developer (AI took all the dev jobs) with an empty basket. We\u0026rsquo;ll call him Adam. Adam\u0026rsquo;s main function (ba-dum ching) is now to take apples from a pile, shine them up, and put them one-by-one into the basket. This basket of shiny apples is destined to become delicious apple pies. It\u0026rsquo;s a very important job.\n Apples plus human effort equals pie. Not to be confused with apple-human-pie, which is less appetizing.\n  In our above example, the pile of apples is our array, arr. Our basket is done, the accumulator. The initial value of done is an empty array, which we see as [] at the end of our reduce function. The apple that our out-of-work dev is currently shining, you guessed it, is curr. Once Adam processes the current apple, he places it into the basket (.concat()). When there are no more apples in the pile, he returns the basket of polished apples to us, and then probably goes home to his cat, or something.\nUsing reduce recursively to address nested arrays So that\u0026rsquo;s all well and good, and now we have a basket of polished apples. But we still have some nested arrays to deal with. Going back to our analogy, let\u0026rsquo;s say that some of the apples in the pile are in boxes. Within each box there could be more apples, and/or more boxes containing smaller, cuter apples.\n Adorable, slightly skewed apples just want to be loved/eaten.\n  Here\u0026rsquo;s what we want our apple-processing-function/Adam to do:\n If the pile of apples is a pile of apples, take an apple from the pile. If the apple is an apple, polish it, put it in the basket. If the apple is a box, open the box. If the box contains an apple, go to step 2. If the box contains another box, open this box, and go to step 3. When the pile is no more, give us the basket of shiny apples. If the pile of apples is not a pile of apples, give back whatever it is.  A recursive reduce function that accomplishes this is:\nfunction flatten(arr) { if (Array.isArray(arr)) { return arr.reduce(function(done,curr){ return done.concat(flatten(curr)); }, []); } else { return arr; } } // [ 1, 2, 3, 4 ] Bear with me and I\u0026rsquo;ll explain.\n An act of a function calling itself. Recursion is used to solve problems that contain smaller sub-problems. A recursive function can receive two inputs: a base case (ends recursion) or a recursive case (continues recursion). (MDN)\n If you examine our code above, you\u0026rsquo;ll see that flatten() appears twice. The first time it appears, it tells Adam what to do with the pile of apples. The second time, it tells him what to do with the thing he\u0026rsquo;s currently holding, providing instructions in the case it\u0026rsquo;s an apple, and in the case it\u0026rsquo;s not an apple. The thing to note is that these instructions are a repeat of the original instructions we started with - and that\u0026rsquo;s recursion.\nWe\u0026rsquo;ll break it down line-by-line for clarity:\n function flatten(arr) { - we name our overall function and specify that it will take an argument, arr. `if (Array.isArray(arr)) {we examine the provided \u0026ldquo;arrgument\u0026rdquo; (I know, I\u0026rsquo;m very funny) to determine if it is an array. `return arr.reduce(function(done,curr){if the previous line is true and the argument is an array, we want to reduce it. This is our recursive case. We\u0026rsquo;ll apply the following function to each array item\u0026hellip; `return done.concat(flatten(curr));nexpected plot twist appears! The function we want to apply is the very function we\u0026rsquo;re in. Colloquially: take it from the top. }, []);ell our reduce function to start with an empty accumulator (done`), and wrap it up. `} else {this resolves our if statement at line 2. If the provided argument isn\u0026rsquo;t an array\u0026hellip; return arr;rn whatever thearr` is. (Hopefully a cute apple.) This is our base case that breaks us out of recursion. `}end the else statement. } - end the overall function.  And we\u0026rsquo;re done! We\u0026rsquo;ve gone from our 24 line, 4-layers-deep nested for loop solution to a much more concise, 9 line recursive reduce solution. Reduce and recursion can seem a little impenetrable at first, but they\u0026rsquo;re valuable tools that will save you lots of future effort once you grasp them.\nAnd don\u0026rsquo;t worry about Adam, our out-of-work developer. He got so much press after being featured in this article that he opened up his very own AI-managed apple pie factory. He\u0026rsquo;s very happy.\n +1 for you if you saw that one coming.\n  ",url:"https://victoria.dev/blog/understanding-array.prototype.reduce-and-recursion-using-apple-pie/"},"https:\/\/victoria.dev\/blog\/iterating-over-objects-and-arrays-frequent-errors\/":{title:"Iterating over objects and arrays: frequent errors",tags:["coding","computing","javascript"],content:"Here\u0026rsquo;s some complaining a quick overview of some code that has confounded me more than once. I\u0026rsquo;m told even very experienced developers encounter these situations regularly, so if you find yourself on your third cup of coffee scratching your head over why your code is doing exactly what you told it to do (and not what you want it to do), maybe this post can help you.\nThe example code is JavaScript, since that\u0026rsquo;s what I\u0026rsquo;ve been working in lately, but I believe the concepts to be pretty universal.\nQuick reference for equivalent statements    This\u0026hellip; \u0026hellip;is the same as this     i++; i = i + 1;   i--; i = i - 1;   apples += 5 apples = apples + 5;   apples -= 5 apples = apples - 5;   apples *= 5 apples = apples * 5;   apples /= 5 apples = apples / 5;    Quick reference for logical statements    This\u0026hellip; \u0026hellip;gives this     3 == '3' true (type converted)   3 === '3' false (type matters; integer is not a string)   3 != '3' false (type converted, 3: 3)   3 !== '3' true (type matters; integer is not a string)   || logical \u0026ldquo;or\u0026rdquo;: either side evaluated   \u0026amp;\u0026amp; logical \u0026ldquo;and\u0026rdquo;: both sides evaluated    Objects Given a breakfast object that looks like this:\nvar breakfast = { \u0026#39;eggs\u0026#39;: 2, \u0026#39;waffles\u0026#39;: 2, \u0026#39;fruit\u0026#39;: { \u0026#39;blueberries\u0026#39;: 5, \u0026#39;strawberries\u0026#39;: 1, }, \u0026#39;coffee\u0026#39;: 1 } Or like this:\nIterate over object properties We can iterate through each breakfast item using a for loop as follows:\nfor (item in breakfast) { console.log(\u0026#39;item: \u0026#39;, item); } This produces:\nitem: eggs item: waffles item: fruit item: coffee Get object property value We can access the value of the property or nested properties (in this example, the number of items) like this:\nconsole.log(\u0026#39;How many waffles? \u0026#39;, breakfast[\u0026#39;waffles\u0026#39;]) console.log(\u0026#39;How many strawberries? \u0026#39;, breakfast[\u0026#39;fruit\u0026#39;][\u0026#39;strawberries\u0026#39;]) Or equivalent syntax:\nconsole.log(\u0026#39;How many waffles? \u0026#39;, breakfast.waffles) console.log(\u0026#39;How many strawberries? \u0026#39;, breakfast.fruit.strawberries) This produces:\nHow many waffles? 2 How many strawberries? 1 Get object property from the value If instead I want to access the property via the value, for example, to find out which items are served in twos, I can do so by iterating like this:\nfor (item in breakfast) { if (breakfast[item] == 2) { console.log(\u0026#39;Two of: \u0026#39;, item); } } Which gives us:\nTwo of: eggs Two of: waffles Alter nested property values Say I want to increase the number of fruits in breakfast, because sugar is bad for me and I like things that are bad for me. I can do that like this:\nvar fruits = breakfast[\u0026#39;fruit\u0026#39;]; for (f in fruits) { fruits[f] += 1; } console.log(fruits); Which gives us:\n{ blueberries: 6, strawberries: 2 } Arrays Given an array of waffles that looks like this:\nvar wafflesIAte = [ 1, 3, 2, 0, 5, 2, 11 ]; Or like this:\nIterate through array items We can iterate through each item in the array using a for loop:\nfor (var i = 0; i \u0026lt; wafflesIAte.length; i++) { console.log(\u0026#39;array index: \u0026#39;, i); console.log(\u0026#39;item from array: \u0026#39;, wafflesIAte[i]); } This produces:\narray index: 0 item from array: 1 array index: 1 item from array: 3 array index: 2 item from array: 2 array index: 3 item from array: 0 array index: 4 item from array: 5 array index: 5 item from array: 2 array index: 6 item from array: 11 Some things to remember: i in the above context is a placeholder; we could substitute anything we like (x, n, underpants, etc). It simply denotes each instance of the iteration.\ni \u0026lt; wafflesIAte.length tells our for loop to continue as long as i is less than the array\u0026rsquo;s length (in this case, 7).\ni++ is equivalent to i+1 and means we\u0026rsquo;re incrementing through our array by one each time. We could also use i+2 to proceed with every other item in the array, for example.\nAccess array item by index We can specify an item in the array using the array index, written as wafflesIAte[i] where i is any index of the array. This gives the item at that location.\nArray index always starts with 0, which is accessed with wafflesIAte[0]. Using wafflesIAte[1] gives us the second item in the array, which is \u0026ldquo;3\u0026rdquo;.\nWays to get mixed up over arrays Remember that wafflesIAte.length and the index of the last item in the array are different. The former is 7, the latter is 6.\nWhen incrementing i, remember that [i+1] and [i]+1 are different:\nconsole.log(\u0026#39;[i+1] gives next array index: \u0026#39;, wafflesIAte[0+1]); console.log(\u0026#39;[i]+1 gives index value + 1: \u0026#39;, wafflesIAte[0]+1); Produces:\n[i+1] gives next array index: 3 [i]+1 gives index value + 1: 2 Practice makes\u0026hellip; better The more often you code and correct your errors, the better you\u0026rsquo;ll remember it next time!\nThat\u0026rsquo;s all for now. If you have a correction, best practice, or another common error for me to add, please let me know!\n",url:"https://victoria.dev/blog/iterating-over-objects-and-arrays-frequent-errors/"},"https:\/\/victoria.dev\/blog\/how-to-replace-a-string-with-sed-in-current-and-recursive-subdirectories\/":{title:"How to replace a string with sed in current and recursive subdirectories",tags:["terminal"],content:"Rebranding? Moving to a new domain or static site? I\u0026rsquo;ve had more than a few of these situations myself! Here\u0026rsquo;s how sed can help make these changes easier.\nUpdate a string in multiple files with sed Meet your new friend sed. This amazingly powerful tool lives in your terminal and is available to be totally underused for things like finding and replacing strings in files. You\u0026rsquo;ve got two levels of intensity to choose from:\n Non-recursive: Just the files in my current directory. Recursive: This directory and all the subdirectories it contains, with maximum prejudice.  Here\u0026rsquo;s how!\nCurrent directory, non-recursive Non-recursive means sed won\u0026rsquo;t change files in any subdirectories of the current folder.\n. ├── index.html # Change this file └── blog ├── list.html # Don\u0026#39;t change └── single.html # these files Run this command to search all the files in your current directory and replace a given string. For example, to replace all occurrences of \u0026ldquo;foo\u0026rdquo; with \u0026ldquo;bar\u0026rdquo;:\nsed -i -- \u0026#39;s/foo/bar/g\u0026#39; * Here\u0026rsquo;s what each component of the command does:\n -i will change the original, and stands for \u0026ldquo;in-place.\u0026rdquo; s is for substitute, so we can find and replace. foo is the string we\u0026rsquo;ll be taking away, bar is the string we\u0026rsquo;ll use instead today. g as in \u0026ldquo;global\u0026rdquo; means \u0026ldquo;all occurrences, please.\u0026rdquo; * denotes all file types. (No more rhymes. What a tease.)  You can limit the operation to one file type, such as txt, by using:\nsed -i -- \u0026#39;s/foo/bar/g\u0026#39; *.txt Current directory and subdirectories, recursive You can supplement sed with find to expand your scope to all of the current folder\u0026rsquo;s subdirectories. This will include any hidden files.\nfind . -type f -exec sed -i \u0026#39;s/foo/bar/g\u0026#39; {} + To ignore hidden files (such as .git) you can pass the negation modifier -not -path '*/\\.*', like this:\nfind . -type f -not -path \u0026#39;*/\\.*\u0026#39; -exec sed -i \u0026#39;s/foo/bar/g\u0026#39; {} + This will exclude any file that has the string /. in its path.\nYou can also limit this operation to file names that end in a certain extension, like Markdown:\nfind . -type f -name \u0026#34;*.md\u0026#34; -exec sed -i \u0026#39;s/foo/bar/g\u0026#39; {} + Working with URLs: change the separator If you want to update a URL, the / separator in your strings will need escaping. It ends up looking like this\u0026hellip;\nfind . -type f -exec sed -i \\ \u0026#39;s/https:\\/\\/www.oldurl.com\\/blog/https:\\/\\/www.newurl.com\\/blog/g\u0026#39; {} + You can avoid confusion and mistakes by changing the separator to any non-conflicting character. The character that follows the s will be treated as the separator. In this case, using a , or _ would do. This doesn\u0026rsquo;t require escaping and is much more readable:\nfind . -type f -exec sed -i \\ \u0026#39;s_https://www.oldurl.com/blog_https://www.newurl.com/blog_g\u0026#39; {} + Command-line superpowers for all! I hope this was helpful! If you like this post, there\u0026rsquo;s a lot more where that came from. I write about all sorts of ways to save time with tricks for your terminal. There\u0026rsquo;s some below!\n",url:"https://victoria.dev/blog/how-to-replace-a-string-with-sed-in-current-and-recursive-subdirectories/"},"https:\/\/victoria.dev\/blog\/top-free-resources-for-developing-coding-superpowers\/":{title:"Top free resources for developing coding superpowers",tags:["coding"],content:"I\u0026rsquo;m frequently asked for my opinion on how to get started with being a freelance developer. If you\u0026rsquo;re hoping to live the life of a remote working digital nomad, whichever career you choose, having a little coding expertise in your back pocket will be a big benefit.\nHere\u0026rsquo;s a quick list of resources that you should definitely look at first if you\u0026rsquo;re hoping to gain some coding superpowers for free.\nfreeCodeCamp (freecodecamp.org ) An amazingly high value curriculum that can take you from zero to full-stack. This is always my top recommendation for someone looking to test the waters and see if a development career is interesting enough to pursue. The toughest part about learning to code on your own is getting stuck and not having quick help - this is the problem that I think freeCodeCamp (fCC) solves best by allowing you to immerse yourself in a hugely supportive social community. Through their forum, you can get quick advice if you get stuck on a challenge, and even team up with someone to tackle projects in-depth. The fCC community is lively and diverse with people from all over the world, and many local chapters even host regular meetups.\nHackerRank (hackerrank.com ) Solve challenges tailored for every level of coder over a variety of relevant topics. Enter competitions and increase your chances of getting hired. I love HackerRank especially for its algorithm and statistics challenges - if you\u0026rsquo;re hoping to get into data science, this is an area that you\u0026rsquo;ll need to be especially sharp in. Seasoned developers return to HackerRank to hone their skills and enter competitions that can win you swag and get you noticed for jobs.\nStack Overflow (stackoverflow.com/ ) Even seasoned developers have questions. This is the top search hit that comes up when you Google that error message you thought only you were getting. If you\u0026rsquo;re shy about asking a question you can\u0026rsquo;t find the answer to - don\u0026rsquo;t be! Simply asking it will be of help to the next person who comes looking for the exact same solution.\nThe Odin Project (theodinproject.com/ ) A curriculum for web developers built on a collection of resources designed to take you from \u0026ldquo;What\u0026rsquo;s the Internet?\u0026rdquo; to web dev hire. For those specifically interested in web development, there\u0026rsquo;s a community here for you. The Odin Project (TOP) has plenty of tutorials and practice projects to flesh out your knowledge of web dev essentials.\nA coding glossary for kids - and for the rest of us, too March 4, 2020 update: I recently received a nice email on behalf of Amelia and her class, from Ms. Lincoln. Amelia was kind enough to suggest adding the below resource for young people like her who want to learn how to program!\nHere\u0026rsquo;s a handy glossary and list of games and resources for kids who want to have some fun while learning how to code: Software Programming and Coding Glossary for Kids . Thanks for paying it forward, Amelia, and helping others find resources like these!\nNovember 2, 2020 update: Another kind contribution for kids who want to learn to code came my way courtesy of Steven and Carol. Steven\u0026rsquo;s been using this post to explore programming and was kind enough to share A Beginner’s Guide to Coding and Programming , which has a list of fun coding games to learn with! Thanks for sharing, Steven and Carol!\nStart for free Dive in - it\u0026rsquo;s free! Good luck on your journey to coding superpowers!\nHave one I missed? Let me know!\n",url:"https://victoria.dev/blog/top-free-resources-for-developing-coding-superpowers/"},"https:\/\/victoria.dev\/blog\/things-you-need-to-know-about-becoming-a-data-scientist\/":{title:"Things you need to know about becoming a Data Scientist",tags:["data","life"],content:"I recently attended a panel discussion hosted by General Assembly in Singapore entitled, \u0026ldquo;So you want to be a Data Scientist/Analyst\u0026rdquo;. The panel featured professionals in different stages of their careers and offered a wealth of information to an audience of hopefuls, including tips on how to land a job as a data scientist, and stories debunking myths that color this field.\nThe panelists  Misrab Faizullah-Khan - VP of Data Science, GO_JEK Anthony Ta - Data Scientist, Tech in Asia Leow Guo Jun - Data Scientist, GO_JEK Gabriel Jiang - Data Scientist Adam Drake - Chief Data Officer, Atazzo  Here\u0026rsquo;s a rundown of the major points discussed, paraphrased for brevity.\nWhat\u0026rsquo;s a day-in-the-life like We\u0026rsquo;re mostly \u0026ldquo;data janitors.\u0026rdquo; A large part of working with data begins with and consists of data sanitation. Without quality data, you won\u0026rsquo;t get accurate results. Understanding how data should be sanitized largely encompasses skills that aren\u0026rsquo;t directly related to data analytics. To fully understand the problem you\u0026rsquo;re hoping to solve, you need to talk with the people involved. It\u0026rsquo;s important that everyone understands all the elements of a project, and exactly what those elements are being called. \u0026ldquo;Sales,\u0026rdquo; as an example, may be calculated differently depending on who you\u0026rsquo;re talking to.\nWhat\u0026rsquo;s a data \u0026ldquo;scientist\u0026rdquo; vs. data \u0026ldquo;analyst\u0026rdquo; It largely depends on the company you work for. \u0026ldquo;Data [insert modifier]\u0026rdquo; is only a recent distinction for a job field that has historically been called \u0026ldquo;Business Analytics.\u0026rdquo; In a smaller company, as with any other position, one person may handle a variety of data-related tasks under the title of \u0026ldquo;Data Scientist.\u0026rdquo; In a larger company with more staff and finer grain specialization, you may have a \u0026ldquo;Data Analyst\u0026rdquo; that handles less technical aspects, and a \u0026ldquo;Data Scientist\u0026rdquo; whose work is very technical and involves quantitative learning or machine learning.\nThe field of data science/analytics is fresh enough that standard definitions for job titles really haven\u0026rsquo;t been agreed upon yet. When considering a position, focus on the company rather than the title.\nShould I join a startup or large company There\u0026rsquo;s no wrong answer. Being aware of your own working style and preferences will help guide your decision.\nStartups generally offer more freedom and less micromanaging. This also means that you\u0026rsquo;ll necessarily receive less guidance, and will need to be able to figure stuff out, learn, and make progress under your own power.\nIn a big company, you\u0026rsquo;re likely to experience more structure, and be expected to follow very clearly defined pre-existing processes. Your job scope will likely be more focused than it would be at a startup. You\u0026rsquo;ll experience less freedom in general, but also more certainty in what\u0026rsquo;s expected of you.\nIn the end, especially at the beginning of your career, don\u0026rsquo;t put too much stock in choosing one or the other. If you like the company, big or small, give it a try. If you\u0026rsquo;re not happy there after a few months, then try another one. No career decision is ever permanent.\nIt\u0026rsquo;s also worthwhile to note that even if you find a company you like the first time around, it\u0026rsquo;s in your best interest to change companies after one or two years. The majority of the salary raises you\u0026rsquo;ll earn in your lifetime will occur in the first ten years of your career. Say you\u0026rsquo;re hired by Company A as a junior data scientist for two years - after two years, you\u0026rsquo;re no longer a junior. You can now earn, say, a 30% higher salary in a data scientist position, but it\u0026rsquo;s unlikely that Company A will give you a 30% raise after two years. At that point it\u0026rsquo;s time to find Company B and put a few more years of experience on your resume, then probably change companies again. You don\u0026rsquo;t earn the big bucks sticking with one company for decades - you\u0026rsquo;ll always be the junior developer.\nWhat do you look for when hiring a candidate Overall, the most important skills for a data science candidate are soft skills. Curiosity, tenacity, and good communication skills are vital. Persistence, especially when it comes to adapting to a quickly changing industry, is important. The most promising candidates are passionate enough about the field to be learning everything they can, even outside of their work scope. Hard skills like coding and algorithms can be taught - it\u0026rsquo;s the soft skills that set good candidates apart.\nHacking skills are also vital. This doesn\u0026rsquo;t necessarily mean you can write code. Someone who has a grasp of overall concepts, knows algorithms, and has curiosity enough to continuously learn is going to go farther than someone who can just write code. It takes creativity to build hacking skills on top of being familiar with the basic navigation points. Having the ability to come up with solutions that use available tools in new ways - that\u0026rsquo;s hacking skill.\nDesign thinking is another important asset. Being able to understand how systems integrate on both technical and business levels is very valuable. If you\u0026rsquo;re able to see the big picture, you\u0026rsquo;re more likely to find different ways to accomplish the overall objective.\nYou might think that seeing buzzwords on resumes makes you look more attractive as a candidate - more often, it stands out as a red flag. Putting \u0026ldquo;advanced machine learning\u0026rdquo; on your CV and then demonstrating that you don\u0026rsquo;t know basic algorithms doesn\u0026rsquo;t look good. It\u0026rsquo;s your projects and your interests outside of the job you\u0026rsquo;re applying for that say the most about you. Popular topics in this industry change fast - you\u0026rsquo;re better off having a solid grasp of basic fundamentals as well as a broad array of experience than name-dropping whatever\u0026rsquo;s trending.\nIs there a future for humans in the data science field? When will the machines replace us This isn\u0026rsquo;t a question unique to data science, and many historical examples already exist. Financial investment is a good example - where you used to have a human do calculations and make predictions, computers now do a lot of that automatically, making decisions about risk and possible payoff every day.\nWhere humans won\u0026rsquo;t be replaced, just as in other industries that have embraced automation, is in the human element. You\u0026rsquo;ll still need people to handle communication, be creative, be curious, make interpretations and understand problems\u0026hellip; all those things are fundamentally human aspects of enterprise.\nUltimately, machines and more automation will make human work less of a grind. By automating the mundane stuff, like data sanitization for example, human minds are freed up to develop more interesting things.\nWhat are the future applications for data-driven automation Legal is a good next candidate for automation. There\u0026rsquo;s a lot there that can be handled by programs using data to assess risk.\nMedicine is another field ripe for advances through data. Radiologists, your days are numbered: image detection is coming for you. The whole field of diagnostics is about to drastically change.\nA particularly interesting recent application for data science is in language translation. By looking at similarities in sentence structure and colloquial speech across different languages, we\u0026rsquo;re able to sort similar words based on the \u0026ldquo;space\u0026rdquo; they occupy within the language structure.\nInsurance - the original data science industry - already is and will continue to become very automated. With increased ability to use data to assess risk, we\u0026rsquo;re beginning to see new creative insurance products being introduced. E-commerce companies can now buy insurance on the risk a customer will return a product - hard to do without the accessibility of data that we have today.\nHow do I push data-driven decisions and get my boss to agree with me It\u0026rsquo;s a loaded question. The bottom line is that it depends on the company\u0026rsquo;s data culture and decision path. We\u0026rsquo;ve experienced working for management who say, \u0026ldquo;We\u0026rsquo;ve already made the decisions, we just need the data to prove it.\u0026rdquo; Obviously, that\u0026rsquo;s a tough position to work from.\nGenerally, ask yourself, \u0026ldquo;Am I making my boss look good?\u0026rdquo; You might hear that and think, \u0026ldquo;Why would I let my boss get all the credit?\u0026rdquo; - but who cares? Let them take the credit. If you\u0026rsquo;re producing good work, you\u0026rsquo;re making your team look good. If you make your team look good, you\u0026rsquo;re indispensible to your team and your boss. People who are indispensible are listened to.\nWhat\u0026rsquo;s your best advice for a budding data scientist Don\u0026rsquo;t be too keen to define yourself too quickly. If you narrow your focus too much, especially when you\u0026rsquo;re learning, you can get stuck in a situation of having become an expert in \u0026ldquo;Technology A, version 3\u0026rdquo; when companies are looking to hire for experts in version 4. It happens.\nA broad understanding of fundamentals will be far more valuable to you on the whole. Maybe you start out writing code, and decide you don\u0026rsquo;t like it, but discover that you\u0026rsquo;re really good at designing big picture stuff and leading teams, and you end up as a technical lead. It could even vary depending on the company you work for - so stay flexible.\nYour best bet is to follow what you\u0026rsquo;re passionate about, and try to understand a wide range of overall concepts. Spend the majority of your efforts learning things that are timeless, like the base technologies under hot-topic items like TensorFlow. Arm yourself with a broad understanding of the terrain, different companies, and the products that are out there.\nIf you focus on learning code specifically, learning one language well makes it easier to learn others. Make sure you understand the basics.\nTL;dr it  Adam: Talk more and don\u0026rsquo;t give up. Anthony: [Be] courageous, and hands-on. Gabriel: Be creative. Guo Jun: It\u0026rsquo;s worth the pain. Misrab: Evaluate yourself and maintain a feedback loop.  General Assembly is one of many schools and resources available to those interested in a career in data science. I\u0026rsquo;ve listed a few others in this post if you\u0026rsquo;re looking for more. Good luck!\n",url:"https://victoria.dev/blog/things-you-need-to-know-about-becoming-a-data-scientist/"},"https:\/\/victoria.dev\/blog\/how-i-turned-unity-into-a-tiling-window-manager\/":{title:"How I turned Unity into a tiling window manager",tags:["linux"],content:"Since Ubuntu 17.10, a default installation of Ubuntu Desktop uses GNOME instead of Unity. After a long and loving battle experience using i3 as my workhorse window manager on my old laptop, I\u0026rsquo;ve decided to mainly use Unity on my new laptop instead. I\u0026rsquo;m a huge fan of tiling window managers as a result of my i3 experience, however, the time it took to configure everything is longer than I wanted to spend getting set up before getting to work with my new laptop. (Read: just can\u0026rsquo;t resist spending hours ricing.)\nI got really hooked on i3\u0026rsquo;s functionality though and needed to find ways to replicate it in Unity. Thankfully it only really took a few small adjustments. For anyone looking to use a full-featured desktop environment that comes pretty close to the functionality of a tiling window manager, I hope these tweaks are useful for you.\nWorkspaces You can create workspaces in Unity that resemble workspaces in i3.\nSet up workspaces Where to find it:\nUnity Tweak Tool \u0026gt; Workspace Settings\nSet \u0026ldquo;Horizontal workspaces\u0026rdquo; to as many as you\u0026rsquo;d like, and \u0026ldquo;Vertical workspaces\u0026rdquo; to 1. This will allow you to access spaces by moving right and left.\nKeyboard Shortcuts Set keyboard shortcuts Where to find it:\nSystem Settings \u0026gt; Keyboard\nSwitch workspaces Where to find it:\nSystem Settings \u0026gt; Keyboard \u0026gt; Navigation\nYou can set keyboard shortcuts that assign numbers to your workspaces, and that let you move left and right between them.\nMove windows around (snap) Where to find it:\nSystem Settings \u0026gt; Keyboard \u0026gt; Windows\nYou can maximize and restore windows using shortcut keys. In my case I have them set to \u0026ldquo;Ctrl+Super+Up\u0026rdquo; and \u0026ldquo;Ctrl+Super+Down\u0026rdquo; respectively.\nI discovered this by accident, and I\u0026rsquo;m not sure if it\u0026rsquo;s listed somewhere I can\u0026rsquo;t find. If I press \u0026ldquo;Ctrl+Super\u0026rdquo; and a left or right arrow key, I can snap a window to the left or right half of the screen.\nCustom shortcuts Where to find it:\nSystem Settings \u0026gt; Keyboard \u0026gt; Custom Shortcuts\n\u0026ldquo;Custom Shortcuts\u0026rdquo; allows you to set any keybinds you\u0026rsquo;re missing from i3. The most important ones for me were the shortcuts to launch a terminal and to use rofi.\nLose the Launcher Where to find it:\nUnity Tweak Tool \u0026gt; Launcher\nTurn on \u0026ldquo;Auto-hide\u0026rdquo; and set \u0026ldquo;Reveal sensitivity\u0026rdquo; to zero.\nStart programs automatically at logon Where to find it:\nStartup Applications\nSimilar to using @reboot with Cron.\nRicing C\u0026rsquo;mon, of course I wasn\u0026rsquo;t just going to leave it stock\u0026hellip;\nUnity Tweak Tool You can do a fair bit with Unity Tweak Tool. Here\u0026rsquo;s my setup:\n Theme: Numix Icons: Numix-circle Cursor: Paper Default font: Noto Sans CJK JP Light 10 Monospace font: Ubuntu Mono Regular Document font: Sans Regular 11 Window title font: Noto Sans CJK JP Light 10  Remove Panel (top status bar) shadow Where to find it:\nRename or delete this file: /usr/share/unity/icons/panel_shadow.png\nLog out and in again to restart Unity.\nPanel opacity Where to find it:\nUnity Tweak Tool \u0026gt; Panel \u0026gt; Transparency level\nMisc other settings Turn off web apps Where to find it:\nUnity Tweak Tool \u0026gt; Web Apps \u0026gt; Integration prompts OFF, uncheck Preauthorized domains\nAutostart Open VPN Not strictly a Unity thing, but useful.\nDownload required packages:\n openvpn network-manager-openvpn network-manager-openvpn-gnome  Download your client.ovpn file from your console page and rename it with client.conf. Create a keys.txt file with your username on line 1 and your password on line 2. (Yeah, it\u0026rsquo;s plain text. Ubuntu\u0026rsquo;s .Private encrypted folder is a good place to store it.)\nIn the client.conf file:\n replace instances of \u0026ldquo;openvpn\u0026rdquo; with your actual IP address add the keys.txt file name directly after auth-user-pass, just like this:  auth-user-pass keys.txt Add both client.conf and keys.txt to /etc/openvpn\nFinally, in /etc/default/openvpn, uncomment AUTOSTART=\u0026quot;all\u0026quot;\nDisplay IP address in the Panel Uses a light little utility called indicator-ip.\nFrom the terminal, run sudo apt install indicator-ip.\nAdd it to Startup Applications to run it automatically.\nIf you have a cat Maybe uncheck this System Settings \u0026gt; Keyboard box:\nHope that was helpful! Check back for more tips later - I\u0026rsquo;ll continue to update this post as I discover them!\n",url:"https://victoria.dev/blog/how-i-turned-unity-into-a-tiling-window-manager/"},"https:\/\/victoria.dev\/blog\/how-i-set-up-a-single-dropbox-folder-on-my-dual-boot-windows-and-linux-system\/":{title:"How I set up a single Dropbox folder on my dual-boot Windows and Linux system",tags:["linux"],content:"This article is deprecated. For an overview of an updated and truly cross-platform remote sync solution based on Git, please see this article: A remote sync solution for iOS and Linux: Git and Working Copy. June 14, 2018: Ben Newhouse, former head of Sync at Dropbox, was kind enough to inform me that this set up might have unintended ill effects on your Dropbox data:\n Dropbox on Windows and Linux maintain metadata on each OS differently, so they\u0026rsquo;ll confuse each other. This will likely break things like version tracking and comments and as this isn\u0026rsquo;t a supported configuration probably worse.\n I\u0026rsquo;ve personally moved away from using Dropbox (and for that matter, dual-boot) since writing this article. My current file sync solution for my all-Linux system uses a private repository on Bitbucket. Still, I never noticed any problems when I used this dual-boot setup. Here\u0026rsquo;s how to do it, at your own risk.\nThe two OS, one Dropbox folder setup I\u0026rsquo;ve now set up two different laptops to dual-boot Windows 10 and different Linux distributions. Here\u0026rsquo;s how to ensure you maintain one Dropbox folder over both operating systems.\nAssuming Windows was there first:\n Mount the Windows drive in Linux Install Dropbox in Linux Sign in to link to Dropbox Immediately change Dropbox folder location to the mounted drive (same Dropbox folder as Windows)  Get it right the first time Dropbox will automatically set up your Dropbox folder when you run it the first time. On Windows it\u0026rsquo;s typically located in your C:\\ or D:\\ drive as D:\\Dropbox and on Linux, it\u0026rsquo;ll go into home/Dropbox by default. They key to being able to specify the folder location is to do so the very first time you start Dropbox.\nMount the Windows Drive in Linux If you started with Windows 10, you\u0026rsquo;ll need to make sure the disk that contains your Dropbox folder is mounted in Linux so that you can access it.\nYou can mount the Windows disk in Linux using the terminal with these steps:\n1. Find the correct disk partition address Type sudo fdisk -l. This will return a list of your partitions and information on them. Look at the \u0026ldquo;Size\u0026rdquo; and \u0026ldquo;Type\u0026rdquo; categories to determine which list item is the Windows disk partition that you wish to mount. Its address will look something like /dev/sda2.\n2. Make a directory to mount the drive to Basically, this is what Linux will use to refer to your Windows drive. Type sudo mkdir -p /mnt/winD (where winD is whatever you want to call your Windows disk)\n3. Mount the disk The syntax follows the format: [mount command] [file system type] [windows disk address] [target address] sudo mount -t ntfs /dev/sda2 /mnt/winD\n4. Mount disk automatically on boot Edit your /etc/fstab file to mount the disk automatically when you boot Linux. sudo vi /etc/fstab Add the mount instruction to the file. The syntax is in the format: [windows disk address] [target address] [file system type] [options] [dump] [pass] /dev/sda2 /mnt/winD fat32 defaults 0 2\nInstall Dropbox in Linux Install Dropbox via package download or using the terminal. The completed installation will produce a GUI sign-in.\nSign in to link to Dropbox Input your credentials and wait while Dropbox does its thing. Once you see the \u0026ldquo;Congratulations!\u0026rdquo; message, stop! Don\u0026rsquo;t click the big obvious button, and read on.\nImmediately change your Dropbox folder location to the mounted drive Click on \u0026ldquo;Advanced Settings\u0026rdquo; and change \u0026ldquo;Dropbox location\u0026rdquo; to the Dropbox folder on your mounted drive. This should be the same folder as in your Windows setup. Ok, now you can click the big obvious \u0026ldquo;Open my Dropbox folder\u0026rdquo; button. You\u0026rsquo;re done!\nI didn\u0026rsquo;t read your article before clicking the big obvious button and now I have two folders Uninstall Dropbox on Linux, making sure you get all the bits at /usr/bin/dropbox. Install Dropbox again and this time, read first! :)\nYou\u0026rsquo;re done I hope you found this helpful! If you have questions or want to bug me about using jpgs for screenshots, click that link below. :) Thanks for reading!\n",url:"https://victoria.dev/blog/how-i-set-up-a-single-dropbox-folder-on-my-dual-boot-windows-and-linux-system/"},"https:\/\/victoria.dev\/blog\/how-i-created-custom-desktop-notifications-using-terminal-and-cron\/":{title:"How I created custom desktop notifications using terminal and cron",tags:["terminal","linux"],content:"In my last post I talked about moving from Windows 10 to running i3 on Linux, built up from Debian Base System. Among other things, this change has taught me about the benefits of using basic tools and running a minimal, lightweight system. You can achieve a lot of functionality with just command line tools and simple utilities. One example I\u0026rsquo;d like to illustrate in this post is setting up desktop notifications.\nI use dunst for desktop notifications. It\u0026rsquo;s a simple, lightweight tool that is easy to configure, doesn\u0026rsquo;t have many dependencies, and can be used across various distributions.\nBattery status/low battery notification I was looking for a simple, versatile set up to create notifications for my battery status without having to rely on separate, standalone GUI apps or services. In my search I came across a simple one-line cron task that seemed to be the perfect fit. I adapted it to my purpose and it looks like this:\n# m h dom mon dow command */5 * * * * acpi --battery | awk -F, '/Discharging/ { if (int($2) \u0026lt; 20) print }' | xargs -ri env DISPLAY=:0 notify-send -u critical -i \u0026quot;/usr/share/icons/Paper/16x16/status/xfce-battery-critical.png\u0026quot; -t 3000 \u0026quot;{}\\nBattery low!\u0026quot;  Psst\u0026hellip; here\u0026rsquo;s a great tool for formatting your crontab times.\n There\u0026rsquo;s a lot going on here, so let\u0026rsquo;s break it down: */5 * * * * Every five minutes, do the following.\nacpi --battery Execute acpi and show battery information, which on its own returns something akin to: Battery 0: Discharging, 65%, 03:01:27 remaining\nPretty straightforward so far. At any point you could input acpi --battery in a terminal and receive the status output. Today\u0026rsquo;s post, however, is about receiving this information passively in a desktop notification. So, moving on:\n| awk -F, '/Discharging/ { if (int($2) \u0026lt; 20) print }' Pipe (|) the result of the previous command to awk. (If you don\u0026rsquo;t know what pipe does, here\u0026rsquo;s an answer from superuser.com that explains it pretty well, I think.) awk can do a lot of things, but in this case, we\u0026rsquo;re using it to examine the status of our battery. Let\u0026rsquo;s zoom in on the awk command:\nawk -F, '/Discharging/ { if (int($2) \u0026lt; 20) print }' Basically, we\u0026rsquo;re saying, \u0026ldquo;Hey, awk, look at that input you just got and try to find the word \u0026ldquo;discharging,\u0026rdquo; then look to see if the number after the first comma is less than 20. If so, print the whole input.\u0026rdquo;\n| xargs -ri Pipe the result of the previous command to xargs, which takes it as its input and does more stuff with it. -ri is equivalent to -r (run the next command only if it receives arguments) and -i (look for \u0026ldquo;{}\u0026rdquo; and replace it with the input). So in this example, xargs serves as our gatekeeper and messenger for the next command.\nenv DISPLAY=:0 Run the following utility in the specified display, in this case, the first display of the local machine.\nnotify-send -u critical -i \u0026quot;/usr/share/icons/Paper/16x16/status/xfce-battery-critical.png\u0026quot; -t 3000 \u0026quot;{}\\nLow battery!\u0026quot; Shows a desktop notification with -u critical (critical urgency), -i (the specified icon), -t 3000 (display time/expires after 3000 milliseconds), and finally {} (the output of awk, replaced by xargs).\nNot bad for a one-liner! I made a few modifications for different states of my battery. Here they all are in my crontab:\n# m h dom mon dow command */5 * * * * acpi --battery | awk -F, '/Discharging/ { if ( (int($2) \u0026lt; 30) \u0026amp;\u0026amp; (int($2) \u0026gt; 15) ) print }' | xargs -ri env DISPLAY=:0 notify-send -a \u0026quot;Battery status\u0026quot; -u normal -i \u0026quot;/usr/share/icons/Paper/16x16/status/xfce-battery-low.png\u0026quot; -t 3000 \u0026quot;{}\\nBattery low!\u0026quot; */5 * * * * acpi --battery | awk -F, '/Discharging/ { if (int($2) \u0026lt; 15) print }' | xargs -ri env DISPLAY=:0 notify-send -a \u0026quot;Battery status\u0026quot; -u critical -i \u0026quot;/usr/share/icons/Paper/16x16/status/xfce-battery-critical.png\u0026quot; -t 3000 \u0026quot;{}\\nSeriously, plug me in.\u0026quot; */60 * * * * acpi --battery | awk -F, '/Discharging/ { if (int($2) \u0026gt; 30) print }' | xargs -ri env DISPLAY=:0 notify-send -a \u0026quot;Battery status\u0026quot; -u normal -i \u0026quot;/usr/share/icons/Paper/16x16/status/xfce-battery-ok.png\u0026quot; \u0026quot;{}\u0026quot; */60 * * * * acpi --battery | awk -F, '/Charging/ { print }' | xargs -ri env DISPLAY=:0 notify-send -a \u0026quot;Battery status\u0026quot; -u normal -i \u0026quot;/usr/share/icons/Paper/16x16/status/xfce-battery-ok-charging.png\u0026quot; \u0026quot;{}\u0026quot; */60 * * * * acpi --battery | awk -F, '/Charging/ { if (int($2) == 100) print }' | xargs -ri env DISPLAY=:0 notify-send -a \u0026quot;Battery status\u0026quot; -u normal -i \u0026quot;/usr/share/icons/Paper/16x16/status/xfce-battery-full-charging.png\u0026quot; \u0026quot;Fully charged.\u0026quot; By the way, you can open your crontab in the editor of your choice by accessing it as root from the /var/spool/cron/crontabs/ directory. It\u0026rsquo;s generally best practice however to make changes to your crontab with the command crontab -e.\nYou can see that each notification makes use of the {} placeholder that tells xargs to put its input there - except for the last one. This is interesting because in this case, we\u0026rsquo;re only using xargs -ri as a kind of switch to present the notification. The actual information that was the input for xargs is not needed in the output in order to create a notification.\nAdditional notifications with command line tools With cron and just a few combinations of simple command line tools, you can create interesting and useful notifications. Consider the following:\nPeriodically check your dhcp address */60 * * * * journalctl | awk -F: \u0026#39;/dhcp/ \u0026amp;\u0026amp; /address/ { print $5 }\u0026#39; | tail -1 | xargs -ri env DISPLAY=:0 notify-send -a \u0026#34;dhcp address\u0026#34; -u normal \u0026#34;{}\u0026#34; Which does the following: */60 * * * * Every 60 minutes.\njournalctl Take the contents of your system log.\n| tail -1'/dhcp/ \u0026amp;\u0026amp; /address/ { print $5 }' Find logs containing both \u0026ldquo;dhcp\u0026rdquo; and \u0026ldquo;address\u0026rdquo; and output the 5th portion as separated by \u0026ldquo;:\u0026rdquo; (the time field counts).\n| tail -1 Take the last line of the output.\n| xargs -ri env DISPLAY=:0 notify-send -a \u0026quot;dhcp address\u0026quot; -u normal \u0026quot;{}\u0026quot; Create the desktop notification including the output.\nPeriodically display the time and date */60 * * * * timedatectl status | awk -F\\n \u0026#39;/Local time/ { print }\u0026#39; | xargs -ri env DISPLAY=:0 notify-send -a \u0026#34;Current Time\u0026#34; -u normal \u0026#34;{}\u0026#34; System log activity You can also search your system logs (try journalctl) for any number of things using awk, enabling you to get periodic notifications of virtually any logged events.\nExperiment As with all things, you are only limited by your imagination! I hope this post has given you some idea about the endless possibilities of these simple utilities. Thanks for reading!\n",url:"https://victoria.dev/blog/how-i-created-custom-desktop-notifications-using-terminal-and-cron/"},"https:\/\/victoria.dev\/blog\/how-a-lifelong-windows-user-switched-to-linux...-the-hard-way\/":{title:"How a lifelong Windows user switched to Linux... the hard way",tags:["linux"],content:"Let me preface this by saying that I\u0026rsquo;ve always been interested in finding out how things work. If a thing does something, I want to know why, how, and if I can customize it.\nIt should come as no surprise that Windows 10 drove me crazy.\nSeemingly arbitrary updates that made for a long start-up process on any given day, surprise restarts that meant I\u0026rsquo;d lose my session and thus my thought process from the day before, and the worst part was I never knew what purpose they served. Windows 10 was an impenetrable mysterious hunk of blue screens and decisions I had no part in.\nThankfully for me and others like me, there is an alternative. Well, there are several - many Linux distros exist and the majority are familiar and totally usable when freshly installed. Perhaps one of the most well-known \u0026ldquo;Windows-like\u0026rdquo; distros is Ubuntu. It has a desktop similar to Windows 10 and is very graphical and geared generally towards those new to Linux.\nI went with Debian, the distro that Ubuntu was forked off of. It doesn\u0026rsquo;t hold your hand as much as other feature-rich distributions, and is more customizable. I started with the small installation image found here which you can download and use to create a bootable USB.\nNot entirely on purpose, thanks mostly to a crappy Internet connection (I\u0026rsquo;m a digital nomad in Southeast Asia - it\u0026rsquo;s par for the course) I ended up starting with Debian base system. It\u0026rsquo;s the most basic Debian install available, and excludes things that I as a previous Windows user take for granted. Among other things, I had to find and install (or at least configure):\n A window manager (i3) An application to let me connect to the Internet (NetworkManager/nm-applet) A program that lets me control the brightness of my screen (xrandr) A program that handles sound, called a sound server (pulseaudio) Event handlers, like telling my laptop to suspend when I close the lid A display for basic status information (i3bar) Desktop notifications (dunst) Basic programs like a graphical file manager (PCmanFM), pdf viewer (Okular), and image editor (Darktable)  If unlike me you\u0026rsquo;d rather start with a more fully-featured desktop environment, Debian\u0026rsquo;s Net Install gives you a \u0026ldquo;Standard system\u0026rdquo; with the GNOME desktop environment by default. There\u0026rsquo;s a full installation guide available for Debian that walks you through the process.\nI learned a lot doing it the hard way, however. After starting with the base system, I have a much better perspective on what\u0026rsquo;s going on under the surface. If in the future I run into a problem with one component, I\u0026rsquo;m much more likely to have an idea of how to begin fixing it, since I put it together in the first place.\nI did hit a few potholes along the way, as well as find some things that worked better than others. I\u0026rsquo;ve discussed some of these events in this post. But first, for motivation, the fully riced screenshot:\nI posted all my config files on GitHub in case you want to swipe my sweet, sweet rice.\nTopics covered below  What are config files in the first place? Where the heck do all these config files go? I picked a stupid user name, how do I change it? How do I get NetworkManager and Dropbox to start automatically? How do I set up OpenVPN with NetworkManager? How do I get my VPN status to show in i3bar? How do I get my Print Screen/backlight control/volume keys to work? I edited my config file, but I don\u0026rsquo;t see any changes. WTF?  What are config files in the first place If you\u0026rsquo;re new to Linux or to coding, these are basically the technical user\u0026rsquo;s version of \u0026ldquo;File \u0026gt; Preferences\u0026rdquo; in GUI programs. Depending on the application you\u0026rsquo;re configuring, there are a few different formats and languages. Thankfully, there are ample config file samples available with a simple web search.\nWhere the heck do all these config files go I found some differing file paths. The below are mine and work:\nsystemd event handlers: /etc/systemd/logind.conf For URxvt: ~/.Xdefaults For i3wm: ~/.config/i3/config For i3bar: ~/.config/i3status/config/i3status.conf For dunst: ~/.config/dunst/dunstrc for Compton: ~/.config/compton.conf I picked a stupid user name, how do I change it This was a fun one. Turns out changing my user name messed up a whole bunch of stuff that I then had to go fix/reinstall, among them:\n Dropbox Anaconda and pip Filepaths written in full in config files  To avoid the hassle I experienced, change your user name before you set up most of your programs. If it\u0026rsquo;s too late for that, just be aware that many programs involving a filepath with /home/oldusername/... are affected.\nTo change your user name via the terminal, login as root, then:\n$ killall -u oldusername $ id oldusername \u0026gt;\u0026gt;\u0026gt; uid=1000(oldusername) gid=1000(oldusername) groups=1000(oldusername),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),108(netdev) # change login name $ usermod -l newusername oldusername # change group name $ groupmod -n newusername oldusername # modify home directory $ usermod -d /home/newusername -m newusername # add comment with full name $ usermod -c \u0026#34;New Full Name\u0026#34; newusername # check that \u0026#34;newusername\u0026#34; replaces \u0026#34;old username\u0026#34; in all fields $ id newusername \u0026gt;\u0026gt;\u0026gt; uid=1000(newusername) gid=1000(newusername) groups=1000(newusername),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),108(netdev) If you previously set your background image in i3 and it disappears, don\u0026rsquo;t forget to check the filepath in your config file.\nHow do I get NetworkManager and Dropbox to start automatically Dropbox for Linux supposedly has an autostart setting where you can type dropbox autostart y into a terminal and it\u0026rsquo;ll listen. This didn\u0026rsquo;t work for me. Instead, I added any applications I wanted to start automatically upon login to my i3 config file with this syntax:\n# start apps automatically exec --no-startup-id /usr/bin/nm-applet exec --no-startup-id dropbox start In the above, exec is \u0026ldquo;execute\u0026rdquo;, --no-startup-id basically saves you from watching your cursor do the spinny loading thing, and the last component of the command is the filepath to the program or the syntax to start it, as if typing it into the terminal.\nHow do I set up OpenVPN with NetworkManager First, make sure you\u0026rsquo;ve done apt-get install network-manager-openvpn to get the plugin.\nYou\u0026rsquo;ll need your client.ovpn file. In my case, I set up my own VPN using Amazon EC2, and downloaded the client.ovpn file from my OpenVPN console page.\nOpen client.ovpn with a text editor (like vim or gedit) and change any instances of \u0026ldquo;remote openvpn port xxxx\u0026rdquo; to read \u0026ldquo;remote \u0026lt; your ip address \u0026gt; port xxxx\u0026rdquo; instead.\nUse nm-applet to set up a new VPN connection. If you\u0026rsquo;ve installed the OpenVPN plugin, you\u0026rsquo;ll see OpenVPN as an option in the dropdown. Choose the option to \u0026ldquo;Import a saved VPN configuration\u0026rdquo;.\nOnce you load up your client.ovpn file and click \u0026ldquo;Create,\u0026rdquo; all the settings will be filled out for you except for your user name and password. Plug those in and you\u0026rsquo;re good to go.\nHow do I get my VPN status to show in i3bar Locate your i3status config file. If you don\u0026rsquo;t have one yet, you can use an awesome template like mine.\nEdit the filepath in the following section of code:\nrun_watch VPN { pidfile = \u0026quot;sys/class/net/yourspecialsetting0\u0026quot; } Where \u0026ldquo;yourspecialsetting\u0026rdquo; is any one of tap/tun/tun tap depending on your VPN settings. If you don\u0026rsquo;t know which it is, you can find out in the VPN configuration settings under VPN \u0026gt; Advanced \u0026gt; \u0026ldquo;Set virtual device type\u0026rdquo;.\nHow do I get my Print Screen/backlight control/volume keys to work I used key bindings in my i3 config file. The tricky part was finding out what controlled the particular function (and on some occasions, having to install something to control the function).\n# brightness adjustment (pre-defined levels) bindsym $mod+Shift+F6 exec xrandr --output eDP-1 --brightness 1 bindsym $mod+F6 exec xrandr --output eDP-1 --brightness 0.8 bindsym $mod+F5 exec xrandr --output eDP-1 --brightness 0.5 bindsym $mod+F7 exec xrandr --output eDP-1 --brightness 0.1 # volume control (increment) bindsym $mod+F12 exec amixer -q sset Master 3%+ bindsym $mod+F11 exec amixer -q sset Master 3%- bindsym $mod+F10 exec amixer -q sset Master toggle As you can see, in the case of xrandr I could only find a way to set certain levels. For volume control, amixer seems to offer increments.\nI edited my config file, but I don\u0026rsquo;t see any changes. WTF For some things, like the wallpaper in i3 and dunst notifications, I found I had to either reload the application (for i3, type i3-msg restart in the terminal) or restart my X session (logout and log back in) to see the changes.\nWas it worth it The switch to Linux has definitely been worth it. Though some of the process of learning and discovery was finicky and frustrating, I honestly enjoyed every minute. Nothing beats the feeling of figuring something out and making it work!\nI hope you\u0026rsquo;ve found this post useful. Thanks for reading!\n",url:"https://victoria.dev/blog/how-a-lifelong-windows-user-switched-to-linux...-the-hard-way/"},"https:\/\/victoria.dev\/blog\/how-i-ditched-wordpress-and-set-up-my-custom-domain-https-site-for-almost-free\/":{title:"How I ditched WordPress and set up my custom domain HTTPS site for (almost) free",tags:["websites"],content:"I got annoyed with WordPress.com. While using the service has its pros (like https and a mobile responsive website, and being very visual and beginner-friendly) it\u0026rsquo;s limiting. For someone who\u0026rsquo;s comfortable enough to be tweaking CSS but who\u0026rsquo;s not interested in creating their own theme (or paying upwards of $50 for one), I felt I wasn\u0026rsquo;t really the type of consumer WordPress.com was suited to.\nTo start with, if you want to remove WordPress advertising and use a custom domain name, it\u0026rsquo;s a minimum of $3 per month. If, like me, the free themes provided aren\u0026rsquo;t just what you\u0026rsquo;re looking for, you\u0026rsquo;re stuck with two choices: buy a theme for $50+, or pay $8.25 per month to do some css customization. I don\u0026rsquo;t know about you, but I feel like there should be a hack for this.\nHow I ditched WordPress and got everything I wanted for free Okay, almost free. You still have to pay at least $0.99 for a domain name.\nFor those of you technical enough to skip reading a long post, the recipe is this:\n Buy a custom domain via this Namecheap affiliate link (Thanks for your support! 😊) Install Hugo, my favorite static site generator Host with GitHub Pages Put your custom domain to work with GitHub Pages Use Cloudflare\u0026rsquo;s free plan Enforce HTTPS for GitHub Pages  Let\u0026rsquo;s do the nitty gritty:\n1. Buy a custom domain This one\u0026rsquo;s pretty simple. Head on over to Namecheap , Gandi, or if you\u0026rsquo;re rolling in dough, GoDaddy. Find your perfect web address and buy it up.\nIf it\u0026rsquo;s a personal domain like yourname.com, it\u0026rsquo;s a pretty good idea to pay upfront for five years or even ten years, if you\u0026rsquo;ve got the cash. It\u0026rsquo;ll save you the trouble of remembering to renew, allow you to build your personal brand, and prevent someone else from buying up your URL.\nIf you\u0026rsquo;re just trying out an idea, you can go with a one-year $0.99 experiment. Namecheap also gives you WhoisGuard (domain registration privacy) free for one year.\n2. Install Hugo I\u0026rsquo;m a big fan of Hugo so far. Admittedly, those who feel more comfortable with a visual, WYSIWYG editor may feel like a fish out of water at first. As long as you\u0026rsquo;re not afraid of using command line, though, using Hugo is pretty straightforward. The fact that I have access to all my code is my favorite part. It\u0026rsquo;s only as simple or complicated as I want it to be.\nHugo is open source and free. They\u0026rsquo;ve got great documentation, and following their Quickstart guide line-by-line will get you set up with your new site in minutes.\nIf you\u0026rsquo;re not used to the idea of your site existing as files and folders, the basic premise is this: Hugo, along with the themes available, helps you to create all the pages and files that your site needs to run.\nBlog posts can be written in Markdown and saved in your /content/blog/ folder; preferences for your site and theme can be set in the config.toml file. After that, generating all your site\u0026rsquo;s pages is as quick and easy as typing the command hugo --theme=\u0026lt;your theme\u0026gt;. You\u0026rsquo;ll be able to see a live version of your site in your browser as you\u0026rsquo;re editing it (go to http://localhost:1313/ in your browser, as described in Step 5) so you\u0026rsquo;re not flying blind.\n3. Host with GitHub Pages If you read to Step 12 of Hugo\u0026rsquo;s Quickstart Guide, you\u0026rsquo;ll see that they even provided instructions for hosting your files on GitHub pages. If you\u0026rsquo;re new to Git, you\u0026rsquo;ll first need to sign up at GitHub and then set up Git. GitHub is a very friendly resource, and you can find a multitude of code examples and guides in connection with it. The Hello World Guide will take you through all you need to know to use GitHub.com.\nOnce you\u0026rsquo;re comfortable with the way GitHub works generally, setting up a site by following the guide on GitHub Pages is no big deal. If you followed the Hugo Quickstart Guide up to Step 11, you\u0026rsquo;ll want to jump to Step 12 after creating the repository on GitHub.\nIn case it\u0026rsquo;s not clear, once you set up your new repository on GitHub called yourusername.github.io, grab the HTTPS link at the top. From there it\u0026rsquo;s just a few simple commands to create the git repository for your site and push it to your new web address:\n## from yoursite/public folder: $ git init $ git remote add origin \u0026lt;paste that https url here!\u0026gt; $ git add --all $ git commit -m \u0026#34;Initial commit.\u0026#34; $ git push origin master Have a little celebration - your site is already up at https://yourusername.github.io! Now for the pizza-de-resilience: the custom domain.\n4. Point your custom domain to GitHub Pages To set up your site at apex (meaning yourname.com will replace yourusername.github.io), there\u0026rsquo;s just four steps:\n Add your domain to your GitHub Pages site repository In your domain registrar\u0026rsquo;s DNS settings, create A records pointing to GitHub\u0026rsquo;s IP addresses In your domain registrar\u0026rsquo;s DNS settings, create a CNAME record pointing to yourusername.github.io Make sure there\u0026rsquo;s a CNAME file in the root directory of your GitHub repository containing yourname.com (your custom domain)  5. Enforce HTTPS for GitHub Pages GitHub Pages supports HTTPS through a partnership with Let\u0026rsquo;s Encrypt! This greatly simplifies the process of serving your site securely. Just look for this clever checkbox in the Settings of your site\u0026rsquo;s GitHub repository.\nWhy do I need HTTPS anyway? For one, it\u0026rsquo;ll give your site a little boost on Google. More importantly, it\u0026rsquo;s fundamental to your website security. You can learn more about HTTPS and TLS in this post.\nThat\u0026rsquo;s pretty much it! If you don\u0026rsquo;t see changes right away, give all your services a lunch hour or so to propogate. Soon your site will be up and running at https://yourname.com.\nThanks for reading! If you found this post helpful, there\u0026rsquo;s a lot more where this came from. You can subscribe to victoria.dev to see new posts first!\n",url:"https://victoria.dev/blog/how-i-ditched-wordpress-and-set-up-my-custom-domain-https-site-for-almost-free/"},"https:\/\/victoria.dev\/blog\/iteration-in-python-for-list-and-map\/":{title:"Iteration in Python: for, list, and map",tags:["python","coding"],content:"Iteration in Python can be a little hard to understand. Subtle differences in terminology like iteration, iterator, iterating, and iterable aren\u0026rsquo;t the most beginner-friendly.\nWhen tackling new concepts, I find concrete examples to be most useful. I\u0026rsquo;ll share some in this post and discuss appropriate situations for each. (Pun intended.)\nFor loop First, in pseudocode:\nfor iterating_variable in iterable: statement(s) I find for loops to be the most readable way to iterate in Python. This is especially nice when you\u0026rsquo;re writing code that someone else needs to read and understand, which is always.\nAn iterating_variable, loosely speaking, is anything you could put in a group. For example: a letter in a string, an item from a list, or an integer in a range of integers.\nAn iterable houses the things you iterate on. This can also take different forms: a string with multiple characters, a range of numbers, a list, and so on.\nA statement or multiple statements indicates doing something to the iterating variable. This could be anything from mathematical expressions to simply printing a result.\nHere are a couple simple examples that print each iterating_variable of an iterable:\nfor letter in \u0026#34;Hello world\u0026#34;: print(letter) for i in range(10): print(i) breakfast_menu = [\u0026#34;toast\u0026#34;, \u0026#34;eggs\u0026#34;, \u0026#34;waffles\u0026#34;, \u0026#34;coffee\u0026#34;] for choice in breakfast_menu: print(choice) You can even use a for loop in a more compact situation, such as this one-liner:\nbreakfast_buffet = \u0026#34; \u0026#34;.join(str(item) for item in breakfast_menu) The downside to for loops is that they can be a bit verbose, depending on how much you\u0026rsquo;re trying to achieve. Still, for anyone hoping to make their Python code as easily understood as possible, for loops are the most straightforward choice.\nList comprehensions A pseudocode example:\nnew_list = [statement(s) for iterating_variable in iterable] List comprehensions are a concise and elegant way to create a new list by iterating on variables. Once you have a grasp of how they work, you can perform efficient iterations with very little code.\nList comprehensions will always return a list, which may or may not be appropriate for your situation.\nFor example, you could use a list comprehension to quickly calculate and print tip percentage on a few bar tabs at once:\ntabs = [23.60, 42.10, 17.50] tabs_incl_tip = [round(tab*1.15, 2) for tab in tabs] print(tabs_incl_tip) \u0026gt;\u0026gt;\u0026gt; [27.14, 48.41, 20.12] In one concise line, we\u0026rsquo;ve taken each tab amount, added a 15% tip, rounded it to the nearest cent, and made a new list of the tabs plus the tip values.\nList comprehensions can be an elegant tool if output to a list is useful to you. Be advised that the more statements you add, the more complicated your list comprehension begins to look, especially once you get into nested list comprehensions. If your code isn\u0026rsquo;t well annotated, it may become difficult for another reader to figure out.\nMap How to map, in pseudocode:\nmap(statement, iterable) Map is pretty compact, for better or worse. It can be harder to read and understand, especially if your line of code has a lot of parentheses.\nIn terms of efficiency for character count, map is hard to beat. It applies your statement to every instance of your iterable and returns an iterator.\nHere\u0026rsquo;s an example casting each element of input() (the iterable) from string representation to integer representation. Since map returns an iterator, you also cast the result to a list representation.\nvalues = list(map(int, input().split())) weights = list(map(int, input().split())) It\u0026rsquo;s worth noting that you can also use for loops, list comprehension, and map all together:\noutput = sum([x[0] * x[1] for x in zip(values, weights)]) / sum(weights) print(round(output, 1)) Your iteration toolbox Each of these methods of iteration in Python have a special place in the code I write every day. I hope these examples have helped you see how to use for loops, list comprehensions, and map in your own Python code!\nIf you like this post, there\u0026rsquo;s a lot more where that came from! I write about efficient programming for coders and for leading technical teams. Check out the posts below!\n",url:"https://victoria.dev/blog/iteration-in-python-for-list-and-map/"},"https:\/\/victoria.dev\/blog\/the-basics-of-streaming-with-twitter-api\/":{title:"The basics of streaming with Twitter API",tags:["api","python"],content:"Twitter has begun asking users to apply to use the API, presumably to cut down on unpalatable uses. This article is still relevant, however, no longer represents the best way to get started with Twitter API streaming. For more on the current API, please see the Twitter API Docs. This post covers my initial adventures in exploring Twitter\u0026rsquo;s Streaming API. I\u0026rsquo;ll talk about:\n Getting started making a Twitter Streaming program using a template   Problems arising from programming in Python with Windows 10 and Unicode Refining the search function  You can follow along with my template on Github.\n1. Getting started If you\u0026rsquo;re new to Twitter\u0026rsquo;s Streaming API like I was this morning, here it is from the top.\nTwitter\u0026rsquo;s Streaming API basically enables you to continually load a stream of tweets based on search parameters of your choosing, as they\u0026rsquo;re created in real time. Using a little Python script run from the Windows 10 command line, you can have these tweets display (print) as they\u0026rsquo;re retrieved, while your script is running. The stream will continually update until you stop the script, which from the terminal is achieved with ctrl+c.\nI started with @adamdrake\u0026rsquo;s twitterstreamtemplate on Github, which makes use of queuing in Python (still a concept I\u0026rsquo;m just getting familiar with, but this article has a nice basic overview of it). The Twython 3.4.0 documentation was also very helpful.\nThe Twitter API requires authentication, for which you\u0026rsquo;ll need to register an application with Twitter. You can do this here: https://dev.twitter.com/appss\u0026gt;. Don\u0026rsquo;t worry too much about its name or description if you\u0026rsquo;re just tinkering, like I am. Once you\u0026rsquo;ve registered, you can obtain your new credentials (consumer_key, consumer_secret, token, and token_secret) from the Keys and Access Tokens tab.\nInput your credentials in the appropriate places:\n# Input your credentials below consumer_key = \u0026#34;xxx\u0026#34; consumer_secret = \u0026#34;xxx\u0026#34; token = \u0026#34;xxx\u0026#34; token_secret = \u0026#34;xxx\u0026#34; You can set parameters to find specific tweets, or just show a sample stream. The relevant lines are:\n# stream.statuses.filter(track=\u0026#39;twitter\u0026#39;, language=\u0026#39;en\u0026#39;) stream.statuses.sample(language=\u0026#34;en\u0026#34;) As written, it\u0026rsquo;ll show you the sample stream. Switch the commented lines if you want to set your own search parameters. (More on that below.)\n2. Hiccups: Windows 10, Python, and Unicode My first pass at this program consisted of two goals: 1) successfully pull up a stream of tweets based on my search parameters, and 2) print them. The relevant line for achieving the second goal is:\nprint(tweet) I opened up the terminal and typed python tweet_stream.py to run the program. After a brief pause it returned this error:\nreturn codecs.charmap_encode(input,self.errors,encoding_map)[0] UnicodeEncodeError: 'charmap' codec can't encode characters in position 1270-1271: character maps to \u0026lt;undefined\u0026gt; The short explanation is that Windows 10 doesn\u0026rsquo;t play well with Python\u0026rsquo;s default output encoding, UTF-8. (I\u0026rsquo;m making my way through Python\u0026rsquo;s documentation on Unicode, which is rather long but truly fascinating to me.)\nI explored installing a Linux shell with updated Python so I could use Bash. While that\u0026rsquo;s a good eventual idea for programming in general, it\u0026rsquo;s definitely a big diversion from the topic at hand. Instead, I implemented a program-specific fix using a much faster and less complicated method: convert the tweets to ASCII before they display in the Windows 10 terminal.\nThis is achieved by changing the encoding of the tweet text:\nprint(tweet[\u0026#34;text\u0026#34;].encode(\u0026#34;ascii\u0026#34;, \u0026#34;ignore\u0026#34;)) The above displays the text of the tweets using ASCII encoding, and ignores any characters it can\u0026rsquo;t encode. Instead of ignore, you could also use other handlers to indicate errors. Python documentation gives an example of what these different handlers do here.\n\u0026gt;\u0026gt;\u0026gt; u.encode(\u0026#39;ascii\u0026#39;, \u0026#39;ignore\u0026#39;) b\u0026#39;abcd\u0026#39; \u0026gt;\u0026gt;\u0026gt; u.encode(\u0026#39;ascii\u0026#39;, \u0026#39;replace\u0026#39;) b\u0026#39;?abcd?\u0026#39; \u0026gt;\u0026gt;\u0026gt; u.encode(\u0026#39;ascii\u0026#39;, \u0026#39;xmlcharrefreplace\u0026#39;) # inserts XML character reference b\u0026#39;\u0026amp;#40960;abcd\u0026amp;#1972;\u0026#39; \u0026gt;\u0026gt;\u0026gt; u.encode(\u0026#39;ascii\u0026#39;, \u0026#39;backslashreplace\u0026#39;) # inserts a \\uNNNN escape sequence b\u0026#39;\\\\ua000abcd\\\\u07b4\u0026#39; \u0026gt;\u0026gt;\u0026gt; u.encode(\u0026#39;ascii\u0026#39;, \u0026#39;namereplace\u0026#39;) # inserts a \\N{...} escape sequence b\u0026#39;\\\\N{YI SYLLABLE IT}abcd\\\\u07b4\u0026#39; Now running the program with the Windows 10 terminal should give you\u0026hellip; a lot of tweets. (It\u0026rsquo;ll keep going until you stop the program using ctrl+c. On a couple occassions when my program wasn\u0026rsquo;t finding many tweets, ctrl+c didn\u0026rsquo;t seem to work. I hit it a few more times in succession before it seemed to sync up with the program actually doing something, and stopped the operation.)\n3. Refining the search function To retrieve a stream of tweets filtered by your search parameters, you\u0026rsquo;ll want to alter this line of code:\nstream.statuses.filter(track=\u0026#34;twitter\u0026#34;, language=\u0026#34;en\u0026#34;) The track parameter is what lets you filter your tweets by keywords, hashtags, user mentions, and urls. In the above example, it will search for tweets containing the keyword \u0026ldquo;twitter\u0026rdquo;. You can find its accepted phrases here. Note that the phrases don\u0026rsquo;t work like search engine phrases - characters in the tweet must exactly match the search phrase, and punctuation within the quotes will be considered part of the phrase. For example, \u0026ldquo;café\u0026rdquo; will not find \u0026ldquo;cafe\u0026rdquo; or vice versa, and \u0026ldquo;hello,\u0026rdquo; is a different phrase than \u0026ldquo;hello\u0026rdquo;.\nThe language parameter in the example above limits the search results to tweets written in English. There are further parameters you can use to filter your results, such as follow (which limits your stream to searching the timelines of the users you specify) and location (which can allow you to filter by geolocated tweets). Find the full list of parameters and their values here.\nI intended to use the track parameter to find tweets containing any one of a list of hashtags. I started out with the code below:\nstream.statuses.filter(track=\u0026#34;#coding,#programming,#travel\u0026#34;, language=\u0026#34;en\u0026#34;) Note that the comma separators will cause the program to find tweets that contain any of those search terms: #coding OR #programming, etc. If you want to have your search find tweets containing all your search terms, use spaces only.\nThe track function is pretty literal, and thus not very intuitive. For example, my \u0026ldquo;#programming\u0026rdquo; phrase called up tweets that were related to computer programming as well as television programming. While you could set other parameters that omit certain words, it\u0026rsquo;s not long before such an approach turns into a very lengthy guessing game. A cleaner (and faster) approach is to choose more specific search phrases, for example:\nstream.statuses.filter( track=\u0026#34;#coding,#python,#digitalnomad,#laptoplifestyle\u0026#34;, language=\u0026#34;en\u0026#34; ) Your keywords will vary depending on what you want your stream to display. The Twitter Advanced Search page is a good resource for trying out different combinations before running your script.\nI hope you find this overview of my first Twitter streaming script useful! Thanks for reading!\n",url:"https://victoria.dev/blog/the-basics-of-streaming-with-twitter-api/"}}</script><script src=/js/lunr.min.js></script><script src=/js/search.js></script></footer></main><nav aria-label="Victoria.dev menu for mobile" id=menu-mobile><ul role=menubar><li role=none class=menu-item><a role=menuitem href=/><img src=/icon/bookmark.svg alt="home page icon" height=18px class=filter-icon></a></li><li role=none class=menu-item><span role=menuitem href=/blog/><img src=/icon/quote.svg alt="blog icon" height=18px class=filter-icon></span></li><li role=none class=menu-item><a role=menuitem href=/about/><img src=/icon/profile.svg alt="about page icon" height=18px class=filter-icon></a></li><li role=none class=menu-item><a role=menuitem href=/bookshelf/><img src=/icon/book.svg alt="bookshelf icon" height=18px class=filter-icon></a></li></ul></nav></body></html>